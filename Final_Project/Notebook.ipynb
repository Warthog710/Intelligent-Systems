{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "CSC180 Final Project Notebook",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPj0i3KMcwaa"
      },
      "source": [
        "#### *CSC 180  Intelligent Systems (Spring 2021)*\n",
        "\n",
        "#### *Dr. Haiquan Chen, Dept of Computer Scicence*\n",
        "\n",
        "#### *California State University, Sacramento*\n",
        "<hr>\n",
        "\n",
        "## Final Project: Panda Runner\n",
        "### **Team Members:** Quinn Roemer, Logan Hollmer\n",
        "\n",
        "#### **Description:**\n",
        "In this project we created various CNN models to a play a game of our own design. In this game, the Deep Learning Neural Network was responsible for determining what action to take based on the current frame being displayed on screen. We treated this as a classification problem with the network capable of determining whether it should either jump, slide, or perform no action. In this game, the player was automatically moving forward at a set rate. The dataset used for this project was custom made and was composed of screenshots taken during gameplay, along with a corresponding label for the correct action to take. We trained both custom CNN models of our own design and models using transfer learning from ``VGG16`` and ``MobileNet``. In order to tune the hyper-parameters of our models we used genetic algorithms. Each model was represented by a \"chromosome\" which defined cetain hyper-parameter settings. These were then evolved over numerous generations in an attempt to produce the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1xFbNMIcwac"
      },
      "source": [
        "## Google Colab\n",
        "Tasks related to Google Colab, mount Google Drive and install the Python library DEAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1of6ev-Ycwad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12818429-0a30-45b3-a9ad-f7adb4d2486f"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF_T5lD2cwad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7bc2b4-ce35-40b5-ea6f-c577dd42818d"
      },
      "source": [
        "# Install DEAP\n",
        "!pip install deap"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/d1/803c7a387d8a7e6866160b1541307f88d534da4291572fb32f69d2548afb/deap-1.3.1-cp37-cp37m-manylinux2010_x86_64.whl (157kB)\n",
            "\r\u001b[K     |██                              | 10kB 24.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 17.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 15.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 13.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 51kB 9.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 81kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 92kB 10.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deap) (1.19.5)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VouB_ZPfcwad"
      },
      "source": [
        "## Helpful Tensorflow functions (Little Gems)\n",
        "The following functions are provided by Dr. Haiquan Chen with a few of our own design. They were used to help streamline our work with Tensorflow by encapsulating common or complex tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV_cYXgOcwae"
      },
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import json\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections.abc import Sequence\n",
        "from sklearn import preprocessing\n",
        "from tensorflow import keras\n",
        "from sklearn import metrics\n",
        "\n",
        "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = \"{}-{}\".format(name, x)\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
        "def encode_text_index(df, name):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df[name] = le.fit_transform(df[name])\n",
        "    return le.classes_\n",
        "\n",
        "\n",
        "# Encode a numeric column as zscores\n",
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "\n",
        "\n",
        "# Convert all missing values in the specified column to the median\n",
        "def missing_median(df, name):\n",
        "    med = df[name].median()\n",
        "    df[name] = df[name].fillna(med)\n",
        "\n",
        "\n",
        "# Convert all missing values in the specified column to the default\n",
        "def missing_default(df, name, default_value):\n",
        "    df[name] = df[name].fillna(default_value)\n",
        "\n",
        "\n",
        "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
        "def to_xy(df, target):\n",
        "    result = []\n",
        "    for x in df.columns:\n",
        "        if x != target:\n",
        "            result.append(x)\n",
        "    # find out the type of the target column. \n",
        "    target_type = df[target].dtypes\n",
        "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
        "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
        "    if target_type in (np.int64, np.int32):\n",
        "        # Classification\n",
        "        dummies = pd.get_dummies(df[target])\n",
        "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
        "    else:\n",
        "        # Regression\n",
        "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
        "\n",
        "# Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "\n",
        "# Regression chart.\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Remove all rows where the specified column is +/- sd standard deviations\n",
        "def remove_outliers(df, name, sd):\n",
        "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
        "    df.drop(drop_rows, axis=0, inplace=True)\n",
        "\n",
        "\n",
        "# Encode a column to a range between normalized_low and normalized_high.\n",
        "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
        "                         data_low=None, data_high=None):\n",
        "    if data_low is None:\n",
        "        data_low = min(df[name])\n",
        "        data_high = max(df[name])\n",
        "\n",
        "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
        "               * (normalized_high - normalized_low) + normalized_low\n",
        "\n",
        "#Given a dict and a name, save it as a json file\n",
        "def saveDict(dict, fileName, path='/content/drive/MyDrive/CSC180_Final_Project/saved_dictionaries/'):\n",
        "    try:\n",
        "        with open (path + fileName, 'w') as out:\n",
        "            json.dump(dict, out, indent=4)\n",
        "        \n",
        "        out.close()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "#Given a file name, load it into a dictionary and return it\n",
        "def openDict(fileName, path='/content/drive/MyDrive/CSC180_Final_Project/saved_dictionaries/'):\n",
        "    dict = {}\n",
        "\n",
        "    try:\n",
        "        file = open(path + fileName)\n",
        "        data = json.load(file)\n",
        "        file.close()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    for (key, value) in data.items():\n",
        "        dict[key] = value\n",
        "\n",
        "    return dict\n",
        "\n",
        "# Plots a confusion matrix   \n",
        "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(names))\n",
        "    plt.xticks(tick_marks, names, rotation=45)\n",
        "    plt.yticks(tick_marks, names)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ugkXPoecwah"
      },
      "source": [
        "## Data Preprocessing\n",
        "In this section, we load our dataset and perform preprocessing. For the purpose of this project, all images will be downsampled to ``(224, 224, 3)`` from their original ``(1280, 720, 3)``. This is the size that our transfer learning models were trained on, and the size that we will use to train our custom CNN models. As previously stated, the dataset we are using was custom built by automatically taking screenshots at varying intervals to capture actions in our game as it was played by us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2TG-qDhcwai",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "271d3f0c-966b-4781-cb3e-d5bc29e36267"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "showImage = 0\n",
        "label_decoder = ['None', 'Jump', 'Slide']\n",
        "\n",
        "# Load the images\n",
        "images = np.load('/content/drive/MyDrive/CSC180_Final_Project/images.npy', allow_pickle=True)\n",
        "print(f'Shape: {images.shape}\\n')\n",
        "plt.imshow(images[showImage])\n",
        "\n",
        "# Load the labels (0 = None, 1 = W, 2 = S)\n",
        "labels = np.load('/content/drive/MyDrive/CSC180_Final_Project/labels.npy', allow_pickle=True)\n",
        "print(f'Shape: {labels.shape}\\n')\n",
        "print(label_decoder[labels[showImage]])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (5095, 224, 224, 3)\n",
            "\n",
            "Shape: (5095,)\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixZ_IXv2cwaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7674db4e-96b4-44ec-e443-d222354c80d3"
      },
      "source": [
        "# Perform OHE encoding on the labels\n",
        "labels = to_categorical(labels, 3)\n",
        "print(f'Shape: {labels.shape}\\n')\n",
        "print(labels[showImage])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (5095, 3)\n",
            "\n",
            "[1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_m0_kqlcwaj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "87aea71c-2fcf-48fb-a1d0-acb628f58ed2"
      },
      "source": [
        "# Convert images to float32 and normalize\n",
        "images = images.astype('float32')\n",
        "images /= 255\n",
        "plt.imshow(images[showImage])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe5fe826050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4jZIJI8cwaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4b35b2-a96a-4c56-e1c4-85078b3c31b8"
      },
      "source": [
        "# Create train, test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "print(f'x_train: {x_train.shape}')\n",
        "print(f'x_test: {x_test.shape}')\n",
        "print(f'y_train: {y_train.shape}')\n",
        "print(f'y_test: {y_test.shape}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train: (3821, 224, 224, 3)\n",
            "x_test: (1274, 224, 224, 3)\n",
            "y_train: (3821, 3)\n",
            "y_test: (1274, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z8f1paBcwaj"
      },
      "source": [
        "## Part 1: Custom CNN Models\n",
        "Below we define several custom CNN builder functions. Each of these functions accept  a chromosome that is used to create the model. The values in the chromome are as follows: ``[\"CNN Type\", \"Activation Function Type\", \"Optimizer Typer\", \"Number of Fully-Connected Layers\", \"Dropout Chance\"]``. When training our models in our GA, we will evolve the chromosome to produce the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt1Dln59cwak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78719c9f-56a9-4af0-daae-4f808efff936"
      },
      "source": [
        "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Helper functions\n",
        "def get_activation_function(x):\n",
        "  if x % 3 == 0:\n",
        "    return 'relu'\n",
        "  elif x % 3 == 1:\n",
        "    return 'tanh'\n",
        "  else:\n",
        "    return 'sigmoid'\n",
        "\n",
        "def get_optimizer(x):\n",
        "  if x % 2 == 0:\n",
        "    return 'adam'\n",
        "  else:\n",
        "    return 'sgd'\n",
        "\n",
        "# CNN One: https://levelup.gitconnected.com/simple-image-classification-with-cnn-dd5ee3b725\n",
        "def cnn1(chromosome):\n",
        "  activation_type = get_activation_function(chromosome[1])\n",
        "  optimizer_type = get_optimizer(chromosome[2])\n",
        "  dense_layers = chromosome[3] + 2\n",
        "  dropout_chance = float(chromosome[4])/float(13)\n",
        "\n",
        "  # Create the model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3)))\n",
        "  model.add(Activation(activation_type))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding = 'same'))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation(activation_type))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3)))\n",
        "  model.add(Activation(activation_type))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Add the dense layers\n",
        "  for x in range(dense_layers):\n",
        "    model.add(Dense(pow(2, dense_layers + 1 - x), activation=activation_type))\n",
        "    model.add(Dropout(dropout_chance))\n",
        "\n",
        "  # Add the output layer\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer_type, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# CNN Two: https://towardsdatascience.com/the-4-convolutional-neural-network-models-that-can-classify-your-fashion-images-9fe7f3e5399d\n",
        "def cnn2(chromosome):\n",
        "  activation_type = get_activation_function(chromosome[1])\n",
        "  optimizer_type = get_optimizer(chromosome[2])\n",
        "  dense_layers = chromosome[3] + 2\n",
        "  dropout_chance = float(chromosome[4])/float(13)\n",
        "\n",
        "  # Create the model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation=activation_type, input_shape=(224, 224, 3)))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation=activation_type))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(dropout_chance))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation=activation_type))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout_chance))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), activation=activation_type))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(dropout_chance))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Add the dense layers\n",
        "  for x in range(dense_layers):\n",
        "    model.add(Dense(pow(2, dense_layers + 1 - x), activation=activation_type))\n",
        "    model.add(Dropout(dropout_chance))\n",
        "\n",
        "  # Add the output layer\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer_type, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# CNN Three: Of my own design\n",
        "def cnn3(chromosome):\n",
        "  activation_type = get_activation_function(chromosome[1])\n",
        "  optimizer_type = get_optimizer(chromosome[2])\n",
        "  dense_layers = chromosome[3] + 2\n",
        "  dropout_chance = float(chromosome[4])/float(13)\n",
        "\n",
        "  # Create the model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), input_shape=(224, 224, 3), activation=activation_type))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), activation=activation_type))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation=activation_type))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "\n",
        "  # Add the dense layers\n",
        "  for x in range(dense_layers):\n",
        "    model.add(Dense(pow(2, dense_layers + 1 - x), activation=activation_type))\n",
        "    model.add(Dropout(dropout_chance))\n",
        "\n",
        "  # Add the output layer\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer_type, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "#Print the models\n",
        "chromosome = [1, 2, 4, 3, 5]\n",
        "\n",
        "print('Custom CNN 1:\\n')\n",
        "model = cnn1(chromosome)\n",
        "print(f'{model.summary()}\\n\\n')\n",
        "\n",
        "print('Custom CNN 2:\\n')\n",
        "model = cnn2(chromosome)\n",
        "print(f'{model.summary()}\\n\\n')\n",
        "\n",
        "print('Custom CNN 3:\\n')\n",
        "model = cnn3(chromosome)\n",
        "print(f'{model.summary()}\\n\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Custom CNN 1:\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 222, 222, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 109, 109, 64)      18496     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 109, 109, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 52, 52, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 52, 52, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 86528)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                5537856   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 15        \n",
            "=================================================================\n",
            "Total params: 5,633,899\n",
            "Trainable params: 5,633,899\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "\n",
            "Custom CNN 2:\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 222, 222, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 220, 220, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 220, 220, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 110, 110, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 110, 110, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 108, 108, 64)      18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 108, 108, 64)      256       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 108, 108, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 106, 106, 128)     73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 106, 106, 128)     512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 53, 53, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 53, 53, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 359552)            0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                23011392  \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 15        \n",
            "=================================================================\n",
            "Total params: 23,117,707\n",
            "Trainable params: 23,117,195\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "\n",
            "Custom CNN 3:\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 111, 111, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 55, 55, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 27, 27, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 13, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 3)                 15        \n",
            "=================================================================\n",
            "Total params: 104,299\n",
            "Trainable params: 104,299\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnR3rHescwam"
      },
      "source": [
        "## GA Evalution Function\n",
        "After defining our models, we will need to create a function to evaluate a chromosome. This function will be responsible for training the model, saving certain items related to that model, and returning the F1 score of that particular trained model as the evalution. In addition to this, we will save chromosome that have already been tested in a dictionary to prevent the same chromosome from being trained multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "920-HFl5cwao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ea24938-9ca7-48dd-8636-381ddfeb5454"
      },
      "source": [
        "#Create a dictionary of the models that have been already evaluated\n",
        "import os\n",
        "\n",
        "seen_models=None\n",
        "\n",
        "if (os.path.exists('/content/drive/MyDrive/CSC180_Final_Project/saved_dictionaries_custom_cnn/Evaluated_Models_Custom_CNN.json')):\n",
        "  seen_models=openDict('Evaluated_Models_Custom_CNN.json', path='/content/drive/MyDrive/CSC180_Final_Project/saved_dictionaries_custom_cnn/')\n",
        "else:\n",
        "  seen_models={}\n",
        "\n",
        "print(seen_models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'[6, 2, 4, 1, 6]': 0.3377031557765229, '[4, 2, 5, 1, 2]': 0.8071872892752893, '[1, 2, 6, 5, 4]': 0.3377031557765229, '[3, 4, 5, 6, 1]': 0.8143833295581191, '[4, 2, 6, 3, 6]': 0.3377031557765229, '[2, 3, 1, 3, 4]': 0.3377031557765229, '[4, 4, 1, 5, 6]': 0.3377031557765229, '[1, 6, 5, 3, 3]': 0.3377031557765229, '[2, 6, 5, 4, 4]': 0.3377031557765229, '[5, 5, 1, 1, 4]': 0.3377031557765229, '[1, 6, 4, 1, 2]': 0.3377031557765229, '[4, 5, 6, 6, 5]': 0.3377031557765229, '[1, 2, 2, 5, 2]': 0.3377031557765229, '[2, 3, 3, 2, 2]': 0.3377031557765229, '[1, 5, 1, 3, 6]': 0.3377031557765229, '[6, 3, 2, 2, 5]': 0.7219013226133286, '[5, 5, 6, 5, 3]': 0.3377031557765229, '[1, 6, 3, 2, 1]': 0.5480895894676187, '[3, 4, 6, 4, 3]': 0.3377031557765229, '[1, 3, 1, 1, 4]': 0.3377031557765229, '[1, 6, 5, 1, 2]': 0.3377031557765229, '[1, 6, 4, 3, 3]': 0.38668664568507743, '[6, 2, 5, 1, 5]': 0.3377031557765229, '[4, 3, 2, 2, 4]': 0.06415591403484866, '[1, 5, 4, 3, 6]': 0.3377031557765229, '[5, 4, 5, 6, 1]': 0.3662523285369883, '[6, 2, 5, 1, 2]': 0.3377031557765229, '[4, 3, 2, 2, 5]': 0.3377031557765229, '[6, 4, 5, 6, 1]': 0.8089703979331224, '[3, 3, 2, 2, 5]': 0.6283836171416858, '[4, 4, 5, 6, 1]': 0.5015225657542478, '[3, 3, 2, 2, 1]': 0.8011924119595518, '[6, 4, 5, 6, 5]': 0.3377031557765229, '[3, 4, 5, 2, 1]': 0.8130177913763149, '[6, 3, 2, 6, 5]': 0.3377031557765229, '[3, 4, 5, 1, 1]': 0.8104291820595612, '[6, 4, 5, 6, 2]': 0.8139435572980876, '[3, 4, 2, 2, 1]': 0.3377031557765229, '[6, 3, 5, 6, 5]': 0.3377031557765229, '[3, 3, 5, 2, 1]': 0.8175045865486478, '[3, 3, 5, 6, 1]': 0.7854791850222859, '[3, 3, 5, 2, 5]': 0.3377031557765229, '[6, 4, 5, 2, 1]': 0.8100613392196753, '[3, 4, 5, 6, 2]': 0.81963992507217, '[3, 4, 5, 6, 4]': 0.5448993720785914, '[6, 3, 2, 2, 1]': 0.7911053448617258, '[6, 4, 2, 2, 1]': 0.3377031557765229, '[5, 4, 5, 2, 1]': 0.40388878983030135, '[3, 4, 5, 6, 5]': 0.3377031557765229, '[3, 6, 5, 6, 1]': 0.8115358020675714, '[3, 4, 5, 6, 6]': 0.3377031557765229, '[3, 4, 3, 6, 2]': 0.811682443709639, '[3, 4, 5, 2, 2]': 0.8124961612181408, '[5, 1, 6, 6, 2]': 0.3377031557765229, '[3, 4, 5, 4, 2]': 0.8143588385532103, '[3, 4, 5, 1, 2]': 0.8114097257142092, '[6, 6, 5, 1, 1]': 0.8145606512672624, '[1, 3, 4, 5, 2]': 0.3377031557765229, '[3, 5, 6, 1, 2]': 0.3377031557765229, '[2, 4, 6, 5, 2]': 0.3377031557765229, '[2, 3, 4, 6, 3]': 0.3377031557765229, '[2, 5, 2, 4, 1]': 0.3377031557765229, '[1, 5, 6, 1, 1]': 0.3377031557765229, '[4, 5, 3, 4, 1]': 0.3377031557765229, '[3, 5, 3, 2, 3]': 0.3377031557765229, '[5, 4, 1, 5, 6]': 0.3377031557765229, '[1, 2, 6, 5, 5]': 0.3377031557765229, '[1, 5, 6, 1, 6]': 0.3377031557765229, '[5, 3, 1, 6, 5]': 0.3377031557765229, '[4, 1, 4, 4, 4]': 0.3377031557765229, '[4, 6, 3, 2, 3]': 0.3377031557765229, '[5, 3, 4, 3, 2]': 0.3377031557765229, '[1, 3, 5, 6, 2]': 0.804553443096013, '[2, 5, 3, 2, 1]': 0.3377031557765229, '[5, 3, 1, 2, 5]': 0.3377031557765229, '[6, 4, 3, 6, 4]': 0.5428228405513444, '[3, 2, 3, 2, 5]': 0.3377031557765229, '[3, 1, 5, 1, 3]': 0.797909248146896, '[3, 5, 5, 1, 4]': 0.3377031557765229, '[5, 3, 5, 5, 3]': 0.3377031557765229, '[2, 3, 2, 5, 1]': 0.8012014511172759, '[5, 6, 1, 3, 2]': 0.3377031557765229, '[1, 1, 1, 5, 4]': 0.3377031557765229, '[5, 3, 2, 2, 2]': 0.3377031557765229, '[6, 6, 6, 1, 6]': 0.3377031557765229, '[5, 2, 2, 2, 4]': 0.3377031557765229, '[2, 2, 4, 6, 3]': 0.3377031557765229, '[3, 3, 3, 2, 5]': 0.7292399503271264, '[5, 5, 6, 3, 2]': 0.3377031557765229, '[3, 3, 4, 1, 2]': 0.8095553732904825, '[4, 1, 5, 4, 4]': 0.3377031557765229, '[1, 3, 4, 6, 2]': 0.3377031557765229, '[4, 3, 2, 5, 3]': 0.3377031557765229, '[2, 6, 3, 2, 1]': 0.3377031557765229, '[6, 5, 5, 1, 1]': 0.3377031557765229, '[1, 6, 6, 1, 1]': 0.3377031557765229, '[3, 5, 5, 1, 2]': 0.3377031557765229, '[3, 5, 6, 1, 4]': 0.3377031557765229, '[5, 6, 1, 1, 2]': 0.3377031557765229, '[1, 2, 3, 4, 1]': 0.5426372091059481, '[4, 5, 6, 5, 5]': 0.3377031557765229, '[6, 4, 3, 1, 4]': 0.8155327622635093, '[6, 6, 5, 6, 3]': 0.3377031557765229, '[1, 6, 1, 5, 5]': 0.3377031557765229, '[5, 2, 6, 3, 2]': 0.3377031557765229, '[6, 5, 5, 6, 3]': 0.3377031557765229, '[6, 5, 6, 3, 2]': 0.3377031557765229, '[5, 5, 5, 1, 1]': 0.3377031557765229, '[5, 3, 4, 3, 3]': 0.7953845693950317, '[5, 5, 4, 3, 2]': 0.3377031557765229, '[6, 3, 5, 1, 1]': 0.8211977714365055, '[1, 2, 3, 1, 1]': 0.5459659284544077, '[3, 1, 5, 4, 3]': 0.8056267863333547, '[4, 6, 5, 1, 2]': 0.3377031557765229, '[3, 1, 3, 1, 3]': 0.8059090165656838, '[3, 1, 5, 1, 2]': 0.8099628506899174, '[3, 3, 4, 1, 3]': 0.7938344457482388, '[3, 3, 5, 1, 1]': 0.7986320177340336, '[6, 1, 5, 1, 3]': 0.8162127614182155, '[3, 1, 5, 2, 3]': 0.8198240635400844, '[3, 5, 3, 1, 3]': 0.3377031557765229, '[5, 1, 5, 1, 1]': 0.4236986645495041, '[3, 5, 5, 1, 3]': 0.3377031557765229, '[2, 2, 3, 2, 1]': 0.3377031557765229, '[2, 1, 5, 1, 3]': 0.5395697568117358, '[6, 4, 5, 1, 3]': 0.8087598775630576, '[3, 1, 3, 1, 4]': 0.818892257751483, '[6, 4, 5, 1, 2]': 0.8190371818447786, '[3, 4, 3, 1, 4]': 0.8095462218725877, '[6, 1, 5, 1, 2]': 0.803942765956695, '[6, 1, 3, 1, 4]': 0.8144279021707752, '[3, 1, 5, 1, 4]': 0.8121257853164728, '[6, 1, 3, 1, 3]': 0.8108972449536748, '[6, 1, 4, 1, 3]': 0.3377031557765229, '[3, 3, 5, 1, 2]': 0.3377031557765229, '[6, 1, 5, 1, 4]': 0.8142222158533244, '[6, 3, 5, 1, 3]': 0.7051200700060243, '[3, 1, 5, 1, 1]': 0.8095937251792595, '[3, 4, 3, 1, 1]': 0.8147047637244977, '[6, 4, 6, 1, 4]': 0.3377031557765229, '[6, 4, 3, 2, 3]': 0.8179009618979193, '[1, 2, 5, 1, 3]': 0.3377031557765229, '[6, 4, 3, 1, 3]': 0.8105079424752383, '[6, 1, 3, 2, 3]': 0.8075182635381694, '[6, 5, 3, 1, 4]': 0.3377031557765229, '[3, 2, 5, 1, 4]': 0.3377031557765229, '[6, 1, 5, 5, 3]': 0.8150871837889297, '[6, 1, 5, 6, 3]': 0.3377031557765229, '[6, 5, 5, 1, 3]': 0.3377031557765229, '[6, 1, 5, 1, 6]': 0.5421917466184899, '[3, 1, 3, 5, 4]': 0.5418226941168941, '[1, 1, 3, 1, 4]': 0.3377031557765229, '[5, 1, 3, 2, 4]': 0.4318419373726994, '[3, 1, 3, 1, 2]': 0.8086041892902929, '[3, 1, 3, 4, 4]': 0.5405687645156622, '[4, 2, 3, 1, 4]': 0.5495502626350722, '[3, 3, 3, 1, 4]': 0.3377031557765229, '[3, 1, 6, 1, 4]': 0.3377031557765229, '[3, 5, 3, 1, 4]': 0.3377031557765229, '[3, 1, 3, 1, 1]': 0.8163090467418154, '[3, 1, 1, 1, 4]': 0.8111496717495049, '[1, 1, 3, 2, 4]': 0.3377031557765229, '[3, 1, 3, 1, 5]': 0.8069659562385046, '[2, 1, 3, 1, 4]': 0.37078247741869447}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpsuIpEPcwao"
      },
      "source": [
        "#Helper function decodes the CNN type asked for in the chromosome\n",
        "def get_CNN_type(chromosome):\n",
        "  if chromosome[0] % 3 == 0:\n",
        "    print('CNN Type 1\\n')\n",
        "    model = cnn1(chromosome)\n",
        "    return model\n",
        "  elif chromosome[0] % 3 == 1:\n",
        "    print('CNN Type 2\\n')\n",
        "    model = cnn2(chromosome)\n",
        "    return model\n",
        "  else:\n",
        "    print('CNN Type 3\\n')\n",
        "    model = cnn3(chromosome)\n",
        "    return model\n",
        "\n",
        "#Evaluation function for DEAP\n",
        "def evaFitness(chromosome):\n",
        "  #Check if this model has been evaluated... if so return the previous score\n",
        "  modelStr = str(chromosome)\n",
        "  if modelStr in seen_models:\n",
        "    return (seen_models[modelStr],)\n",
        "\n",
        "  #Else, create this model and train it\n",
        "  print('Training: ' + modelStr)\n",
        "\n",
        "  #Make the directory to save the model\n",
        "  if not os.path.exists('/content/drive/MyDrive/CSC180_Final_Project/Custom_CNN_Models/' + modelStr):\n",
        "    os.mkdir('/content/drive/MyDrive/CSC180_Final_Project/Custom_CNN_Models/' + modelStr)\n",
        "\n",
        "  #Create checkpoints\n",
        "  monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=4, verbose=2, mode='auto')\n",
        "  checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/CSC180_Final_Project/Custom_CNN_Models/' + modelStr + '/best_weights.hdf5', verbose=0, save_best_only=True)\n",
        "\n",
        "  #Determine the CNN type to build\n",
        "  model = get_CNN_type(chromosome)\n",
        "\n",
        "  #Train the model\n",
        "  model.fit(x_train, y_train, batch_size=32, epochs=1000, verbose=1, validation_data=(x_test, y_test), callbacks=[monitor, checkpointer])\n",
        "\n",
        "  #Load the best weights\n",
        "  model.load_weights('/content/drive/MyDrive/CSC180_Final_Project/Custom_CNN_Models/' + modelStr + '/best_weights.hdf5')\n",
        "\n",
        "  #Score the model\n",
        "  pred = model.predict(x_test)\n",
        "  pred = np.argmax(pred, axis=1)\n",
        "  y_true = np.argmax(y_test, axis=1)\n",
        "  f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "  \n",
        "  #Record the model as seen\n",
        "  seen_models[modelStr] = f1\n",
        "\n",
        "  #Save the model classification score in the folder\n",
        "  scoreFile = open('/content/drive/MyDrive/CSC180_Final_Project/Custom_CNN_Models/' + modelStr + '/classification_score.txt', 'w')\n",
        "  scoreFile.write(metrics.classification_report(y_true, pred, zero_division=1))\n",
        "  scoreFile.close()\n",
        "\n",
        "  #Save the models confusion matrix plot\n",
        "  cm = confusion_matrix(y_true, pred)\n",
        "  plt.figure()\n",
        "  plot_confusion_matrix(cm, label_decoder)\n",
        "  plt.savefig('/content/drive/MyDrive/CSC180_Final_Project/Custom_CNN_Models/' + modelStr + '/CM.png')\n",
        "  plt.close()\n",
        "\n",
        "  #Return the models eval\n",
        "  print(f'F1 Score: {f1}\\n')\n",
        "  return (f1,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUVu6ONFcwao"
      },
      "source": [
        "## Setup & Run GA\n",
        "After defining our evalution function we are ready to create and run our GA. We will use the Python libray DEAP to facilitate this process. The strategies that we picked for our GA our listed below:\n",
        "* **Mate:** Two Point Crossover (``cxTwoPoint()``)\n",
        "* **Mutation:** Uniform Int (``mutUniformInt()``)\n",
        "* **Selection:** Tournament (``selTournament()``)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phDQyklncwao"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from deap import algorithms, base, creator, tools\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6icQF0rucwap"
      },
      "source": [
        "#Creator function to generate our individuals\n",
        "def create_individual():\n",
        "  individual = []\n",
        "  for x in range(5):\n",
        "    individual.append(random.randint(1, 6))\n",
        "\n",
        "  return individual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g083Ayghcwap"
      },
      "source": [
        "#Define our toolbox\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "#Register strategies\n",
        "toolbox.register(\"evaluate\", evaFitness)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low = 1, up = 6, indpb=0.1)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "#Statistics\n",
        "stats = tools.Statistics(key=lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"max\", np.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpBwEWF5cwap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f104661c-1d00-46e0-e0d8-9873300ffeb4"
      },
      "source": [
        "#Define population and hall of fame\n",
        "pop = toolbox.population(n=20)\n",
        "hof = tools.HallOfFame(maxsize=1)\n",
        "\n",
        "#Run the GA\n",
        "pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=15, stats=stats, halloffame=hof, verbose=True)\n",
        "\n",
        "#Save the dictionary\n",
        "saveDict(seen_models, 'Evaluated_Models_Custom_CNN.json', path='/content/drive/MyDrive/CSC180_Final_Project/saved_dictionaries_custom_cnn/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: [6, 5, 6, 3, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 6s 40ms/step - loss: 1.1993 - accuracy: 0.2801 - val_loss: 1.0532 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0673 - accuracy: 0.4536 - val_loss: 1.0279 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0416 - accuracy: 0.4904 - val_loss: 1.0260 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0539 - accuracy: 0.4800 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0417 - accuracy: 0.4967 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0391 - accuracy: 0.5040 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0346 - accuracy: 0.5004 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0327 - accuracy: 0.5029 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0290 - accuracy: 0.5164 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 00009: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [1, 6, 4, 2, 4]\n",
            "CNN Type 2\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 13s 96ms/step - loss: 3.0242 - accuracy: 0.3990 - val_loss: 2.4259 - val_accuracy: 0.4144\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 11s 89ms/step - loss: 1.0754 - accuracy: 0.4865 - val_loss: 2.6982 - val_accuracy: 0.1923\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 11s 89ms/step - loss: 1.0554 - accuracy: 0.4976 - val_loss: 3.4010 - val_accuracy: 0.3038\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 1.0471 - accuracy: 0.4894 - val_loss: 1.1612 - val_accuracy: 0.4451\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 11s 89ms/step - loss: 1.0383 - accuracy: 0.5005 - val_loss: 1.0231 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 11s 89ms/step - loss: 1.0320 - accuracy: 0.5048 - val_loss: 1.0282 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 11s 89ms/step - loss: 1.0331 - accuracy: 0.4983 - val_loss: 1.0267 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 11s 89ms/step - loss: 1.0261 - accuracy: 0.5078 - val_loss: 1.0261 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 1.0346 - accuracy: 0.4968 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 00009: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [2, 5, 2, 5, 1]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 20ms/step - loss: 1.1870 - accuracy: 0.3168 - val_loss: 1.0489 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0556 - accuracy: 0.4761 - val_loss: 1.0271 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0424 - accuracy: 0.4876 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0342 - accuracy: 0.4887 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0486 - accuracy: 0.4784 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0481 - accuracy: 0.4790 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0329 - accuracy: 0.4858 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0454 - accuracy: 0.4848 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 00008: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [3, 4, 1, 3, 5]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.1963 - accuracy: 0.4204 - val_loss: 1.0472 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.1215 - accuracy: 0.4406 - val_loss: 1.0400 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0953 - accuracy: 0.4514 - val_loss: 1.0406 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0813 - accuracy: 0.4434 - val_loss: 1.0334 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0568 - accuracy: 0.4694 - val_loss: 1.0282 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0527 - accuracy: 0.4605 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0468 - accuracy: 0.4746 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0451 - accuracy: 0.4717 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0395 - accuracy: 0.4907 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0435 - accuracy: 0.4752 - val_loss: 1.0242 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0367 - accuracy: 0.4894 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0346 - accuracy: 0.4973 - val_loss: 1.0241 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0301 - accuracy: 0.5043 - val_loss: 1.0242 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0334 - accuracy: 0.4909 - val_loss: 1.0238 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0334 - accuracy: 0.5021 - val_loss: 1.0240 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0384 - accuracy: 0.4966 - val_loss: 1.0238 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0224 - accuracy: 0.5099 - val_loss: 1.0235 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0315 - accuracy: 0.5033 - val_loss: 1.0234 - val_accuracy: 0.5039\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0234 - accuracy: 0.5095 - val_loss: 1.0232 - val_accuracy: 0.5039\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0312 - accuracy: 0.5027 - val_loss: 1.0220 - val_accuracy: 0.5039\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0257 - accuracy: 0.5051 - val_loss: 1.0132 - val_accuracy: 0.5039\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0204 - accuracy: 0.4997 - val_loss: 0.9590 - val_accuracy: 0.5039\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.9621 - accuracy: 0.5035 - val_loss: 0.8787 - val_accuracy: 0.5039\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.8982 - accuracy: 0.5485 - val_loss: 0.7748 - val_accuracy: 0.6625\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.8334 - accuracy: 0.5816 - val_loss: 0.7238 - val_accuracy: 0.6515\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7907 - accuracy: 0.6195 - val_loss: 0.6959 - val_accuracy: 0.6562\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7782 - accuracy: 0.6081 - val_loss: 0.6812 - val_accuracy: 0.6515\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7496 - accuracy: 0.6288 - val_loss: 0.6752 - val_accuracy: 0.6625\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7416 - accuracy: 0.6407 - val_loss: 0.6600 - val_accuracy: 0.6515\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7326 - accuracy: 0.6348 - val_loss: 0.6604 - val_accuracy: 0.6460\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7263 - accuracy: 0.6448 - val_loss: 0.6517 - val_accuracy: 0.6562\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7341 - accuracy: 0.6321 - val_loss: 0.6578 - val_accuracy: 0.6452\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7112 - accuracy: 0.6344 - val_loss: 0.6535 - val_accuracy: 0.6484\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7103 - accuracy: 0.6437 - val_loss: 0.6460 - val_accuracy: 0.6546\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7269 - accuracy: 0.6261 - val_loss: 0.6456 - val_accuracy: 0.6562\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7018 - accuracy: 0.6396 - val_loss: 0.6575 - val_accuracy: 0.6452\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7179 - accuracy: 0.6390 - val_loss: 0.6649 - val_accuracy: 0.6421\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7299 - accuracy: 0.6337 - val_loss: 0.6552 - val_accuracy: 0.6499\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6934 - accuracy: 0.6505 - val_loss: 0.6541 - val_accuracy: 0.6484\n",
            "Epoch 00039: early stopping\n",
            "F1 Score: 0.5430046513732978\n",
            "\n",
            "Training: [6, 4, 4, 1, 2]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.1718 - accuracy: 0.4495 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.1135 - accuracy: 0.4361 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0957 - accuracy: 0.4591 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0719 - accuracy: 0.4612 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0865 - accuracy: 0.4565 - val_loss: 1.0260 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0579 - accuracy: 0.4752 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0377 - accuracy: 0.4723 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 00007: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 5, 5, 3, 4]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0992 - accuracy: 0.4816 - val_loss: 1.0306 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0609 - accuracy: 0.4819 - val_loss: 1.0268 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0594 - accuracy: 0.4879 - val_loss: 1.0261 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0616 - accuracy: 0.4697 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0566 - accuracy: 0.4740 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0586 - accuracy: 0.4665 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0492 - accuracy: 0.4748 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0507 - accuracy: 0.4861 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0568 - accuracy: 0.4761 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0588 - accuracy: 0.4873 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0428 - accuracy: 0.4822 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0462 - accuracy: 0.4768 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0468 - accuracy: 0.4913 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 00013: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 1, 4, 6, 1]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 20ms/step - loss: 1.0892 - accuracy: 0.4480 - val_loss: 1.0284 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0639 - accuracy: 0.4630 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0521 - accuracy: 0.4792 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0560 - accuracy: 0.4690 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0382 - accuracy: 0.4870 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0435 - accuracy: 0.4814 - val_loss: 1.0268 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0334 - accuracy: 0.5026 - val_loss: 1.0260 - val_accuracy: 0.5039\n",
            "Epoch 00007: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 1, 6, 1, 6]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 19ms/step - loss: 1.1849 - accuracy: 0.4091 - val_loss: 1.0241 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0764 - accuracy: 0.4562 - val_loss: 1.0091 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0471 - accuracy: 0.4849 - val_loss: 1.0240 - val_accuracy: 0.5126\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0116 - accuracy: 0.4996 - val_loss: 0.9983 - val_accuracy: 0.5055\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9792 - accuracy: 0.5225 - val_loss: 0.9493 - val_accuracy: 0.5228\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9681 - accuracy: 0.5050 - val_loss: 0.8662 - val_accuracy: 0.5565\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9034 - accuracy: 0.5480 - val_loss: 0.7736 - val_accuracy: 0.6060\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8321 - accuracy: 0.5709 - val_loss: 0.6859 - val_accuracy: 0.6460\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7646 - accuracy: 0.6110 - val_loss: 0.6639 - val_accuracy: 0.6499\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7249 - accuracy: 0.6228 - val_loss: 0.6574 - val_accuracy: 0.6476\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7108 - accuracy: 0.6492 - val_loss: 0.6494 - val_accuracy: 0.6515\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7384 - accuracy: 0.6226 - val_loss: 0.6603 - val_accuracy: 0.6491\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7138 - accuracy: 0.6363 - val_loss: 0.6502 - val_accuracy: 0.6562\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7216 - accuracy: 0.6368 - val_loss: 0.6532 - val_accuracy: 0.6554\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7226 - accuracy: 0.6254 - val_loss: 0.6521 - val_accuracy: 0.6538\n",
            "Epoch 00015: early stopping\n",
            "F1 Score: 0.5385031148747619\n",
            "\n",
            "Training: [3, 1, 4, 6, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 6s 42ms/step - loss: 1.0699 - accuracy: 0.4309 - val_loss: 1.0263 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0566 - accuracy: 0.4872 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0536 - accuracy: 0.4915 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0292 - accuracy: 0.5119 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0320 - accuracy: 0.5016 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 42ms/step - loss: 1.0377 - accuracy: 0.4993 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0366 - accuracy: 0.4994 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0333 - accuracy: 0.5073 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0357 - accuracy: 0.4982 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0356 - accuracy: 0.5008 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0358 - accuracy: 0.4999 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 00011: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 5, 1, 5, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.1146 - accuracy: 0.3037 - val_loss: 1.0448 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0509 - accuracy: 0.4713 - val_loss: 1.0289 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0398 - accuracy: 0.4942 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0289 - accuracy: 0.5071 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0308 - accuracy: 0.5056 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0423 - accuracy: 0.4856 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0326 - accuracy: 0.4951 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0290 - accuracy: 0.5075 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0255 - accuracy: 0.5034 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0322 - accuracy: 0.5010 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0394 - accuracy: 0.4925 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0290 - accuracy: 0.5113 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0298 - accuracy: 0.5076 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0398 - accuracy: 0.4927 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0353 - accuracy: 0.5012 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 00015: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 2, 2, 3, 6]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0893 - accuracy: 0.4564 - val_loss: 1.0277 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0647 - accuracy: 0.4592 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0581 - accuracy: 0.4669 - val_loss: 1.0264 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0513 - accuracy: 0.4804 - val_loss: 1.0263 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0456 - accuracy: 0.4982 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0494 - accuracy: 0.4923 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 00006: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 4, 1, 4, 5]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 19ms/step - loss: 1.1726 - accuracy: 0.3563 - val_loss: 1.0307 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0883 - accuracy: 0.4443 - val_loss: 1.0284 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0603 - accuracy: 0.4717 - val_loss: 1.0242 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0603 - accuracy: 0.4741 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0482 - accuracy: 0.4993 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0483 - accuracy: 0.4807 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0486 - accuracy: 0.4876 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 00007: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [4, 3, 2, 3, 5]\n",
            "CNN Type 2\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 12s 92ms/step - loss: 7.5335 - accuracy: 0.3697 - val_loss: 5.0192 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0743 - accuracy: 0.5021 - val_loss: 6.1094 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0570 - accuracy: 0.4968 - val_loss: 5.8108 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0507 - accuracy: 0.4825 - val_loss: 2.9419 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0357 - accuracy: 0.5049 - val_loss: 1.3744 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0448 - accuracy: 0.4829 - val_loss: 1.0802 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0290 - accuracy: 0.5060 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0279 - accuracy: 0.5005 - val_loss: 1.0261 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0187 - accuracy: 0.5178 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0322 - accuracy: 0.4964 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0283 - accuracy: 0.5062 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0237 - accuracy: 0.5110 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0327 - accuracy: 0.5018 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0271 - accuracy: 0.5028 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0260 - accuracy: 0.5038 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0281 - accuracy: 0.5013 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 11s 91ms/step - loss: 1.0353 - accuracy: 0.4888 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0326 - accuracy: 0.4965 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 00018: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 5, 5, 5, 4]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 20ms/step - loss: 1.1024 - accuracy: 0.3955 - val_loss: 1.0383 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0644 - accuracy: 0.4691 - val_loss: 1.0299 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0694 - accuracy: 0.4685 - val_loss: 1.0275 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0623 - accuracy: 0.4611 - val_loss: 1.0262 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0664 - accuracy: 0.4655 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0553 - accuracy: 0.4648 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0716 - accuracy: 0.4609 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0661 - accuracy: 0.4664 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0508 - accuracy: 0.4730 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0406 - accuracy: 0.4947 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0565 - accuracy: 0.4629 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0420 - accuracy: 0.4886 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0469 - accuracy: 0.4823 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0464 - accuracy: 0.4725 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0421 - accuracy: 0.4906 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 00015: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [1, 2, 6, 1, 3]\n",
            "CNN Type 2\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 13s 100ms/step - loss: 1.1156 - accuracy: 0.2762 - val_loss: 1.0663 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0591 - accuracy: 0.5011 - val_loss: 1.0384 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0447 - accuracy: 0.4922 - val_loss: 1.0289 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0325 - accuracy: 0.5079 - val_loss: 1.0263 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0398 - accuracy: 0.4961 - val_loss: 1.0259 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0373 - accuracy: 0.4950 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0464 - accuracy: 0.4868 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0389 - accuracy: 0.4897 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0320 - accuracy: 0.5055 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0383 - accuracy: 0.4907 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0326 - accuracy: 0.5021 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0419 - accuracy: 0.4940 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0336 - accuracy: 0.4911 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0262 - accuracy: 0.5108 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0419 - accuracy: 0.4875 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0331 - accuracy: 0.4948 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 00016: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [4, 3, 5, 1, 1]\n",
            "CNN Type 2\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.3934 - accuracy: 0.4758 - val_loss: 1.0576 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.9478 - accuracy: 0.6030 - val_loss: 1.0313 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.8527 - accuracy: 0.6079 - val_loss: 0.9784 - val_accuracy: 0.5126\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.8256 - accuracy: 0.6163 - val_loss: 0.8380 - val_accuracy: 0.5871\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7952 - accuracy: 0.6246 - val_loss: 0.7662 - val_accuracy: 0.6452\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7890 - accuracy: 0.6263 - val_loss: 0.7554 - val_accuracy: 0.6421\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7626 - accuracy: 0.6280 - val_loss: 0.7070 - val_accuracy: 0.6586\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7747 - accuracy: 0.6228 - val_loss: 0.7076 - val_accuracy: 0.6538\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7534 - accuracy: 0.6289 - val_loss: 0.6917 - val_accuracy: 0.6609\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7568 - accuracy: 0.6177 - val_loss: 0.6873 - val_accuracy: 0.6570\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7464 - accuracy: 0.6469 - val_loss: 0.6866 - val_accuracy: 0.6562\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7481 - accuracy: 0.6310 - val_loss: 0.6776 - val_accuracy: 0.6609\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7421 - accuracy: 0.6311 - val_loss: 0.6727 - val_accuracy: 0.6609\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7346 - accuracy: 0.6293 - val_loss: 0.7432 - val_accuracy: 0.6068\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7421 - accuracy: 0.6280 - val_loss: 0.6825 - val_accuracy: 0.6507\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7367 - accuracy: 0.6321 - val_loss: 0.6696 - val_accuracy: 0.6507\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.7202 - accuracy: 0.6234 - val_loss: 0.6490 - val_accuracy: 0.6586\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.6960 - accuracy: 0.6287 - val_loss: 0.7274 - val_accuracy: 0.6052\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.6868 - accuracy: 0.6266 - val_loss: 0.6360 - val_accuracy: 0.6436\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.6519 - accuracy: 0.6540 - val_loss: 0.6029 - val_accuracy: 0.6586\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.6483 - accuracy: 0.6476 - val_loss: 0.5994 - val_accuracy: 0.6570\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.6209 - accuracy: 0.6479 - val_loss: 0.5925 - val_accuracy: 0.6562\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.6302 - accuracy: 0.6350 - val_loss: 0.5875 - val_accuracy: 0.6570\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.6236 - accuracy: 0.6411 - val_loss: 1.0900 - val_accuracy: 0.5039\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.6277 - accuracy: 0.6433 - val_loss: 0.5475 - val_accuracy: 0.6593\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5780 - accuracy: 0.6450 - val_loss: 0.5479 - val_accuracy: 0.7881\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5485 - accuracy: 0.7377 - val_loss: 0.5579 - val_accuracy: 0.7582\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5558 - accuracy: 0.7564 - val_loss: 0.5386 - val_accuracy: 0.7928\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5440 - accuracy: 0.7538 - val_loss: 0.5801 - val_accuracy: 0.7363\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5400 - accuracy: 0.7575 - val_loss: 0.5392 - val_accuracy: 0.7661\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5251 - accuracy: 0.7789 - val_loss: 0.5125 - val_accuracy: 0.7951\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5323 - accuracy: 0.7723 - val_loss: 0.5086 - val_accuracy: 0.7826\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5263 - accuracy: 0.7664 - val_loss: 0.5203 - val_accuracy: 0.7920\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5098 - accuracy: 0.7881 - val_loss: 0.4936 - val_accuracy: 0.8179\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.5105 - accuracy: 0.7769 - val_loss: 0.5121 - val_accuracy: 0.7865\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.4882 - accuracy: 0.7895 - val_loss: 0.5442 - val_accuracy: 0.7402\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.4740 - accuracy: 0.7946 - val_loss: 0.5329 - val_accuracy: 0.7543\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 11s 88ms/step - loss: 0.4746 - accuracy: 0.7870 - val_loss: 0.5137 - val_accuracy: 0.7575\n",
            "Epoch 00038: early stopping\n",
            "F1 Score: 0.817283523009102\n",
            "\n",
            "Training: [5, 5, 3, 5, 6]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 22ms/step - loss: 1.1646 - accuracy: 0.4010 - val_loss: 1.0359 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.1063 - accuracy: 0.4378 - val_loss: 1.0288 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0927 - accuracy: 0.4478 - val_loss: 1.0274 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0820 - accuracy: 0.4553 - val_loss: 1.0268 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0849 - accuracy: 0.4363 - val_loss: 1.0263 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0620 - accuracy: 0.4634 - val_loss: 1.0261 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0538 - accuracy: 0.4661 - val_loss: 1.0261 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0576 - accuracy: 0.4760 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0493 - accuracy: 0.4753 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0373 - accuracy: 0.4958 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0462 - accuracy: 0.4778 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0494 - accuracy: 0.4826 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0323 - accuracy: 0.4955 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0575 - accuracy: 0.4737 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0467 - accuracy: 0.4896 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0423 - accuracy: 0.4960 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0457 - accuracy: 0.4840 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0330 - accuracy: 0.4966 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0324 - accuracy: 0.5017 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0389 - accuracy: 0.4995 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 00020: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 6, 3, 3, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 36ms/step - loss: 1.0881 - accuracy: 0.4608 - val_loss: 1.0590 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0561 - accuracy: 0.4977 - val_loss: 1.0412 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0440 - accuracy: 0.4972 - val_loss: 1.0331 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0394 - accuracy: 0.4893 - val_loss: 1.0292 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0350 - accuracy: 0.4989 - val_loss: 1.0273 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0433 - accuracy: 0.4830 - val_loss: 1.0263 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0376 - accuracy: 0.4922 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0367 - accuracy: 0.4922 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0330 - accuracy: 0.4988 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0282 - accuracy: 0.5047 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0306 - accuracy: 0.4985 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0359 - accuracy: 0.4968 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0294 - accuracy: 0.4989 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0308 - accuracy: 0.5010 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0344 - accuracy: 0.4984 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0334 - accuracy: 0.4995 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0328 - accuracy: 0.4927 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0366 - accuracy: 0.4883 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 33ms/step - loss: 1.0221 - accuracy: 0.5141 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 00019: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [2, 2, 6, 4, 6]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 20ms/step - loss: 1.0925 - accuracy: 0.4080 - val_loss: 1.0327 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0697 - accuracy: 0.4527 - val_loss: 1.0298 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0586 - accuracy: 0.4769 - val_loss: 1.0275 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0581 - accuracy: 0.4782 - val_loss: 1.0268 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0464 - accuracy: 0.4953 - val_loss: 1.0269 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0473 - accuracy: 0.4960 - val_loss: 1.0267 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0359 - accuracy: 0.5080 - val_loss: 1.0260 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0510 - accuracy: 0.4850 - val_loss: 1.0259 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0339 - accuracy: 0.4971 - val_loss: 1.0259 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0407 - accuracy: 0.5001 - val_loss: 1.0260 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0441 - accuracy: 0.4838 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0371 - accuracy: 0.4951 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0412 - accuracy: 0.4912 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0307 - accuracy: 0.5049 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0359 - accuracy: 0.4945 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0336 - accuracy: 0.5004 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 00016: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 5, 4, 1, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.3074 - accuracy: 0.2203 - val_loss: 1.1213 - val_accuracy: 0.1923\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1064 - accuracy: 0.3973 - val_loss: 1.0531 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0668 - accuracy: 0.4809 - val_loss: 1.0339 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0444 - accuracy: 0.4977 - val_loss: 1.0286 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0479 - accuracy: 0.4923 - val_loss: 1.0262 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0452 - accuracy: 0.4969 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0300 - accuracy: 0.5069 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0388 - accuracy: 0.5006 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0263 - accuracy: 0.5182 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0411 - accuracy: 0.4974 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0366 - accuracy: 0.4953 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0541 - accuracy: 0.4756 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0428 - accuracy: 0.4942 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0369 - accuracy: 0.5002 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0293 - accuracy: 0.5111 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0340 - accuracy: 0.5011 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0368 - accuracy: 0.4979 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 00017: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "gen\tnevals\tavg     \tmax     \n",
            "0  \t20    \t0.381987\t0.817284\n",
            "Training: [5, 1, 6, 1, 1]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 19ms/step - loss: 1.0575 - accuracy: 0.4825 - val_loss: 1.0266 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0508 - accuracy: 0.4739 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0550 - accuracy: 0.4732 - val_loss: 1.0265 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0526 - accuracy: 0.4745 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0485 - accuracy: 0.4817 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0336 - accuracy: 0.5066 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 00006: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 1, 1, 3, 2]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 19ms/step - loss: 1.0605 - accuracy: 0.4711 - val_loss: 1.0281 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0434 - accuracy: 0.4991 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0490 - accuracy: 0.4932 - val_loss: 1.0336 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0339 - accuracy: 0.4934 - val_loss: 1.0375 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0402 - accuracy: 0.4923 - val_loss: 1.0231 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0361 - accuracy: 0.4946 - val_loss: 1.0225 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0332 - accuracy: 0.5071 - val_loss: 1.0211 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0348 - accuracy: 0.4931 - val_loss: 1.0216 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0340 - accuracy: 0.4947 - val_loss: 1.0195 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0245 - accuracy: 0.5064 - val_loss: 1.0208 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0271 - accuracy: 0.5089 - val_loss: 1.0161 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0241 - accuracy: 0.5033 - val_loss: 1.0148 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0376 - accuracy: 0.4872 - val_loss: 1.0118 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0143 - accuracy: 0.5084 - val_loss: 1.0083 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0119 - accuracy: 0.5080 - val_loss: 1.0067 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0127 - accuracy: 0.4996 - val_loss: 0.9997 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0070 - accuracy: 0.4963 - val_loss: 1.0203 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0026 - accuracy: 0.5059 - val_loss: 0.9747 - val_accuracy: 0.5039\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9892 - accuracy: 0.4957 - val_loss: 1.0257 - val_accuracy: 0.4992\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9926 - accuracy: 0.4909 - val_loss: 0.9456 - val_accuracy: 0.5039\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9675 - accuracy: 0.5045 - val_loss: 1.1899 - val_accuracy: 0.2057\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9823 - accuracy: 0.4870 - val_loss: 0.9225 - val_accuracy: 0.5000\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9601 - accuracy: 0.5054 - val_loss: 0.9127 - val_accuracy: 0.5102\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9496 - accuracy: 0.5014 - val_loss: 0.9093 - val_accuracy: 0.5267\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9481 - accuracy: 0.4958 - val_loss: 0.9013 - val_accuracy: 0.5283\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9519 - accuracy: 0.4887 - val_loss: 0.9007 - val_accuracy: 0.5094\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9413 - accuracy: 0.5084 - val_loss: 0.8992 - val_accuracy: 0.5196\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9284 - accuracy: 0.4997 - val_loss: 0.8901 - val_accuracy: 0.5385\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9168 - accuracy: 0.5226 - val_loss: 1.0153 - val_accuracy: 0.5031\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9416 - accuracy: 0.5120 - val_loss: 0.8911 - val_accuracy: 0.5141\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9316 - accuracy: 0.5006 - val_loss: 0.8814 - val_accuracy: 0.5361\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9125 - accuracy: 0.5188 - val_loss: 0.8767 - val_accuracy: 0.5408\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9126 - accuracy: 0.5159 - val_loss: 0.8915 - val_accuracy: 0.5071\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9036 - accuracy: 0.5230 - val_loss: 0.9067 - val_accuracy: 0.5063\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9110 - accuracy: 0.5132 - val_loss: 0.9069 - val_accuracy: 0.5047\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9157 - accuracy: 0.5023 - val_loss: 0.8656 - val_accuracy: 0.5432\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8976 - accuracy: 0.5115 - val_loss: 0.8999 - val_accuracy: 0.5094\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9069 - accuracy: 0.5174 - val_loss: 0.8546 - val_accuracy: 0.5353\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8822 - accuracy: 0.5181 - val_loss: 0.8518 - val_accuracy: 0.5314\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8819 - accuracy: 0.5092 - val_loss: 0.8440 - val_accuracy: 0.5424\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8749 - accuracy: 0.5148 - val_loss: 0.8412 - val_accuracy: 0.5495\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8554 - accuracy: 0.5309 - val_loss: 0.8321 - val_accuracy: 0.5534\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8667 - accuracy: 0.5278 - val_loss: 0.8182 - val_accuracy: 0.5620\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8295 - accuracy: 0.5595 - val_loss: 0.8084 - val_accuracy: 0.5612\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8376 - accuracy: 0.5340 - val_loss: 0.8040 - val_accuracy: 0.5612\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8133 - accuracy: 0.5511 - val_loss: 0.7741 - val_accuracy: 0.5824\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8047 - accuracy: 0.5676 - val_loss: 0.8073 - val_accuracy: 0.5659\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7909 - accuracy: 0.5727 - val_loss: 0.7612 - val_accuracy: 0.5903\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7756 - accuracy: 0.5763 - val_loss: 0.8126 - val_accuracy: 0.5612\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7647 - accuracy: 0.5918 - val_loss: 0.7085 - val_accuracy: 0.6177\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7767 - accuracy: 0.5732 - val_loss: 0.6828 - val_accuracy: 0.6374\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7535 - accuracy: 0.5949 - val_loss: 0.7003 - val_accuracy: 0.6138\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7320 - accuracy: 0.5975 - val_loss: 1.0041 - val_accuracy: 0.5165\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7302 - accuracy: 0.6023 - val_loss: 0.7156 - val_accuracy: 0.5997\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7044 - accuracy: 0.6238 - val_loss: 0.6505 - val_accuracy: 0.6413\n",
            "Epoch 56/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7488 - accuracy: 0.6103 - val_loss: 0.6461 - val_accuracy: 0.6444\n",
            "Epoch 57/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6724 - accuracy: 0.6367 - val_loss: 0.6398 - val_accuracy: 0.6484\n",
            "Epoch 58/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6765 - accuracy: 0.6263 - val_loss: 0.6438 - val_accuracy: 0.6413\n",
            "Epoch 59/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7088 - accuracy: 0.6398 - val_loss: 0.9574 - val_accuracy: 0.4961\n",
            "Epoch 60/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6787 - accuracy: 0.6155 - val_loss: 0.6525 - val_accuracy: 0.6452\n",
            "Epoch 61/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6568 - accuracy: 0.6483 - val_loss: 0.6513 - val_accuracy: 0.6334\n",
            "Epoch 00061: early stopping\n",
            "F1 Score: 0.5354629605217104\n",
            "\n",
            "Training: [3, 4, 6, 1, 5]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.2808 - accuracy: 0.3942 - val_loss: 1.0592 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.2128 - accuracy: 0.4060 - val_loss: 1.0582 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1369 - accuracy: 0.4349 - val_loss: 1.0348 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1190 - accuracy: 0.4377 - val_loss: 1.0325 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0831 - accuracy: 0.4484 - val_loss: 1.0294 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0887 - accuracy: 0.4513 - val_loss: 1.0272 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0665 - accuracy: 0.4658 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0544 - accuracy: 0.4941 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0610 - accuracy: 0.4722 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0474 - accuracy: 0.4930 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0429 - accuracy: 0.4965 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0398 - accuracy: 0.5049 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0513 - accuracy: 0.4819 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0389 - accuracy: 0.4974 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0391 - accuracy: 0.5005 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 00015: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 2, 6, 1, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.1281 - accuracy: 0.3577 - val_loss: 1.0363 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0667 - accuracy: 0.4850 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0376 - accuracy: 0.4992 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0502 - accuracy: 0.4937 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0363 - accuracy: 0.5030 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0361 - accuracy: 0.5014 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0370 - accuracy: 0.4976 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 00007: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [1, 5, 6, 3, 6]\n",
            "CNN Type 2\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 13s 101ms/step - loss: 1.1807 - accuracy: 0.2924 - val_loss: 1.0514 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0957 - accuracy: 0.3878 - val_loss: 1.0315 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0635 - accuracy: 0.4830 - val_loss: 1.0287 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0499 - accuracy: 0.4899 - val_loss: 1.0272 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0484 - accuracy: 0.4862 - val_loss: 1.0268 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0473 - accuracy: 0.4986 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0467 - accuracy: 0.4837 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0462 - accuracy: 0.4898 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0290 - accuracy: 0.5123 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0391 - accuracy: 0.4948 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0371 - accuracy: 0.4960 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0408 - accuracy: 0.4939 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0330 - accuracy: 0.5076 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 12s 99ms/step - loss: 1.0348 - accuracy: 0.5000 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 00014: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 5, 6, 3, 3]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 20ms/step - loss: 1.1862 - accuracy: 0.3227 - val_loss: 1.0461 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0724 - accuracy: 0.4360 - val_loss: 1.0278 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0713 - accuracy: 0.4679 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0575 - accuracy: 0.4617 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0569 - accuracy: 0.4701 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0374 - accuracy: 0.4994 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0517 - accuracy: 0.4837 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0507 - accuracy: 0.4804 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0422 - accuracy: 0.4854 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0440 - accuracy: 0.4898 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 00010: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 1, 4, 6, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 6s 42ms/step - loss: 1.0501 - accuracy: 0.4697 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0472 - accuracy: 0.4790 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0390 - accuracy: 0.4896 - val_loss: 1.0262 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 42ms/step - loss: 1.0386 - accuracy: 0.4842 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0397 - accuracy: 0.4873 - val_loss: 1.0262 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0385 - accuracy: 0.4833 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0539 - accuracy: 0.4626 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0503 - accuracy: 0.4837 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 00008: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 1, 6, 4, 5]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 21ms/step - loss: 1.1687 - accuracy: 0.3827 - val_loss: 1.0344 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.1181 - accuracy: 0.4262 - val_loss: 1.0382 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0774 - accuracy: 0.4463 - val_loss: 1.0277 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0690 - accuracy: 0.4605 - val_loss: 1.0299 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0461 - accuracy: 0.4690 - val_loss: 1.0259 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0622 - accuracy: 0.4749 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0461 - accuracy: 0.4843 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0369 - accuracy: 0.5041 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0360 - accuracy: 0.5065 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0403 - accuracy: 0.4987 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0334 - accuracy: 0.5079 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0410 - accuracy: 0.4883 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0279 - accuracy: 0.5082 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 00013: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 4, 6, 3, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.1086 - accuracy: 0.4035 - val_loss: 1.0280 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0475 - accuracy: 0.4925 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0554 - accuracy: 0.4704 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0482 - accuracy: 0.4799 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0521 - accuracy: 0.4896 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0325 - accuracy: 0.5063 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0323 - accuracy: 0.5185 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0256 - accuracy: 0.5124 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0395 - accuracy: 0.4972 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0453 - accuracy: 0.4935 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0445 - accuracy: 0.4927 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 00011: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "1  \t13    \t0.387976\t0.543005\n",
            "Training: [5, 1, 2, 1, 6]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 19ms/step - loss: 1.3049 - accuracy: 0.4401 - val_loss: 1.0645 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.2186 - accuracy: 0.4581 - val_loss: 1.0274 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.1728 - accuracy: 0.4526 - val_loss: 1.0350 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.1374 - accuracy: 0.4327 - val_loss: 1.0346 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0908 - accuracy: 0.4468 - val_loss: 1.0265 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0721 - accuracy: 0.4598 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0633 - accuracy: 0.4716 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0476 - accuracy: 0.4831 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0414 - accuracy: 0.4920 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0358 - accuracy: 0.4959 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0422 - accuracy: 0.4804 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0221 - accuracy: 0.5119 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0293 - accuracy: 0.5146 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0370 - accuracy: 0.4896 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0338 - accuracy: 0.4989 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0344 - accuracy: 0.4964 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 00016: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 4, 1, 3, 6]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 20ms/step - loss: 1.1999 - accuracy: 0.4134 - val_loss: 1.0647 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0878 - accuracy: 0.4414 - val_loss: 1.0295 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0687 - accuracy: 0.4662 - val_loss: 1.0326 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0507 - accuracy: 0.4672 - val_loss: 1.0363 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0506 - accuracy: 0.4917 - val_loss: 1.0263 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0546 - accuracy: 0.4788 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0419 - accuracy: 0.4891 - val_loss: 1.0261 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0427 - accuracy: 0.4969 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0387 - accuracy: 0.4800 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0389 - accuracy: 0.5006 - val_loss: 1.0239 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0388 - accuracy: 0.4885 - val_loss: 1.0239 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0393 - accuracy: 0.4885 - val_loss: 1.0240 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0412 - accuracy: 0.4824 - val_loss: 1.0231 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0433 - accuracy: 0.4910 - val_loss: 1.0235 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0307 - accuracy: 0.4994 - val_loss: 1.0229 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0296 - accuracy: 0.5052 - val_loss: 1.0223 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0291 - accuracy: 0.5005 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0323 - accuracy: 0.4991 - val_loss: 1.0225 - val_accuracy: 0.5039\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0458 - accuracy: 0.4722 - val_loss: 1.0224 - val_accuracy: 0.5039\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0301 - accuracy: 0.5007 - val_loss: 1.0217 - val_accuracy: 0.5039\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0300 - accuracy: 0.5066 - val_loss: 1.0212 - val_accuracy: 0.5039\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0402 - accuracy: 0.4844 - val_loss: 1.0214 - val_accuracy: 0.5039\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0377 - accuracy: 0.4903 - val_loss: 1.0203 - val_accuracy: 0.5039\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0345 - accuracy: 0.4968 - val_loss: 1.0210 - val_accuracy: 0.5039\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0400 - accuracy: 0.4909 - val_loss: 1.0215 - val_accuracy: 0.5039\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0330 - accuracy: 0.4909 - val_loss: 1.0198 - val_accuracy: 0.5039\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0210 - accuracy: 0.5136 - val_loss: 1.0225 - val_accuracy: 0.5039\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0378 - accuracy: 0.4872 - val_loss: 1.0183 - val_accuracy: 0.5039\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0339 - accuracy: 0.4923 - val_loss: 1.0179 - val_accuracy: 0.5039\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0294 - accuracy: 0.4984 - val_loss: 1.0175 - val_accuracy: 0.5039\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0328 - accuracy: 0.4922 - val_loss: 1.0189 - val_accuracy: 0.5039\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0230 - accuracy: 0.5042 - val_loss: 1.0149 - val_accuracy: 0.5039\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0395 - accuracy: 0.4821 - val_loss: 1.0135 - val_accuracy: 0.5039\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0298 - accuracy: 0.4921 - val_loss: 1.0140 - val_accuracy: 0.5039\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0318 - accuracy: 0.4907 - val_loss: 1.0121 - val_accuracy: 0.5039\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0249 - accuracy: 0.4952 - val_loss: 1.0096 - val_accuracy: 0.5039\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0208 - accuracy: 0.5023 - val_loss: 1.0049 - val_accuracy: 0.5039\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0244 - accuracy: 0.4951 - val_loss: 1.0043 - val_accuracy: 0.5039\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0144 - accuracy: 0.4961 - val_loss: 0.9992 - val_accuracy: 0.5039\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0081 - accuracy: 0.5042 - val_loss: 0.9921 - val_accuracy: 0.5039\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0050 - accuracy: 0.5022 - val_loss: 0.9862 - val_accuracy: 0.5039\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0082 - accuracy: 0.4969 - val_loss: 0.9801 - val_accuracy: 0.5039\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0033 - accuracy: 0.5086 - val_loss: 0.9855 - val_accuracy: 0.5039\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0108 - accuracy: 0.4834 - val_loss: 0.9683 - val_accuracy: 0.5039\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0086 - accuracy: 0.4773 - val_loss: 0.9678 - val_accuracy: 0.5039\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9913 - accuracy: 0.4910 - val_loss: 0.9889 - val_accuracy: 0.5031\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9949 - accuracy: 0.4896 - val_loss: 0.9422 - val_accuracy: 0.5039\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9879 - accuracy: 0.5015 - val_loss: 0.9539 - val_accuracy: 0.5039\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9917 - accuracy: 0.4765 - val_loss: 0.9304 - val_accuracy: 0.5063\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9751 - accuracy: 0.4944 - val_loss: 0.9540 - val_accuracy: 0.5039\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9840 - accuracy: 0.4904 - val_loss: 0.9769 - val_accuracy: 0.5039\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9810 - accuracy: 0.5003 - val_loss: 0.9146 - val_accuracy: 0.5031\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9791 - accuracy: 0.4948 - val_loss: 0.9088 - val_accuracy: 0.5094\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9638 - accuracy: 0.5038 - val_loss: 0.9261 - val_accuracy: 0.5063\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9686 - accuracy: 0.4930 - val_loss: 0.9287 - val_accuracy: 0.5063\n",
            "Epoch 56/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9677 - accuracy: 0.4914 - val_loss: 0.9810 - val_accuracy: 0.5047\n",
            "Epoch 57/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9526 - accuracy: 0.5038 - val_loss: 0.9139 - val_accuracy: 0.5071\n",
            "Epoch 00057: early stopping\n",
            "F1 Score: 0.3791991995290184\n",
            "\n",
            "Training: [3, 5, 3, 5, 5]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.1330 - accuracy: 0.3808 - val_loss: 1.0340 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0813 - accuracy: 0.4528 - val_loss: 1.0272 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0864 - accuracy: 0.4340 - val_loss: 1.0262 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0691 - accuracy: 0.4481 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0720 - accuracy: 0.4568 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0450 - accuracy: 0.4857 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0492 - accuracy: 0.4762 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0655 - accuracy: 0.4673 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0591 - accuracy: 0.4812 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0464 - accuracy: 0.4875 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0413 - accuracy: 0.4863 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0367 - accuracy: 0.4948 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0426 - accuracy: 0.4909 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0361 - accuracy: 0.4957 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0299 - accuracy: 0.4976 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0408 - accuracy: 0.4982 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0387 - accuracy: 0.4998 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 00017: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 4, 4, 1, 2]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 20ms/step - loss: 1.1939 - accuracy: 0.4167 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0913 - accuracy: 0.4552 - val_loss: 1.0277 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0924 - accuracy: 0.4431 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0607 - accuracy: 0.4783 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0658 - accuracy: 0.4546 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0689 - accuracy: 0.4534 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0621 - accuracy: 0.4755 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 00007: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 1, 6, 1, 6]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.4389 - accuracy: 0.3599 - val_loss: 1.1808 - val_accuracy: 0.3038\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.3851 - accuracy: 0.3258 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1822 - accuracy: 0.3909 - val_loss: 1.0281 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.1394 - accuracy: 0.4170 - val_loss: 1.0285 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.1107 - accuracy: 0.4450 - val_loss: 1.0285 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0978 - accuracy: 0.4605 - val_loss: 1.0265 - val_accuracy: 0.5039\n",
            "Epoch 00006: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 4, 6, 3, 5]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.1191 - accuracy: 0.3938 - val_loss: 1.0278 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0829 - accuracy: 0.4259 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0561 - accuracy: 0.4619 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0516 - accuracy: 0.4652 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0443 - accuracy: 0.4782 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0473 - accuracy: 0.4950 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0425 - accuracy: 0.4965 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0367 - accuracy: 0.5011 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0350 - accuracy: 0.4983 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0309 - accuracy: 0.5038 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0346 - accuracy: 0.5008 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 00011: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [3, 4, 1, 3, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0768 - accuracy: 0.4387 - val_loss: 1.0239 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0523 - accuracy: 0.4693 - val_loss: 1.0188 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0454 - accuracy: 0.4606 - val_loss: 0.9027 - val_accuracy: 0.5510\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.9126 - accuracy: 0.5609 - val_loss: 0.7615 - val_accuracy: 0.6546\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7856 - accuracy: 0.6376 - val_loss: 0.7980 - val_accuracy: 0.5981\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7321 - accuracy: 0.6585 - val_loss: 0.6600 - val_accuracy: 0.6515\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7082 - accuracy: 0.6567 - val_loss: 0.6337 - val_accuracy: 0.6711\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6950 - accuracy: 0.6660 - val_loss: 0.6051 - val_accuracy: 0.7096\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6720 - accuracy: 0.6718 - val_loss: 0.5866 - val_accuracy: 0.7221\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6608 - accuracy: 0.6880 - val_loss: 0.5658 - val_accuracy: 0.7253\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6551 - accuracy: 0.6769 - val_loss: 0.5799 - val_accuracy: 0.7473\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6312 - accuracy: 0.7123 - val_loss: 0.5646 - val_accuracy: 0.7072\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5996 - accuracy: 0.7253 - val_loss: 0.5422 - val_accuracy: 0.7661\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6204 - accuracy: 0.7140 - val_loss: 0.5114 - val_accuracy: 0.7889\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5838 - accuracy: 0.7366 - val_loss: 0.4870 - val_accuracy: 0.7998\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5874 - accuracy: 0.7353 - val_loss: 0.4938 - val_accuracy: 0.8014\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5676 - accuracy: 0.7680 - val_loss: 0.4719 - val_accuracy: 0.8053\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5533 - accuracy: 0.7689 - val_loss: 0.4636 - val_accuracy: 0.7967\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5629 - accuracy: 0.7545 - val_loss: 0.4534 - val_accuracy: 0.8108\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5374 - accuracy: 0.7706 - val_loss: 0.4558 - val_accuracy: 0.8014\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5323 - accuracy: 0.7708 - val_loss: 0.4496 - val_accuracy: 0.8046\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5241 - accuracy: 0.7784 - val_loss: 0.4444 - val_accuracy: 0.8053\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5183 - accuracy: 0.7802 - val_loss: 0.5778 - val_accuracy: 0.7402\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5087 - accuracy: 0.7900 - val_loss: 0.4705 - val_accuracy: 0.7983\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5131 - accuracy: 0.7817 - val_loss: 0.4490 - val_accuracy: 0.8093\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5049 - accuracy: 0.8018 - val_loss: 0.4366 - val_accuracy: 0.8140\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5107 - accuracy: 0.7833 - val_loss: 0.4346 - val_accuracy: 0.8108\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4832 - accuracy: 0.8090 - val_loss: 0.4370 - val_accuracy: 0.8093\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4707 - accuracy: 0.8098 - val_loss: 0.4318 - val_accuracy: 0.8132\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5010 - accuracy: 0.7944 - val_loss: 0.4796 - val_accuracy: 0.7951\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4976 - accuracy: 0.7952 - val_loss: 0.4422 - val_accuracy: 0.8108\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4694 - accuracy: 0.8117 - val_loss: 0.4435 - val_accuracy: 0.8108\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4860 - accuracy: 0.7980 - val_loss: 0.5208 - val_accuracy: 0.7771\n",
            "Epoch 00033: early stopping\n",
            "F1 Score: 0.8117603971082942\n",
            "\n",
            "2  \t14    \t0.444099\t0.81176 \n",
            "Training: [3, 1, 1, 3, 5]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.3021 - accuracy: 0.3475 - val_loss: 1.0350 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1587 - accuracy: 0.4154 - val_loss: 1.0162 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1329 - accuracy: 0.4112 - val_loss: 1.0286 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0701 - accuracy: 0.4636 - val_loss: 0.9478 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.9934 - accuracy: 0.4956 - val_loss: 0.8272 - val_accuracy: 0.6484\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.9211 - accuracy: 0.5471 - val_loss: 0.7483 - val_accuracy: 0.6531\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.8675 - accuracy: 0.5591 - val_loss: 0.6958 - val_accuracy: 0.6468\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.8060 - accuracy: 0.6020 - val_loss: 0.6674 - val_accuracy: 0.6444\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7678 - accuracy: 0.6091 - val_loss: 0.6390 - val_accuracy: 0.6554\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7591 - accuracy: 0.6030 - val_loss: 0.6332 - val_accuracy: 0.6499\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7298 - accuracy: 0.6241 - val_loss: 0.6265 - val_accuracy: 0.6546\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7451 - accuracy: 0.6186 - val_loss: 0.6209 - val_accuracy: 0.6554\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7356 - accuracy: 0.6171 - val_loss: 0.6201 - val_accuracy: 0.6499\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7148 - accuracy: 0.6380 - val_loss: 0.6207 - val_accuracy: 0.6538\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7365 - accuracy: 0.6093 - val_loss: 0.6212 - val_accuracy: 0.6546\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7008 - accuracy: 0.6410 - val_loss: 0.6181 - val_accuracy: 0.6484\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6923 - accuracy: 0.6384 - val_loss: 0.6169 - val_accuracy: 0.6538\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6965 - accuracy: 0.6359 - val_loss: 0.6105 - val_accuracy: 0.6499\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7045 - accuracy: 0.6359 - val_loss: 0.6114 - val_accuracy: 0.6515\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6788 - accuracy: 0.6440 - val_loss: 0.6073 - val_accuracy: 0.6546\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6874 - accuracy: 0.6307 - val_loss: 0.6027 - val_accuracy: 0.6570\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6774 - accuracy: 0.6492 - val_loss: 0.6073 - val_accuracy: 0.6641\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7055 - accuracy: 0.6181 - val_loss: 0.5943 - val_accuracy: 0.6538\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6708 - accuracy: 0.6573 - val_loss: 0.5964 - val_accuracy: 0.6515\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6825 - accuracy: 0.6669 - val_loss: 0.5821 - val_accuracy: 0.6538\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6788 - accuracy: 0.6499 - val_loss: 0.8156 - val_accuracy: 0.5526\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7103 - accuracy: 0.6319 - val_loss: 0.5863 - val_accuracy: 0.6970\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6585 - accuracy: 0.6784 - val_loss: 0.5659 - val_accuracy: 0.7088\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6700 - accuracy: 0.6673 - val_loss: 0.5873 - val_accuracy: 0.7896\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6542 - accuracy: 0.6774 - val_loss: 0.5556 - val_accuracy: 0.7857\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6497 - accuracy: 0.6687 - val_loss: 0.5530 - val_accuracy: 0.7637\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6575 - accuracy: 0.6657 - val_loss: 0.5391 - val_accuracy: 0.7849\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6475 - accuracy: 0.6720 - val_loss: 0.5360 - val_accuracy: 0.7943\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6296 - accuracy: 0.6894 - val_loss: 0.5651 - val_accuracy: 0.7590\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6251 - accuracy: 0.6762 - val_loss: 0.5384 - val_accuracy: 0.7881\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6315 - accuracy: 0.6877 - val_loss: 0.5318 - val_accuracy: 0.7896\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6419 - accuracy: 0.6911 - val_loss: 0.5206 - val_accuracy: 0.7983\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6170 - accuracy: 0.6980 - val_loss: 0.5014 - val_accuracy: 0.7998\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6082 - accuracy: 0.7056 - val_loss: 0.5429 - val_accuracy: 0.7504\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6385 - accuracy: 0.6862 - val_loss: 0.4914 - val_accuracy: 0.8030\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6251 - accuracy: 0.7047 - val_loss: 0.5480 - val_accuracy: 0.7794\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6424 - accuracy: 0.6585 - val_loss: 0.4875 - val_accuracy: 0.8014\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6318 - accuracy: 0.6999 - val_loss: 0.4777 - val_accuracy: 0.8077\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6013 - accuracy: 0.7094 - val_loss: 0.4796 - val_accuracy: 0.8038\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6052 - accuracy: 0.7040 - val_loss: 0.4677 - val_accuracy: 0.8132\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6538 - accuracy: 0.6880 - val_loss: 0.4676 - val_accuracy: 0.8108\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5818 - accuracy: 0.7269 - val_loss: 0.4711 - val_accuracy: 0.8053\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6064 - accuracy: 0.7096 - val_loss: 0.4875 - val_accuracy: 0.7991\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6095 - accuracy: 0.7005 - val_loss: 0.4843 - val_accuracy: 0.8022\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6005 - accuracy: 0.7089 - val_loss: 0.4581 - val_accuracy: 0.8100\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5790 - accuracy: 0.7279 - val_loss: 0.4740 - val_accuracy: 0.8046\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5619 - accuracy: 0.7368 - val_loss: 0.4684 - val_accuracy: 0.8046\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5958 - accuracy: 0.7182 - val_loss: 0.4652 - val_accuracy: 0.8046\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5815 - accuracy: 0.7247 - val_loss: 0.4552 - val_accuracy: 0.8124\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5677 - accuracy: 0.7531 - val_loss: 0.4662 - val_accuracy: 0.8014\n",
            "Epoch 56/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5972 - accuracy: 0.7289 - val_loss: 0.4531 - val_accuracy: 0.8085\n",
            "Epoch 57/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5675 - accuracy: 0.7416 - val_loss: 0.4535 - val_accuracy: 0.8069\n",
            "Epoch 58/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5721 - accuracy: 0.7379 - val_loss: 0.4591 - val_accuracy: 0.8061\n",
            "Epoch 59/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5912 - accuracy: 0.7293 - val_loss: 0.4777 - val_accuracy: 0.8053\n",
            "Epoch 60/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5876 - accuracy: 0.7499 - val_loss: 0.4513 - val_accuracy: 0.8100\n",
            "Epoch 61/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5619 - accuracy: 0.7675 - val_loss: 0.4450 - val_accuracy: 0.8155\n",
            "Epoch 62/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5450 - accuracy: 0.7756 - val_loss: 0.4450 - val_accuracy: 0.8124\n",
            "Epoch 63/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5528 - accuracy: 0.7800 - val_loss: 0.4377 - val_accuracy: 0.8140\n",
            "Epoch 64/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5603 - accuracy: 0.7607 - val_loss: 0.4426 - val_accuracy: 0.8155\n",
            "Epoch 65/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5500 - accuracy: 0.7722 - val_loss: 0.4792 - val_accuracy: 0.7943\n",
            "Epoch 66/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5650 - accuracy: 0.7530 - val_loss: 0.4462 - val_accuracy: 0.8108\n",
            "Epoch 67/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5525 - accuracy: 0.7693 - val_loss: 0.4422 - val_accuracy: 0.8155\n",
            "Epoch 00067: early stopping\n",
            "F1 Score: 0.8130561051200045\n",
            "\n",
            "Training: [6, 4, 6, 1, 6]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.3079 - accuracy: 0.4305 - val_loss: 1.0958 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.2293 - accuracy: 0.4218 - val_loss: 1.0415 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.1641 - accuracy: 0.4064 - val_loss: 1.0326 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1054 - accuracy: 0.4418 - val_loss: 1.0286 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0821 - accuracy: 0.4600 - val_loss: 1.0270 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0821 - accuracy: 0.4332 - val_loss: 1.0263 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0541 - accuracy: 0.4611 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0527 - accuracy: 0.4692 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0533 - accuracy: 0.4545 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0527 - accuracy: 0.4705 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0375 - accuracy: 0.4901 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0467 - accuracy: 0.4881 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0334 - accuracy: 0.5039 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0324 - accuracy: 0.5004 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0389 - accuracy: 0.4947 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0289 - accuracy: 0.5020 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0364 - accuracy: 0.4968 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0340 - accuracy: 0.4927 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 00018: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [3, 1, 1, 3, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0928 - accuracy: 0.4476 - val_loss: 1.0268 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0751 - accuracy: 0.4580 - val_loss: 1.0034 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0217 - accuracy: 0.5015 - val_loss: 0.8513 - val_accuracy: 0.5659\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.8650 - accuracy: 0.5924 - val_loss: 0.7145 - val_accuracy: 0.6680\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7590 - accuracy: 0.6576 - val_loss: 0.6526 - val_accuracy: 0.6829\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7214 - accuracy: 0.6609 - val_loss: 0.6305 - val_accuracy: 0.7488\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6822 - accuracy: 0.6866 - val_loss: 0.5903 - val_accuracy: 0.7143\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6625 - accuracy: 0.6961 - val_loss: 0.5693 - val_accuracy: 0.7276\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6489 - accuracy: 0.6957 - val_loss: 0.5838 - val_accuracy: 0.6947\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6349 - accuracy: 0.7242 - val_loss: 0.5239 - val_accuracy: 0.7928\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6203 - accuracy: 0.7201 - val_loss: 0.5801 - val_accuracy: 0.6892\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5785 - accuracy: 0.7459 - val_loss: 0.5018 - val_accuracy: 0.7928\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5755 - accuracy: 0.7547 - val_loss: 0.4912 - val_accuracy: 0.7889\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5558 - accuracy: 0.7522 - val_loss: 0.5262 - val_accuracy: 0.7535\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5729 - accuracy: 0.7557 - val_loss: 0.4630 - val_accuracy: 0.8108\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5609 - accuracy: 0.7601 - val_loss: 0.4692 - val_accuracy: 0.7983\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5493 - accuracy: 0.7631 - val_loss: 0.4680 - val_accuracy: 0.7991\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5402 - accuracy: 0.7664 - val_loss: 0.4656 - val_accuracy: 0.8085\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5375 - accuracy: 0.7835 - val_loss: 0.4449 - val_accuracy: 0.8053\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5356 - accuracy: 0.7782 - val_loss: 0.4407 - val_accuracy: 0.8132\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5110 - accuracy: 0.7942 - val_loss: 0.4399 - val_accuracy: 0.8100\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5164 - accuracy: 0.7949 - val_loss: 0.4564 - val_accuracy: 0.8108\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4938 - accuracy: 0.8036 - val_loss: 0.4342 - val_accuracy: 0.8085\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4810 - accuracy: 0.8128 - val_loss: 0.4379 - val_accuracy: 0.8061\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5302 - accuracy: 0.7764 - val_loss: 0.4408 - val_accuracy: 0.8140\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4751 - accuracy: 0.8020 - val_loss: 0.5214 - val_accuracy: 0.7567\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5008 - accuracy: 0.7928 - val_loss: 0.4296 - val_accuracy: 0.8140\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4861 - accuracy: 0.8034 - val_loss: 0.4450 - val_accuracy: 0.8085\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4854 - accuracy: 0.8063 - val_loss: 0.4582 - val_accuracy: 0.8022\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4747 - accuracy: 0.8115 - val_loss: 0.4276 - val_accuracy: 0.8155\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4622 - accuracy: 0.8134 - val_loss: 0.4236 - val_accuracy: 0.8155\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4725 - accuracy: 0.8054 - val_loss: 0.4392 - val_accuracy: 0.8069\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4712 - accuracy: 0.8111 - val_loss: 0.5436 - val_accuracy: 0.7661\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4715 - accuracy: 0.8123 - val_loss: 0.4312 - val_accuracy: 0.8163\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4467 - accuracy: 0.8188 - val_loss: 0.5428 - val_accuracy: 0.7575\n",
            "Epoch 00035: early stopping\n",
            "F1 Score: 0.8144901340470023\n",
            "\n",
            "Training: [3, 4, 1, 3, 6]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.1886 - accuracy: 0.3521 - val_loss: 1.0462 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1102 - accuracy: 0.4447 - val_loss: 1.0435 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0734 - accuracy: 0.4658 - val_loss: 1.0319 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0464 - accuracy: 0.4871 - val_loss: 1.0291 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0469 - accuracy: 0.4925 - val_loss: 1.0271 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0449 - accuracy: 0.4947 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0309 - accuracy: 0.5114 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0310 - accuracy: 0.5029 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0251 - accuracy: 0.5132 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0344 - accuracy: 0.5049 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0271 - accuracy: 0.5099 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0407 - accuracy: 0.4897 - val_loss: 1.0242 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0261 - accuracy: 0.5053 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0298 - accuracy: 0.5029 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0286 - accuracy: 0.5057 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0327 - accuracy: 0.4950 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 00016: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 1, 6, 1, 5]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 19ms/step - loss: 1.2220 - accuracy: 0.3966 - val_loss: 1.0299 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.1759 - accuracy: 0.4074 - val_loss: 1.0342 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.1041 - accuracy: 0.4493 - val_loss: 1.0238 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0569 - accuracy: 0.4477 - val_loss: 0.9952 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0323 - accuracy: 0.4715 - val_loss: 0.9593 - val_accuracy: 0.5149\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9904 - accuracy: 0.5021 - val_loss: 0.9423 - val_accuracy: 0.5416\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9781 - accuracy: 0.4890 - val_loss: 0.8821 - val_accuracy: 0.5518\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9240 - accuracy: 0.5221 - val_loss: 0.8672 - val_accuracy: 0.5683\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8909 - accuracy: 0.5339 - val_loss: 0.7592 - val_accuracy: 0.6366\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.8008 - accuracy: 0.5897 - val_loss: 0.6758 - val_accuracy: 0.6484\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7535 - accuracy: 0.6215 - val_loss: 0.6624 - val_accuracy: 0.6711\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7194 - accuracy: 0.6199 - val_loss: 0.6478 - val_accuracy: 0.6523\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7172 - accuracy: 0.6305 - val_loss: 0.6488 - val_accuracy: 0.6468\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7057 - accuracy: 0.6409 - val_loss: 0.6434 - val_accuracy: 0.6562\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6878 - accuracy: 0.6449 - val_loss: 0.6451 - val_accuracy: 0.6538\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.7004 - accuracy: 0.6251 - val_loss: 0.6320 - val_accuracy: 0.6515\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6997 - accuracy: 0.6343 - val_loss: 0.6499 - val_accuracy: 0.6468\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6719 - accuracy: 0.6439 - val_loss: 0.6349 - val_accuracy: 0.6538\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6794 - accuracy: 0.6329 - val_loss: 0.6496 - val_accuracy: 0.6491\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.6788 - accuracy: 0.6400 - val_loss: 0.6658 - val_accuracy: 0.6350\n",
            "Epoch 00020: early stopping\n",
            "F1 Score: 0.5385912725067458\n",
            "\n",
            "Training: [5, 4, 1, 3, 1]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 20ms/step - loss: 1.0665 - accuracy: 0.4688 - val_loss: 1.0422 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0346 - accuracy: 0.4950 - val_loss: 1.0340 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0536 - accuracy: 0.4694 - val_loss: 1.0243 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0335 - accuracy: 0.4965 - val_loss: 1.0277 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0487 - accuracy: 0.4828 - val_loss: 1.0199 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0304 - accuracy: 0.4939 - val_loss: 1.0227 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0390 - accuracy: 0.4895 - val_loss: 1.0240 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0369 - accuracy: 0.4884 - val_loss: 1.0230 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0330 - accuracy: 0.4979 - val_loss: 1.0170 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0294 - accuracy: 0.4944 - val_loss: 1.0127 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0175 - accuracy: 0.4968 - val_loss: 1.0076 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0229 - accuracy: 0.4916 - val_loss: 1.0047 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0137 - accuracy: 0.4964 - val_loss: 0.9961 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0113 - accuracy: 0.4925 - val_loss: 0.9968 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0027 - accuracy: 0.4930 - val_loss: 1.0316 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0019 - accuracy: 0.4803 - val_loss: 1.0122 - val_accuracy: 0.4396\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9917 - accuracy: 0.4861 - val_loss: 0.9537 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9700 - accuracy: 0.4972 - val_loss: 0.9409 - val_accuracy: 0.5047\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9744 - accuracy: 0.4860 - val_loss: 0.9400 - val_accuracy: 0.4976\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 0.9592 - accuracy: 0.4886 - val_loss: 0.9393 - val_accuracy: 0.5047\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9509 - accuracy: 0.4986 - val_loss: 0.9852 - val_accuracy: 0.5039\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9428 - accuracy: 0.4985 - val_loss: 1.0028 - val_accuracy: 0.4655\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9369 - accuracy: 0.5028 - val_loss: 0.8970 - val_accuracy: 0.4976\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9271 - accuracy: 0.5045 - val_loss: 0.8894 - val_accuracy: 0.5235\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9122 - accuracy: 0.5122 - val_loss: 0.9258 - val_accuracy: 0.5031\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9265 - accuracy: 0.4937 - val_loss: 0.8838 - val_accuracy: 0.5361\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9169 - accuracy: 0.4964 - val_loss: 0.9673 - val_accuracy: 0.4772\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9192 - accuracy: 0.5127 - val_loss: 0.9071 - val_accuracy: 0.5000\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9104 - accuracy: 0.5034 - val_loss: 0.9050 - val_accuracy: 0.5071\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9042 - accuracy: 0.5087 - val_loss: 0.8966 - val_accuracy: 0.5078\n",
            "Epoch 00030: early stopping\n",
            "F1 Score: 0.4383486851865126\n",
            "\n",
            "Training: [3, 1, 4, 6, 5]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 6s 43ms/step - loss: 1.2081 - accuracy: 0.4007 - val_loss: 1.0386 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.1332 - accuracy: 0.4379 - val_loss: 1.0407 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 42ms/step - loss: 1.1132 - accuracy: 0.4412 - val_loss: 1.0265 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.1015 - accuracy: 0.4430 - val_loss: 1.0272 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0775 - accuracy: 0.4611 - val_loss: 1.0257 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0667 - accuracy: 0.4608 - val_loss: 1.0262 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 42ms/step - loss: 1.0636 - accuracy: 0.4698 - val_loss: 1.0264 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 42ms/step - loss: 1.0508 - accuracy: 0.4740 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0506 - accuracy: 0.4598 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 41ms/step - loss: 1.0541 - accuracy: 0.4769 - val_loss: 1.0259 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 42ms/step - loss: 1.0359 - accuracy: 0.4898 - val_loss: 1.0256 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 42ms/step - loss: 1.0365 - accuracy: 0.4968 - val_loss: 1.0253 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 42ms/step - loss: 1.0399 - accuracy: 0.4944 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 00013: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 1, 6, 4, 6]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 21ms/step - loss: 1.2594 - accuracy: 0.3905 - val_loss: 1.0623 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.2056 - accuracy: 0.3900 - val_loss: 1.0673 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.1540 - accuracy: 0.4144 - val_loss: 1.0415 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.1094 - accuracy: 0.4359 - val_loss: 1.0324 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0772 - accuracy: 0.4399 - val_loss: 1.0294 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0546 - accuracy: 0.4661 - val_loss: 1.0264 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0375 - accuracy: 0.4946 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0541 - accuracy: 0.4747 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0493 - accuracy: 0.4839 - val_loss: 1.0243 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0351 - accuracy: 0.4978 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0394 - accuracy: 0.4984 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0458 - accuracy: 0.4898 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0415 - accuracy: 0.4983 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 00013: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [5, 1, 1, 3, 6]\n",
            "CNN Type 3\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 3s 19ms/step - loss: 1.2848 - accuracy: 0.3658 - val_loss: 1.0342 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.1322 - accuracy: 0.4239 - val_loss: 1.0296 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0842 - accuracy: 0.4500 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0570 - accuracy: 0.4873 - val_loss: 1.0259 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0589 - accuracy: 0.4683 - val_loss: 1.0240 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0547 - accuracy: 0.4714 - val_loss: 1.0215 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0405 - accuracy: 0.4990 - val_loss: 1.0222 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0375 - accuracy: 0.4915 - val_loss: 1.0188 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0404 - accuracy: 0.4814 - val_loss: 1.0176 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0403 - accuracy: 0.4930 - val_loss: 1.0165 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0324 - accuracy: 0.5020 - val_loss: 1.0158 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0325 - accuracy: 0.4871 - val_loss: 1.0169 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 2s 19ms/step - loss: 1.0400 - accuracy: 0.4886 - val_loss: 1.0142 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0196 - accuracy: 0.5033 - val_loss: 1.0117 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0175 - accuracy: 0.5088 - val_loss: 1.0183 - val_accuracy: 0.5039\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0320 - accuracy: 0.4934 - val_loss: 1.0089 - val_accuracy: 0.5039\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0252 - accuracy: 0.5004 - val_loss: 1.0005 - val_accuracy: 0.5039\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0266 - accuracy: 0.4835 - val_loss: 0.9974 - val_accuracy: 0.5039\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0122 - accuracy: 0.5061 - val_loss: 0.9935 - val_accuracy: 0.5039\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0205 - accuracy: 0.4800 - val_loss: 0.9797 - val_accuracy: 0.5039\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0081 - accuracy: 0.4999 - val_loss: 0.9740 - val_accuracy: 0.5039\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0174 - accuracy: 0.4684 - val_loss: 0.9753 - val_accuracy: 0.5039\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0109 - accuracy: 0.4990 - val_loss: 0.9659 - val_accuracy: 0.5039\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9938 - accuracy: 0.4918 - val_loss: 0.9515 - val_accuracy: 0.5039\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9962 - accuracy: 0.4926 - val_loss: 0.9906 - val_accuracy: 0.4961\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 1.0016 - accuracy: 0.4868 - val_loss: 0.9359 - val_accuracy: 0.5039\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9891 - accuracy: 0.4856 - val_loss: 0.9358 - val_accuracy: 0.5031\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9812 - accuracy: 0.5020 - val_loss: 0.9803 - val_accuracy: 0.5173\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9952 - accuracy: 0.4758 - val_loss: 0.9245 - val_accuracy: 0.5047\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9828 - accuracy: 0.4889 - val_loss: 0.9663 - val_accuracy: 0.5039\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9726 - accuracy: 0.4962 - val_loss: 0.9336 - val_accuracy: 0.5243\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9671 - accuracy: 0.5083 - val_loss: 0.9649 - val_accuracy: 0.5063\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9645 - accuracy: 0.5005 - val_loss: 0.9003 - val_accuracy: 0.5235\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9581 - accuracy: 0.5152 - val_loss: 0.8965 - val_accuracy: 0.5102\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9770 - accuracy: 0.4932 - val_loss: 0.9655 - val_accuracy: 0.5047\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9570 - accuracy: 0.4935 - val_loss: 1.1457 - val_accuracy: 0.3995\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9694 - accuracy: 0.4961 - val_loss: 1.0648 - val_accuracy: 0.4309\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 2s 18ms/step - loss: 0.9687 - accuracy: 0.4818 - val_loss: 0.8974 - val_accuracy: 0.5024\n",
            "Epoch 00038: early stopping\n",
            "F1 Score: 0.39950234915849286\n",
            "\n",
            "Training: [4, 1, 6, 1, 2]\n",
            "CNN Type 2\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 13s 99ms/step - loss: 1.3332 - accuracy: 0.2870 - val_loss: 1.0252 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0944 - accuracy: 0.4261 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0868 - accuracy: 0.4351 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0525 - accuracy: 0.4857 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0660 - accuracy: 0.4589 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0568 - accuracy: 0.4852 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 12s 97ms/step - loss: 1.0412 - accuracy: 0.4890 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 00007: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "3  \t16    \t0.521613\t0.81449 \n",
            "Training: [3, 1, 6, 1, 5]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.5250 - accuracy: 0.3390 - val_loss: 1.0313 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.2292 - accuracy: 0.3927 - val_loss: 1.0333 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1447 - accuracy: 0.4319 - val_loss: 1.0309 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1529 - accuracy: 0.4209 - val_loss: 1.0305 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1136 - accuracy: 0.4311 - val_loss: 1.0258 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0736 - accuracy: 0.4455 - val_loss: 1.0268 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0673 - accuracy: 0.4601 - val_loss: 1.0734 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0829 - accuracy: 0.4065 - val_loss: 1.0260 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0516 - accuracy: 0.4801 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0508 - accuracy: 0.4940 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0401 - accuracy: 0.5048 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0461 - accuracy: 0.4899 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0359 - accuracy: 0.4999 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 00013: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [3, 4, 1, 3, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0270 - accuracy: 0.5016 - val_loss: 0.8782 - val_accuracy: 0.5730\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.8610 - accuracy: 0.5993 - val_loss: 0.7100 - val_accuracy: 0.6578\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7094 - accuracy: 0.6486 - val_loss: 0.6564 - val_accuracy: 0.6499\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6842 - accuracy: 0.6491 - val_loss: 0.6258 - val_accuracy: 0.6625\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6340 - accuracy: 0.6767 - val_loss: 0.6028 - val_accuracy: 0.7002\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6187 - accuracy: 0.6973 - val_loss: 0.5833 - val_accuracy: 0.7049\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6024 - accuracy: 0.7124 - val_loss: 0.5457 - val_accuracy: 0.7724\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5840 - accuracy: 0.7215 - val_loss: 0.6014 - val_accuracy: 0.6978\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5628 - accuracy: 0.7363 - val_loss: 0.5090 - val_accuracy: 0.7810\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5495 - accuracy: 0.7453 - val_loss: 0.5088 - val_accuracy: 0.7575\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5413 - accuracy: 0.7532 - val_loss: 0.5199 - val_accuracy: 0.7253\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5117 - accuracy: 0.7767 - val_loss: 0.4896 - val_accuracy: 0.7896\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5125 - accuracy: 0.7726 - val_loss: 0.4774 - val_accuracy: 0.7889\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5032 - accuracy: 0.7869 - val_loss: 0.4790 - val_accuracy: 0.7739\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4650 - accuracy: 0.8039 - val_loss: 0.4788 - val_accuracy: 0.7998\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4633 - accuracy: 0.8058 - val_loss: 0.4503 - val_accuracy: 0.8038\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4499 - accuracy: 0.8031 - val_loss: 0.4386 - val_accuracy: 0.8030\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4341 - accuracy: 0.8160 - val_loss: 0.4372 - val_accuracy: 0.8100\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4356 - accuracy: 0.8131 - val_loss: 0.4325 - val_accuracy: 0.8053\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4364 - accuracy: 0.8099 - val_loss: 0.4416 - val_accuracy: 0.8014\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4076 - accuracy: 0.8338 - val_loss: 0.4374 - val_accuracy: 0.8069\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4251 - accuracy: 0.8234 - val_loss: 0.4327 - val_accuracy: 0.8108\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4154 - accuracy: 0.8244 - val_loss: 0.4220 - val_accuracy: 0.8210\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4049 - accuracy: 0.8306 - val_loss: 0.4213 - val_accuracy: 0.8053\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.3867 - accuracy: 0.8338 - val_loss: 0.4144 - val_accuracy: 0.8163\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.3803 - accuracy: 0.8382 - val_loss: 0.4293 - val_accuracy: 0.8014\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.3815 - accuracy: 0.8418 - val_loss: 0.4391 - val_accuracy: 0.8046\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.3757 - accuracy: 0.8457 - val_loss: 0.4157 - val_accuracy: 0.8203\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.3691 - accuracy: 0.8497 - val_loss: 0.4471 - val_accuracy: 0.7951\n",
            "Epoch 00029: early stopping\n",
            "F1 Score: 0.8157596053317105\n",
            "\n",
            "4  \t10    \t0.67632 \t0.81576 \n",
            "Training: [3, 1, 1, 3, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0792 - accuracy: 0.4695 - val_loss: 1.0353 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0555 - accuracy: 0.4658 - val_loss: 1.0252 - val_accuracy: 0.4074\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.9083 - accuracy: 0.5568 - val_loss: 0.7158 - val_accuracy: 0.7151\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7053 - accuracy: 0.7183 - val_loss: 0.6464 - val_accuracy: 0.7339\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6337 - accuracy: 0.7399 - val_loss: 0.5514 - val_accuracy: 0.7889\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5656 - accuracy: 0.7646 - val_loss: 0.5108 - val_accuracy: 0.8038\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5400 - accuracy: 0.7828 - val_loss: 0.4809 - val_accuracy: 0.8030\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5123 - accuracy: 0.7928 - val_loss: 0.5225 - val_accuracy: 0.7535\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4865 - accuracy: 0.8020 - val_loss: 0.4902 - val_accuracy: 0.7928\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4768 - accuracy: 0.8092 - val_loss: 0.4512 - val_accuracy: 0.8077\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4673 - accuracy: 0.8159 - val_loss: 0.6316 - val_accuracy: 0.7402\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5059 - accuracy: 0.7801 - val_loss: 0.4606 - val_accuracy: 0.8171\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4478 - accuracy: 0.8108 - val_loss: 0.4426 - val_accuracy: 0.8108\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4332 - accuracy: 0.8224 - val_loss: 0.4570 - val_accuracy: 0.8046\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4214 - accuracy: 0.8284 - val_loss: 0.4747 - val_accuracy: 0.7967\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4522 - accuracy: 0.8046 - val_loss: 0.4833 - val_accuracy: 0.7959\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4726 - accuracy: 0.7958 - val_loss: 0.4469 - val_accuracy: 0.8046\n",
            "Epoch 00017: early stopping\n",
            "F1 Score: 0.8100937620134256\n",
            "\n",
            "5  \t9     \t0.758614\t0.81576 \n",
            "Training: [3, 1, 1, 3, 6]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.3350 - accuracy: 0.3953 - val_loss: 1.0721 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1594 - accuracy: 0.4252 - val_loss: 1.0349 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1064 - accuracy: 0.4398 - val_loss: 1.0254 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0872 - accuracy: 0.4464 - val_loss: 1.0244 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0677 - accuracy: 0.4660 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0427 - accuracy: 0.4851 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0449 - accuracy: 0.4932 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0481 - accuracy: 0.4823 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 00008: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "6  \t17    \t0.762861\t0.81576 \n",
            "Training: [3, 1, 1, 2, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.1045 - accuracy: 0.3879 - val_loss: 1.0231 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0567 - accuracy: 0.4618 - val_loss: 1.0160 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0456 - accuracy: 0.4847 - val_loss: 1.0113 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.9684 - accuracy: 0.5200 - val_loss: 0.7681 - val_accuracy: 0.7206\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7927 - accuracy: 0.6303 - val_loss: 0.6938 - val_accuracy: 0.6586\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7290 - accuracy: 0.6542 - val_loss: 0.6790 - val_accuracy: 0.6491\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6946 - accuracy: 0.6659 - val_loss: 0.6328 - val_accuracy: 0.6586\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6761 - accuracy: 0.6640 - val_loss: 0.6208 - val_accuracy: 0.6656\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6605 - accuracy: 0.6747 - val_loss: 0.5978 - val_accuracy: 0.6578\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6648 - accuracy: 0.6698 - val_loss: 0.5851 - val_accuracy: 0.6954\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6413 - accuracy: 0.6925 - val_loss: 0.5697 - val_accuracy: 0.6994\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6292 - accuracy: 0.6997 - val_loss: 0.5502 - val_accuracy: 0.7543\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6362 - accuracy: 0.6885 - val_loss: 0.5325 - val_accuracy: 0.7559\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6209 - accuracy: 0.7077 - val_loss: 0.5312 - val_accuracy: 0.7622\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6222 - accuracy: 0.7131 - val_loss: 0.5084 - val_accuracy: 0.7896\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5901 - accuracy: 0.7209 - val_loss: 0.5014 - val_accuracy: 0.7802\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5876 - accuracy: 0.7346 - val_loss: 0.4942 - val_accuracy: 0.7857\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5606 - accuracy: 0.7524 - val_loss: 0.5090 - val_accuracy: 0.8022\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5664 - accuracy: 0.7513 - val_loss: 0.4701 - val_accuracy: 0.8100\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5644 - accuracy: 0.7469 - val_loss: 0.4896 - val_accuracy: 0.7936\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5430 - accuracy: 0.7586 - val_loss: 0.4518 - val_accuracy: 0.8124\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5516 - accuracy: 0.7653 - val_loss: 0.4578 - val_accuracy: 0.7951\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5300 - accuracy: 0.7575 - val_loss: 0.4937 - val_accuracy: 0.7732\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5402 - accuracy: 0.7652 - val_loss: 0.4423 - val_accuracy: 0.8108\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5152 - accuracy: 0.7759 - val_loss: 0.4434 - val_accuracy: 0.8077\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5125 - accuracy: 0.7798 - val_loss: 0.4568 - val_accuracy: 0.8006\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5092 - accuracy: 0.7718 - val_loss: 0.4431 - val_accuracy: 0.8100\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5012 - accuracy: 0.7857 - val_loss: 0.4291 - val_accuracy: 0.8163\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5003 - accuracy: 0.7789 - val_loss: 0.4314 - val_accuracy: 0.8085\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4915 - accuracy: 0.7853 - val_loss: 0.4566 - val_accuracy: 0.7943\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4667 - accuracy: 0.8085 - val_loss: 0.4369 - val_accuracy: 0.8053\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4903 - accuracy: 0.7925 - val_loss: 0.4299 - val_accuracy: 0.8093\n",
            "Epoch 00032: early stopping\n",
            "F1 Score: 0.8154615338614251\n",
            "\n",
            "Training: [6, 1, 1, 3, 5]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.2100 - accuracy: 0.3968 - val_loss: 1.1114 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1175 - accuracy: 0.4596 - val_loss: 1.0589 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.1128 - accuracy: 0.4381 - val_loss: 1.0316 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0776 - accuracy: 0.4709 - val_loss: 1.0336 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0789 - accuracy: 0.4418 - val_loss: 1.0290 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0492 - accuracy: 0.4703 - val_loss: 1.0213 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0449 - accuracy: 0.4849 - val_loss: 1.0197 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0455 - accuracy: 0.4750 - val_loss: 1.0173 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0345 - accuracy: 0.4752 - val_loss: 0.9899 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.9901 - accuracy: 0.4980 - val_loss: 0.8612 - val_accuracy: 0.5322\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.9076 - accuracy: 0.5327 - val_loss: 0.7646 - val_accuracy: 0.6499\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.8305 - accuracy: 0.5870 - val_loss: 0.7057 - val_accuracy: 0.6476\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7814 - accuracy: 0.6197 - val_loss: 0.6739 - val_accuracy: 0.6554\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7597 - accuracy: 0.6233 - val_loss: 0.6711 - val_accuracy: 0.6444\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7548 - accuracy: 0.6246 - val_loss: 0.6674 - val_accuracy: 0.6444\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7341 - accuracy: 0.6324 - val_loss: 0.6501 - val_accuracy: 0.6578\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7166 - accuracy: 0.6293 - val_loss: 0.6492 - val_accuracy: 0.6562\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7161 - accuracy: 0.6511 - val_loss: 0.6517 - val_accuracy: 0.6531\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7164 - accuracy: 0.6382 - val_loss: 0.6473 - val_accuracy: 0.6538\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7103 - accuracy: 0.6500 - val_loss: 0.6471 - val_accuracy: 0.6538\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7049 - accuracy: 0.6410 - val_loss: 0.6460 - val_accuracy: 0.6546\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7174 - accuracy: 0.6422 - val_loss: 0.6533 - val_accuracy: 0.6460\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7070 - accuracy: 0.6424 - val_loss: 0.6523 - val_accuracy: 0.6515\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6929 - accuracy: 0.6412 - val_loss: 0.6551 - val_accuracy: 0.6476\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7081 - accuracy: 0.6335 - val_loss: 0.6431 - val_accuracy: 0.6531\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6954 - accuracy: 0.6432 - val_loss: 0.6572 - val_accuracy: 0.6444\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7054 - accuracy: 0.6391 - val_loss: 0.6459 - val_accuracy: 0.6531\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7005 - accuracy: 0.6436 - val_loss: 0.6561 - val_accuracy: 0.6484\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6928 - accuracy: 0.6541 - val_loss: 0.6473 - val_accuracy: 0.6531\n",
            "Epoch 00029: early stopping\n",
            "F1 Score: 0.5395732832076976\n",
            "\n",
            "7  \t14    \t0.800212\t0.81576 \n",
            "8  \t14    \t0.814564\t0.81576 \n",
            "Training: [3, 4, 1, 2, 3]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.1106 - accuracy: 0.4568 - val_loss: 1.0220 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0884 - accuracy: 0.4547 - val_loss: 0.8973 - val_accuracy: 0.5322\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.8921 - accuracy: 0.5533 - val_loss: 0.6798 - val_accuracy: 0.6586\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.7338 - accuracy: 0.6497 - val_loss: 0.6482 - val_accuracy: 0.6586\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6908 - accuracy: 0.6578 - val_loss: 0.6377 - val_accuracy: 0.6546\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6882 - accuracy: 0.6605 - val_loss: 0.6283 - val_accuracy: 0.6554\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6893 - accuracy: 0.6462 - val_loss: 0.6190 - val_accuracy: 0.6601\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6710 - accuracy: 0.6474 - val_loss: 0.6102 - val_accuracy: 0.6617\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6599 - accuracy: 0.6711 - val_loss: 0.5956 - val_accuracy: 0.6570\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6194 - accuracy: 0.6984 - val_loss: 0.5905 - val_accuracy: 0.6601\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6390 - accuracy: 0.6848 - val_loss: 0.5775 - val_accuracy: 0.7009\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6441 - accuracy: 0.6770 - val_loss: 0.5814 - val_accuracy: 0.7606\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6253 - accuracy: 0.6949 - val_loss: 0.5497 - val_accuracy: 0.7127\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5952 - accuracy: 0.7198 - val_loss: 0.5293 - val_accuracy: 0.7182\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6004 - accuracy: 0.7187 - val_loss: 0.5421 - val_accuracy: 0.7159\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6042 - accuracy: 0.7285 - val_loss: 0.5054 - val_accuracy: 0.7951\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5924 - accuracy: 0.7153 - val_loss: 0.5216 - val_accuracy: 0.7755\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5841 - accuracy: 0.7238 - val_loss: 0.5102 - val_accuracy: 0.7567\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5645 - accuracy: 0.7401 - val_loss: 0.4709 - val_accuracy: 0.8038\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5498 - accuracy: 0.7525 - val_loss: 0.4749 - val_accuracy: 0.7951\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5658 - accuracy: 0.7405 - val_loss: 0.4653 - val_accuracy: 0.7967\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5350 - accuracy: 0.7693 - val_loss: 0.4728 - val_accuracy: 0.8030\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5328 - accuracy: 0.7694 - val_loss: 0.4630 - val_accuracy: 0.8046\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5201 - accuracy: 0.7827 - val_loss: 0.4474 - val_accuracy: 0.8069\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5183 - accuracy: 0.7788 - val_loss: 0.4620 - val_accuracy: 0.7967\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5326 - accuracy: 0.7667 - val_loss: 0.4451 - val_accuracy: 0.8046\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5115 - accuracy: 0.7771 - val_loss: 0.4651 - val_accuracy: 0.7928\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5103 - accuracy: 0.7768 - val_loss: 0.5641 - val_accuracy: 0.7159\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5316 - accuracy: 0.7622 - val_loss: 0.4820 - val_accuracy: 0.7975\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4887 - accuracy: 0.7946 - val_loss: 0.6057 - val_accuracy: 0.7159\n",
            "Epoch 00030: early stopping\n",
            "F1 Score: 0.8045709042447221\n",
            "\n",
            "Training: [3, 4, 1, 2, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0550 - accuracy: 0.4762 - val_loss: 0.9851 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.9282 - accuracy: 0.5482 - val_loss: 0.7212 - val_accuracy: 0.7025\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7250 - accuracy: 0.6866 - val_loss: 0.6451 - val_accuracy: 0.7049\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6677 - accuracy: 0.6843 - val_loss: 0.6498 - val_accuracy: 0.6523\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6289 - accuracy: 0.6974 - val_loss: 0.5703 - val_accuracy: 0.7206\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6060 - accuracy: 0.7166 - val_loss: 0.5468 - val_accuracy: 0.7661\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5621 - accuracy: 0.7486 - val_loss: 0.5200 - val_accuracy: 0.7810\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5343 - accuracy: 0.7774 - val_loss: 0.4897 - val_accuracy: 0.7928\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5176 - accuracy: 0.7764 - val_loss: 0.4935 - val_accuracy: 0.7716\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5134 - accuracy: 0.7769 - val_loss: 0.5250 - val_accuracy: 0.7873\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4967 - accuracy: 0.7967 - val_loss: 0.4611 - val_accuracy: 0.8030\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4641 - accuracy: 0.8156 - val_loss: 0.4558 - val_accuracy: 0.7912\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4539 - accuracy: 0.8172 - val_loss: 0.4778 - val_accuracy: 0.7998\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4560 - accuracy: 0.8111 - val_loss: 0.4396 - val_accuracy: 0.8038\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4328 - accuracy: 0.8231 - val_loss: 0.4365 - val_accuracy: 0.8085\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4312 - accuracy: 0.8152 - val_loss: 0.5774 - val_accuracy: 0.7268\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4380 - accuracy: 0.8184 - val_loss: 0.4351 - val_accuracy: 0.8046\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4159 - accuracy: 0.8291 - val_loss: 0.4505 - val_accuracy: 0.7849\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4192 - accuracy: 0.8190 - val_loss: 0.4328 - val_accuracy: 0.8046\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4210 - accuracy: 0.8143 - val_loss: 0.4665 - val_accuracy: 0.8014\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4109 - accuracy: 0.8330 - val_loss: 0.4344 - val_accuracy: 0.8124\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.3970 - accuracy: 0.8345 - val_loss: 0.4271 - val_accuracy: 0.8132\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.3897 - accuracy: 0.8399 - val_loss: 0.4306 - val_accuracy: 0.8085\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.3862 - accuracy: 0.8348 - val_loss: 0.5891 - val_accuracy: 0.7316\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4201 - accuracy: 0.8293 - val_loss: 0.4507 - val_accuracy: 0.8022\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.3670 - accuracy: 0.8477 - val_loss: 0.4270 - val_accuracy: 0.8155\n",
            "Epoch 00026: early stopping\n",
            "F1 Score: 0.8159890667622652\n",
            "\n",
            "9  \t12    \t0.813917\t0.815989\n",
            "10 \t13    \t0.814086\t0.815989\n",
            "11 \t15    \t0.815299\t0.815989\n",
            "12 \t9     \t0.815828\t0.815989\n",
            "Training: [3, 4, 1, 2, 2]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0917 - accuracy: 0.4673 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0439 - accuracy: 0.4841 - val_loss: 1.0245 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0523 - accuracy: 0.4704 - val_loss: 1.0047 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0313 - accuracy: 0.4787 - val_loss: 1.0235 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.9655 - accuracy: 0.5203 - val_loss: 0.8126 - val_accuracy: 0.6413\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.8117 - accuracy: 0.6234 - val_loss: 0.7091 - val_accuracy: 0.6499\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.7336 - accuracy: 0.6385 - val_loss: 0.6682 - val_accuracy: 0.6586\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6931 - accuracy: 0.6499 - val_loss: 0.6384 - val_accuracy: 0.6531\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6825 - accuracy: 0.6492 - val_loss: 0.6451 - val_accuracy: 0.6656\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6574 - accuracy: 0.6700 - val_loss: 0.5883 - val_accuracy: 0.7370\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.6343 - accuracy: 0.6938 - val_loss: 0.5615 - val_accuracy: 0.7630\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6109 - accuracy: 0.7035 - val_loss: 0.6326 - val_accuracy: 0.7009\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6094 - accuracy: 0.7095 - val_loss: 0.5246 - val_accuracy: 0.7567\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5885 - accuracy: 0.7316 - val_loss: 0.5154 - val_accuracy: 0.7645\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5693 - accuracy: 0.7356 - val_loss: 0.4950 - val_accuracy: 0.7912\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5556 - accuracy: 0.7393 - val_loss: 0.4913 - val_accuracy: 0.7834\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5563 - accuracy: 0.7476 - val_loss: 0.4719 - val_accuracy: 0.7936\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5256 - accuracy: 0.7574 - val_loss: 0.4686 - val_accuracy: 0.7998\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5184 - accuracy: 0.7752 - val_loss: 0.4901 - val_accuracy: 0.7826\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5158 - accuracy: 0.7689 - val_loss: 0.4505 - val_accuracy: 0.8046\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4985 - accuracy: 0.7789 - val_loss: 0.4961 - val_accuracy: 0.7661\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4993 - accuracy: 0.7898 - val_loss: 0.4376 - val_accuracy: 0.8140\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4880 - accuracy: 0.7883 - val_loss: 0.4441 - val_accuracy: 0.8085\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4903 - accuracy: 0.7854 - val_loss: 0.4344 - val_accuracy: 0.8100\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4826 - accuracy: 0.7956 - val_loss: 0.4416 - val_accuracy: 0.8108\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4950 - accuracy: 0.7814 - val_loss: 0.4293 - val_accuracy: 0.8124\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4554 - accuracy: 0.8078 - val_loss: 0.4747 - val_accuracy: 0.7881\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4584 - accuracy: 0.8087 - val_loss: 0.4757 - val_accuracy: 0.7841\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4603 - accuracy: 0.8085 - val_loss: 0.5345 - val_accuracy: 0.7747\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4525 - accuracy: 0.8046 - val_loss: 0.4341 - val_accuracy: 0.8163\n",
            "Epoch 00030: early stopping\n",
            "F1 Score: 0.8129564235774358\n",
            "\n",
            "Training: [3, 2, 1, 2, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.2240 - accuracy: 0.2195 - val_loss: 1.0806 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0713 - accuracy: 0.4667 - val_loss: 1.0395 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0501 - accuracy: 0.4794 - val_loss: 1.0294 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0294 - accuracy: 0.4992 - val_loss: 1.0266 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0423 - accuracy: 0.4839 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0432 - accuracy: 0.4831 - val_loss: 1.0251 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0299 - accuracy: 0.5025 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0319 - accuracy: 0.4894 - val_loss: 1.0250 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0360 - accuracy: 0.4957 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0361 - accuracy: 0.4972 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0363 - accuracy: 0.4974 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0471 - accuracy: 0.4864 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0268 - accuracy: 0.5066 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0290 - accuracy: 0.5030 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0440 - accuracy: 0.4872 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 00015: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "13 \t12    \t0.791843\t0.815989\n",
            "Training: [3, 5, 1, 2, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.1010 - accuracy: 0.3128 - val_loss: 1.0421 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0463 - accuracy: 0.4924 - val_loss: 1.0281 - val_accuracy: 0.5039\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0382 - accuracy: 0.4917 - val_loss: 1.0255 - val_accuracy: 0.5039\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0336 - accuracy: 0.5047 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 1.0320 - accuracy: 0.5032 - val_loss: 1.0249 - val_accuracy: 0.5039\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0329 - accuracy: 0.5073 - val_loss: 1.0248 - val_accuracy: 0.5039\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0353 - accuracy: 0.5029 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0298 - accuracy: 0.5112 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0236 - accuracy: 0.5057 - val_loss: 1.0247 - val_accuracy: 0.5039\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0422 - accuracy: 0.4916 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 1.0387 - accuracy: 0.4877 - val_loss: 1.0246 - val_accuracy: 0.5039\n",
            "Epoch 00011: early stopping\n",
            "F1 Score: 0.3377031557765229\n",
            "\n",
            "Training: [6, 4, 1, 2, 1]\n",
            "CNN Type 1\n",
            "\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0653 - accuracy: 0.4621 - val_loss: 0.9725 - val_accuracy: 0.5039\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.9851 - accuracy: 0.4954 - val_loss: 0.8251 - val_accuracy: 0.5424\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.8036 - accuracy: 0.6151 - val_loss: 0.7094 - val_accuracy: 0.6915\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6783 - accuracy: 0.6907 - val_loss: 0.6875 - val_accuracy: 0.6766\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.6338 - accuracy: 0.7121 - val_loss: 0.5709 - val_accuracy: 0.7684\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5911 - accuracy: 0.7282 - val_loss: 0.5409 - val_accuracy: 0.7567\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5717 - accuracy: 0.7486 - val_loss: 0.5387 - val_accuracy: 0.7465\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5540 - accuracy: 0.7491 - val_loss: 0.5099 - val_accuracy: 0.7763\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5249 - accuracy: 0.7757 - val_loss: 0.5608 - val_accuracy: 0.7370\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.5241 - accuracy: 0.7686 - val_loss: 0.4733 - val_accuracy: 0.7928\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.5083 - accuracy: 0.7835 - val_loss: 0.5046 - val_accuracy: 0.7543\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4892 - accuracy: 0.7844 - val_loss: 0.4645 - val_accuracy: 0.8046\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4638 - accuracy: 0.7998 - val_loss: 0.4474 - val_accuracy: 0.8069\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4510 - accuracy: 0.8139 - val_loss: 0.4477 - val_accuracy: 0.8061\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4582 - accuracy: 0.8064 - val_loss: 0.4597 - val_accuracy: 0.8069\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4592 - accuracy: 0.8065 - val_loss: 0.4403 - val_accuracy: 0.8053\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4296 - accuracy: 0.8170 - val_loss: 0.4601 - val_accuracy: 0.7936\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4154 - accuracy: 0.8312 - val_loss: 0.4338 - val_accuracy: 0.8093\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4163 - accuracy: 0.8211 - val_loss: 0.4633 - val_accuracy: 0.7967\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.4133 - accuracy: 0.8259 - val_loss: 0.5277 - val_accuracy: 0.7763\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4196 - accuracy: 0.8202 - val_loss: 0.4276 - val_accuracy: 0.8108\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4287 - accuracy: 0.8189 - val_loss: 0.4232 - val_accuracy: 0.8030\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.4027 - accuracy: 0.8316 - val_loss: 0.4363 - val_accuracy: 0.8100\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 4s 37ms/step - loss: 0.3889 - accuracy: 0.8439 - val_loss: 0.4275 - val_accuracy: 0.8061\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.3888 - accuracy: 0.8347 - val_loss: 0.4627 - val_accuracy: 0.7967\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 4s 36ms/step - loss: 0.3841 - accuracy: 0.8384 - val_loss: 0.4256 - val_accuracy: 0.8124\n",
            "Epoch 00026: early stopping\n",
            "F1 Score: 0.8034461042494185\n",
            "\n",
            "14 \t12    \t0.791425\t0.815989\n",
            "15 \t16    \t0.815841\t0.815989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BtJECN7cwap"
      },
      "source": [
        "## Results\n",
        "Below you can find the generational improvement plot as well as information on the best model created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP-oq75Fcwap",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "0b71f720-8c97-4936-b2f7-e53f8b01ea2d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "#Show generational plot\n",
        "gen, avg, max_ = log.select(\"gen\", \"avg\", \"max\")\n",
        "plt.plot(gen, avg, label=\"Average\")\n",
        "plt.plot(gen, max_, label=\"Maximum\")\n",
        "plt.xlabel(\"Generation\")\n",
        "plt.ylabel(\"Fitness\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFzr9CV_cwaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a891c8e8-3f4e-4439-a4fb-c8fee6b507ba"
      },
      "source": [
        "#Print best score and individual\n",
        "print(f'Best Individual:\\n{hof[0]}')\n",
        "print(f'Fitness: {seen_models[str(hof[0])]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Individual:\n",
            "[4, 3, 5, 1, 1]\n",
            "Fitness: 0.817283523009102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "4ijb451-SVaz",
        "outputId": "46f593a6-072d-48e4-d8ab-e386178a54f4"
      },
      "source": [
        "chromosome = [4, 3, 5, 1, 1]\n",
        "\n",
        "def get_model_type(x):\n",
        "  if chromosome[0] % 3 == 0:\n",
        "    return 1\n",
        "  elif chromosome[0] % 3 == 1:\n",
        "    return 2\n",
        "  else:\n",
        "    return 3\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model(f'/content/drive/MyDrive/CSC180_Final_Project/Custom_CNN_Models/{chromosome}/best_weights.hdf5')\n",
        "\n",
        "# Print model info\n",
        "print('Model Info:')\n",
        "print(f'CNN Type: {get_model_type(chromosome[0])}')\n",
        "print(f'Activation Type: {get_activation_function(chromosome[1])}')\n",
        "print(f'Optimizer Type: {get_optimizer(chromosome[2])}')\n",
        "print(f'Number of Fully-Connected Layers: {chromosome[3] + 2} (not including output layer)')\n",
        "print(f'Dropout Chance: {float(chromosome[4])/float(13)}\\n')\n",
        "\n",
        "# Predict\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Report\n",
        "print('Model Report:')\n",
        "print(metrics.classification_report(y_true, pred, zero_division=1))\n",
        "print('\\n')\n",
        "\n",
        "# Plot\n",
        "print('Plot:')\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, label_decoder)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Info:\n",
            "CNN Type: 2\n",
            "Activation Type: relu\n",
            "Optimizer Type: sgd\n",
            "Number of Fully-Connected Layers: 3 (not including output layer)\n",
            "Dropout Chance: 0.07692307692307693\n",
            "\n",
            "Model Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.82       642\n",
            "           1       0.76      0.76      0.76       387\n",
            "           2       0.88      0.94      0.91       245\n",
            "\n",
            "    accuracy                           0.82      1274\n",
            "   macro avg       0.82      0.84      0.83      1274\n",
            "weighted avg       0.82      0.82      0.82      1274\n",
            "\n",
            "\n",
            "\n",
            "Plot:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEmCAYAAAA0k8gFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcVZ3G8e/bCdlIWMMSAwhKFBAlhgyrYARlANGgI4JEwAgiCqggIriBqDO4MCwKYhQxLLI6CLIFBDOAIpJA2EEiixACIWENSyTwmz/OqaFoeqm+qeq61fV+8tyn735P1dP59dmvIgIzM+u7jmYnwMysVTmAmpkV5ABqZlaQA6iZWUEOoGZmBTmAmpkV5ABqPZI0XNIfJD0r6YJluM8USVfVM23NImkbSfc1Ox3WfHI/0IFB0p7AocAGwPPAHOAHEXHDMt53L+BgYKuIWLrMCS05SQGMi4i5zU6LlZ9zoAOApEOBE4D/BNYA1gFOASbX4fZvBf7eDsGzFpIGNzsNViIR4aWFF2BFYDGwWw/nDCUF2MfycgIwNB+bBDwKfBVYAMwHpuZj3wX+BbySn7EvcDRwVtW91wUCGJy3PwM8QMoFPwhMqdp/Q9V1WwE3A8/mn1tVHZsJfA/4c77PVcDobj5bJf2HV6V/V2Bn4O/AU8A3qs7fDLgReCaf+zNgSD52Xf4sL+TPu3vV/b8OPA6cWdmXr3l7fsaEvP0W4ElgUrN/N7w0fnEOtPVtCQwDLurhnG8CWwDjgU1IQeRbVcfXJAXisaQgebKklSPiKFKu9ryIGBkRp/WUEEnLAycBO0XEKFKQnNPFeasAl+VzVwX+G7hM0qpVp+0JTAVWB4YAh/Xw6DVJ38FY4DvAL4FPA5sC2wDflrRePvdV4BBgNOm72x74IkBEbJvP2SR/3vOq7r8KKTe+f/WDI+IfpOB6lqQRwOnA9IiY2UN6bYBwAG19qwILo+ci9hTgmIhYEBFPknKWe1UdfyUffyUiLiflvt5ZMD2vARtLGh4R8yPiri7O+TBwf0ScGRFLI+Ic4F7gI1XnnB4Rf4+Il4DzScG/O6+Q6ntfAc4lBccTI+L5/Py7SX84iIjZEfHX/NyHgF8A76/hMx0VEUtyet4gIn4JzAVuAsaQ/mBZG3AAbX2LgNG91M29BXi4avvhvO//79EpAL8IjOxrQiLiBVKx9wBgvqTLJG1QQ3oqaRpbtf14H9KzKCJezeuVAPdE1fGXKtdLeoekSyU9Luk5Ug57dA/3BngyIl7u5ZxfAhsDP42IJb2cawOEA2jruxFYQqr3685jpOJnxTp5XxEvACOqttesPhgRMyLiQ6Sc2L2kwNJbeippmlcwTX3xc1K6xkXECsA3APVyTY9dVSSNJNUrnwYcnasorA04gLa4iHiWVO93sqRdJY2QtJyknST9KJ92DvAtSatJGp3PP6vgI+cA20paR9KKwJGVA5LWkDQ514UuIVUFvNbFPS4H3iFpT0mDJe0ObARcWjBNfTEKeA5YnHPHX+h0/AngbX2854nArIjYj1S3e+oyp9JaggPoABARx5H6gH6L1AL8CHAQ8Pt8yveBWcDtwB3ALXlfkWddDZyX7zWbNwa9jpyOx0gt0+/nzQGKiFgE7EJq+V9EakHfJSIWFklTHx1GaqB6npQ7Pq/T8aOB6ZKekfTJ3m4maTKwI69/zkOBCZKm1C3FVlruSG9mVpBzoGZmBTmAmpkV5ABqZlaQA6iZWUEDdmIEDR4eGjKq2ckorQ3XX6vZSSi95Qb31j3Ubrv1loURsVo97zlohbdGLH3TgK8uxUtPzoiIHev5/L4YuAF0yCiGvrPXXiht67w/HNvsJJTemisNa3YSSm+1Uct1HlG2zGLpSzX/3315zsm9jSJrqAEbQM2sVQnUGrWLDqBmVi4COgY1OxU1aY0wb2btRapt6fU2ekjSHZLmSJqV960i6WpJ9+efK+f9knSSpLmSbpc0obf7O4CaWcnkInwtS20+EBHjI2Ji3j4CuCYixgHX5G2AnYBxedmfNPFMjxxAzax86pQD7cZkYHpen87rM5lNBs6I5K/ASpLG9HQjB1AzKxfRlxzoaEmzqpb9O90tgKskza46tkZEzM/rj5PeIwZpPtpHqq59lDfOUfsmbkQys5LpU+5yYVXRvCvvi4h5klYHrpZ0b/XBiIj8JtZCHEDNrHzq1AofEfPyzwWSLiK9D+wJSWMiYn4uoi/Ip88D1q66fC16meTbRXgzK5n6NCJJWl7SqMo6sANwJ3AJsE8+bR/g4rx+CbB3bo3fAni2qqjfJedAzaxcxLI0EFVbA7hI6V6Dgd9GxJWSbgbOl7Qv6V1clWFPl5Nehz2X9B6uqb09wAHUzMqnDiORIuIB8ttYO+1fRHqddef9ARzYl2c4gJpZyXgop5lZcR2tMROWA6iZlUsLjYV3ADWzknER3sysuPq0wjecA6iZlY9zoGZmBSzbRCH9ygHUzMrHjUhmZkW4EcnMrDgX4c3MCqjMB9oCHEDNrGRchDczK85FeDOzgtwKb2ZWgFyENzMrzkV4M7Ni5ABqZtZ36Y0eDqBmZn2nvLQAB1AzKxnR0eFGJDOzQlqlCN+wMC8pJB1XtX2YpKMb9TwzGzgk1bQ0WyPzyUuAj0sa3cBnmNlAoz4sTdbIALoUmAYc0vmApHUlXSvpdknXSFon7/+NpJMk/UXSA5I+UXXN1yTdnK/5bgPTbWZNJGrLfQ70HCjAycAUSSt22v9TYHpEvAc4Gzip6tgY4H3ALsCxAJJ2AMYBmwHjgU0lbdv5YZL2lzRL0qxY+lLdP4yZ9Q8HUCAingPOAL7U6dCWwG/z+pmkgFnx+4h4LSLuBtbI+3bIy63ALcAGpIDa+XnTImJiREzU4OH1+yBm1q86OjpqWpqtP1rhTyAFvdNrPH9J1bqqfv5XRPyingkzsxIqSf1mLRoewiPiKeB8YN+q3X8B9sjrU4Dre7nNDOCzkkYCSBorafV6p9XMyqFVivD91Q/0OOCgqu2DgdMlfQ14Epja08URcZWkDYEb85e2GPg0sKAxyTWzZqk0IrWChgXQiBhZtf4EMKJq+2Fguy6u+UwP9zgROLERaTWzcmn7AGpmVohAHQ6gZmaFOAdqZlZQqwTQ5nekMjOrUu+RSJIGSbpV0qV5ez1JN0maK+k8SUPy/qF5e24+vm5v93YANbPyqe9Y+C8D91Rt/xA4PiLWB57m9S6W+wJP5/3H5/N65ABqZuWi+vUDlbQW8GHgV3lbpB5AF+ZTpgO75vXJeZt8fHv18hDXgZpZ6fRhmOZoSbOqtqdFxLSq7ROAw4FReXtV4JmIWJq3HwXG5vWxwCMAEbFU0rP5/IXdPdwB1MzKp/bi+cKImNjlLaRdgAURMVvSpDql7A0cQM2sdOrUCr818FFJOwPDgBVIg3FWkjQ450LXAubl8+cBawOPShoMrAgs6ukBrgM1s1Kptf6ztyAbEUdGxFoRsS5p7o1rI2IK8CegMtfwPsDFef2SvE0+fm1ERE/PcAA1s9Jp8GQiXwcOlTSXVMd5Wt5/GrBq3n8ocERvN3IR3sxKp94d6SNiJjAzrz9Ampy98zkvA7v15b4OoGZWOh4Lb2ZWhFpnKKcDqJmVioAWiZ8OoGZWNp5Q2cyssBaJnw6gZlYygg43IpmZ9Z1wADUzK8xFeDOzgtyIZGZWhJwDNTMrJPUDbY0I6gBqZiUjNyKZmRXlHKiZWRGuAzUzK8Z1oGZmy6BF4qcDqJmVj3OgZmZFeCx88204bi3Ov/TYZiejtI6acV+zk1B6Z+29abOT0JY8H6iZWWGeD9TMrLAWiZ8OoGZWPs6BmpkV4Y70ZmbFpAmVO5qdjJo4gJpZ6TgHamZWkOtAzcyKcB2omVkxcj9QM7PiBnkop5lZMS2SAXUANbNykdyIZGZWWIuU4LsPoJJ+CkR3xyPiSw1JkZm1vYGQA53Vb6kwM6tSj/gpaRhwHTCUFOsujIijJK0HnAusCswG9oqIf0kaCpwBbAosAnaPiId6eka3ATQipndKzIiIeHEZPo+ZWa8EDKpPDnQJsF1ELJa0HHCDpCuAQ4HjI+JcSacC+wI/zz+fjoj1Je0B/BDYvacH9DrgVNKWku4G7s3bm0g6ZZk+lplZd5T6gday9CSSxXlzubwEsB1wYd4/Hdg1r0/O2+Tj26uXh9QyYv8E4N9JWVoi4jZg2xquMzMrRKpt6f0+GiRpDrAAuBr4B/BMRCzNpzwKjM3rY4FHAPLxZ0nF/G7V1AofEY90CsSv1nKdmVlfCeiovQg/WlJ1e820iJhW2YiIV4HxklYCLgI2qFtCqS2APiJpKyByPcKXgXvqmQgzs2p9qAJdGBETezspIp6R9CdgS2AlSYNzLnMtYF4+bR6wNvCopMHAiuSSd3dqKcIfABxIyt4+BozP22ZmDVGPOlBJq+WcJ5KGAx8iZf7+BHwin7YPcHFevyRvk49fGxHdduWEGnKgEbEQmNLbeWZm9SDVbSz8GGC6pEGkzOL5EXFpbhQ/V9L3gVuB0/L5pwFnSpoLPAXs0dsDeg2gkt4GnAhsQWrBuhE4JCIeKPCBzMx6VY/wGRG3A+/tYv8DwGZd7H8Z2K0vz6ilCP9b4HxSNH8LcAFwTl8eYmbWF/UowveHWgLoiIg4MyKW5uUsYFijE2Zm7Sm1wte2NFtPY+FXyatXSDqCNPQpSD3zL++HtJlZOypJ7rIWPdWBziYFzMon+XzVsQCObFSizKy9tUj87HEs/Hr9mRAzM8hj4ctQPq9BTSORJG0MbERV3WdEnNGoRJlZexsIRXgAJB0FTCIF0MuBnYAbSNM+mZnVXWuEz9pa4T8BbA88HhFTgU1IQ5zMzOpOSmPha1marZYi/EsR8ZqkpZJWIM1qsnaD02VmbawEsbEmtQTQWXk86S9JLfOLSaORaiJpcUSMLJg+M2tDHQOlESkivphXT5V0JbBCHiJlZlZ3ohzF81p0WwcqaULnBVgFGJzXayZpkqRLq7Z/Jukzef0hSf8laY6kWflZMyT9Q9IBVddfJ+kySfdJOlVSLfW3ZtZqapxMuQwxtqcc6HE9HKtMi18v/4yI8ZKOB34DbE3qMnUncGo+ZzNST4CHgSuBj/P6tPwASNof2B9gzFhX05q1qpbvxhQRH+jHdFySf94BjIyI54HnJS2pzOcH/K0yA5Skc4D30SmA5pmopwG8a5MJPc7jZ2bl1SrFy5o60tfBUt74nXSejGRJ/vla1Xplu5LGzgHRAdJsABKtkwPtr0D/MLCRpKE5R7l9gXtsJmm9XPe5O6kzv5kNQIM7aluaraE50PxekSX5pXTnk+o0HyTNAt1XNwM/A9YnTcl/Ud0SamalkRqIWiMHWstQTpFe6fG2iDhG0jrAmhHxtxru/y7Sa0SJiMOBwzufEBHrVq3/htSI9IZj+ct8LiJ2qeGZZtbiWqQbaE1F+FNIb7L7VN5+Hji5t4tyF6RzgG8VTp2ZtaWB0I2pYvOImCDpVoCIeFrSkN4uiohTeb0L0jKJiJnAzHrcy8zKrY/vhW+qWgLoK/mtdgHpVaGk1nEzs4YoQftQTWoJoCeRGmxWl/QD0uxMLpabWUNIGjgTKkfE2ZJmk7oeCdg1Iu5peMrMrG21SAm+plb4dYAXgT9U74uIfzYyYWbWvlokA1pTEf4yXn+53DBgPeA+UhclM7O6GlCNSBHx7urtPBPTF7s53cxsmbVI/Oz7SKSIuEXS5o1IjJkZgkEtEkFrqQM9tGqzA5gAPNawFJlZW0tF+Ganoja15EBHVa0vJdWJ/q4xyTEzGyABNHegHxURh/VTeszMWn8yEUmDI2KppK37M0Fm1t4GShH+b6T6zjmSLgEuAF6oHIyI/2lw2sysHZVkopBa1FIHOgxYRHoHUqU/aAAOoGZWdwIGt0gWtKcAunpugb+T1wNnhV+nYWYN0yo50J4mPRkEjMzLqKr1ymJm1gCio8alx7tIa0v6k6S7Jd0l6ct5/yqSrpZ0f/65ct4vSSdJmivp9lpe395TDnR+RBzTl49tZras0kvl6nKrpcBX8+CfUcBsSVcDnwGuiYhjJR0BHAF8HdgJGJeXzYGf55/d6ikH2iKZaDMbUJRa4WtZehIR8yPilrz+PHAPMBaYDEzPp00Hds3rk4EzIvkrsJKkMT09o6ccaJE3Z5qZLbM+TCYyWtKsqu1pETGt80mS1gXeC9wErBER8/Ohx4E18vpY4JGqyx7N++bTjW4DaEQ8VUPizczqStCXCZUXRsTEHu8njSSNnvxKRDxX3Uk/IkJS4UbxVpk538zaSL1eKidpOVLwPLuq7/oTlaJ5/rkg758HrF11+Vp5X7ccQM2sVEQKTLUsPd4nZTVPA+6JiP+uOnQJsE9e3we4uGr/3rk1fgvg2aqifpf6PJ2dmVlDqW5j4bcG9gLukDQn7/sGcCxwvqR9gYeBT+ZjlwM7A3NJb+GY2tsDHEDNrHTqET4j4oYebvWmRvKICODAvjzDAdTMSmVAvdLDzKy/tchQeAdQMysbtf58oGZmzVBphW8FDqBmVjrOgTbZ0MEdrLPqiGYno7TO2nvTZieh9H547f3NTkLbao3wOYADqJm1Jg2k1xqbmfU3F+HNzApqjfDpAGpmJdQiGVAHUDMrl9SNqTUiqAOomZWOc6BmZoXIY+HNzIpwEd7MrKgaZ5svAwdQMysdB1Azs4LkIryZWd+lCZWbnYraOICaWem4Fd7MrCAX4c3MCnAR3sysMDkHamZWiPuBmpkV1yLx0wHUzMpFeEZ6M7PiWiN+OoCaWfm4EcnMrKAWKcE7gJpZ+bRI/HQANbNyEX4rp5lZMe4HamZWXIvETwdQMyuhFomgHc1OgJnZG6nmf73eSfq1pAWS7qzat4qkqyXdn3+unPdL0kmS5kq6XdKE3u7vAGpmpSPVttTgN8COnfYdAVwTEeOAa/I2wE7AuLzsD/y8t5s7gJpZqaRW+PoE0Ii4Dniq0+7JwPS8Ph3YtWr/GZH8FVhJ0pie7u86UDMrnT6MRBotaVbV9rSImNbLNWtExPy8/jiwRl4fCzxSdd6jed98uuEAamal04duTAsjYmLR50RESIqi17sIb2aloxqXgp6oFM3zzwV5/zxg7arz1sr7utXwACrpm5Luyq1acyRtLmmmpIn5+OWSVuriuqMlHdbo9JlZydQaPYtH0EuAffL6PsDFVfv3zq3xWwDPVhX1u9TQIrykLYFdgAkRsUTSaGBI9TkRsXMj02BmradeszFJOgeYRKorfRQ4CjgWOF/SvsDDwCfz6ZcDOwNzgReBqb3dv9F1oGNIdRRLACJiIbxxnKukh4CJEbFQ0jdJfxEWkCpzZ+dz3g6cDKxG+mCfi4h7G5x2M2uCer5ULiI+1c2h7bs4N4AD+3L/RhfhrwLWlvR3SadIen93J0raFNgDGE/6K/BvVYenAQdHxKbAYcAp3dxjf0mzJM1a+OSTdfsQZtbPGlwJWi8NzYFGxOIcGLcBPgCcJ+mIbk7fBrgoIl4EkHRJ/jkS2Aq4oCrnOrSb500jBVsmbDqxcMuamTWXJ1TOIuJVYCYwU9IdvF55W6sO4JmIGF/vtJlZObXKbEwNLcJLeqekcVW7xpMqbbtyHbCrpOGSRgEfAYiI54AHJe2W7ylJmzQy3WbWXC1Sgm94HehIYLqkuyXdDmwEHN3ViRFxC3AecBtwBXBz1eEpwL6SbgPuIg25MrMBqDKhci1LszW6DnQ2qf6ys0lV56xbtf4D4Add3OdB3jwhgJkNRJ5Q2cysuBaJnw6gZlZCLRJBHUDNrGRqmyy5DBxAzax0XAdqZlZAZULlVuAAamal4yK8mVlBzoGamRXUIvHTAdTMSsYd6c3MlkVrRFAHUDMrlXpOqNxoDqBmVjouwpuZFeRuTGZmRbVG/HQANbPyaZH46QBqZuUid2MyMyuuDLPN18IB1MxKpzXCpwOomZVQi2RAHUDNrGw8obKZWSGeD9TMbBk4gJqZFeQivJlZEe4HamZWjHA3JjOz4lokgjqAmlnptEodaEezE2Bm1lmHalt6I2lHSfdJmivpiLqns943NDNbZqpx6ekW0iDgZGAnYCPgU5I2qmcyHUDNrHRU479ebAbMjYgHIuJfwLnA5Hqmc8DWgd56y+yFKwwf9HCz09HJaGBhsxNRYv5+ele27+it9b7hrbfMnjFiiEbXePowSbOqtqdFxLS8PhZ4pOrYo8Dm9UhjxYANoBGxWrPT0JmkWRExsdnpKCt/P71rh+8oInZsdhpq5SK8mQ1U84C1q7bXyvvqxgHUzAaqm4FxktaTNATYA7ikng8YsEX4kprW+yltzd9P7/wd1Sgilko6CJgBDAJ+HRF31fMZioh63s/MrG24CG9mVpADqJlZQQ6gZmYFOYBaaalV3m1rbcsBtAkcGHpW9f0M72Z/2+vqu5Dk/8/9zK3w/UySIn/pkrYBXiON132iuSkrF0k7Ap8D7if15/t9RLxa/f21q06/Q1sAzwJPRcQTkjoi4rXmprB9OIA2iaSvATsDDwMrAcdExC3NTVU55KDwQ+Ak4D3ACsB84MftHjyrSToQmAJcCewNbB4Ri5qbqvbiLH8TSNoE2DoiPkCa7GAQMEfScs1NWfNJGgv8BLgpIn4H/IgUIDYgDcVrW5JWrFrfEtgV2AF4FXgAeLpStHd1R/9wAO0HXfwyvwA8LOkkYFPgP3Kx64OShr/pBu3lReAvwB6SNouIFyJiBil4rt/cpDWPpHHAkZK2zrsWApcBXwDeD3w0/w79h6TlnVPvHx7K2WCd6qtGAv8C/kEqtr8N+GRE/EvSfqT/DP8OvNSs9Pa3yvcjaVNSkPzfiDhc0pPA9yQdT/q+1ibV9bWrDtIUwjtIehl4CPgiMDgi3gYgaS9gT+BPpD/S1mCuA22gTsHzMOB9wBBgL1KRdCqwPPBPYBdSMK3rWN1WIGk74Ezgf4GtgQ+SvpNvAgeTGpGOiogb260RqdPv0IGk7+Zp4ChgJCkXejYpuO4ITI2IO5qU3LbjInwDVf3ib0cKkEcA9wI3AXcB3wAuBe4DJrdT8KyqqxsFDAN2j4g9gV8DFwLrkoLEt4HFwFPNSWlzdQqeU4BrSTn1g4ClpID6IvAMMMXBs385B9pgkiYBXwJujYjv5X0/AT4KTIqIx5qYvKaStAtwLPAccGNEfDXv/xbwWeDDwBPAAcDGed+SdsiBSpoAzImI1/IfmV8BP4qI2ZI2BvYDRgDHRcR9zUxrO3MOtM66aDB6EHgS2DC3vhMRhwFXA1dJGtSOHaAlbQh8DDgSOB1YXtIBABHxfVKRftWIeIo0hdtBEfFyOwTPbB9gdYCIeJ6UA58iaXhE3An8npT73M0Nj83jHGgddaqv+gipiPUMMBs4gfSf4IKIuC2fs3pELGhWeptF0hrArcCMiJgqaRXgQ6TW5Psi4sSmJrCJcmv7IxHxsqTNge+R6ja3AD4O/D0ipuWBBvsDX/AgjOZpu5xPf5D0ReC7pEajXwNfAQ4htbzvnYtgkHKmbUXSW/J/+EOBbSVtm3OZVwI3AhtLWqepiWyS/IfkJOBoSStExE2kxqHTgdtJdeeTJF1HGmjwHQfP5nIOtA7yf/hFEfGCpNWB80k5g3skrUzKgX4HuAb4OvD9iCjTmxX7haQxwH+S6junSfo0cDSp5fh6SSsBwyNifjPT2Qy5PngTYCZwOHAP8M08fPVy0h/b/SLilVwVNL8dSy9l4xzoMsrF0a8CX5A0Mv9SLyT19yQiniblQN+dA8PX2jF4AuTPfz0wUdJnIuIsUgC9MOdEn2nj4PkDUvXFn4HPAxuR+sEOiYidgZWBS3M10W0OnuXgALrsniT1U3wLMDU3Is0FzpVUGajwVmAtSYNI9aIDnqSxkv6Q19eTdDhARPya1N9zW0n75CB6OG36uyhpTdIf4P0i4kJJIyLicVJOfXPgK5KGRsRHSf0/xzYxudaJRyIVlCv7OyLiPklnk0bJ7AR8LiKOkPRz4DpJt5P+I0yJiFebmOR+FRHzJK0qaSap+mKCpK9GxHERcbaktwLfljQoB9U3NMK1kSXAK8DLkoYBh+eub4uAAD4CrCbpyIjYo3nJtK64DrQASauScp4LSY1Fr5K62uxJGq89PyJ+kVtRhwH/jIgHm5Xe/lY9pZqkS4AxpEa0qcC9EfFjSeNJuazDIuLu5qW2uXKJ5VDSpCDvAv4I3ECqA/0YabauLYBDXGwvHwfQgvLooj8CXwbeTaqjWkyq+xxN6ud5ekQsaVoim6iHIPpN0nf0TlLfzj82L5XlkOdIeDdpvP/Fld8ZSdOBcyPiimamz7rnALoMJH2I1O1kE2ANYDtgD2Az0vyVW0dE206A0UUQXYHUl/FDpFz6dc1MX5lJ2o009Hf3iJjb7PRY1xxAl5GkDwPHA1tExFO529JywIiIeKipiSuBTkH0CmBpRHyk8zFLclev3Umz8e+eRx1ZSTmA1oGknYATgS3DM4K/SacgehFwbUT8tMnJKqU8LHM7Upcm5zxLzq3wdRARV0gaAvxR0qbOVb1RnhCjEkT/RirKWxci4iXSFHXWAtqy710jRMTFwDYOnl3LQXQoMAq4qNnpMasHF+GtX0kaHBFtMZjABj4HUDOzglyENzMryAHUzKwgB1Azs4IcQNuYpFclzZF0p6QLJI1Yhnv9RtIn8vqvJG3Uw7mTJG1V4BkPSRpd6/5O5yzu47OOVnqTqlm3HEDb20sRMT4iNiaNTz+g+mDVdHx9EhH79TJByCSgzwHUrGwcQK3iemD9nDu8Po9dvzu/9O7Hkm6WdLukz0OaRUjSzyTdJ+mP5Beg5WMzJU3M6ztKukXSbZKukbQuKVAfknO/20haTdLv8jNulrR1vnZVSVdJukvSr0ivt+iRpN9Lmp2v2b/TsePz/mskrZb3vV3Slfma6yVtUI8v09qDRyJZJae5E+m9RAATgI0j4sEchJ6NiH/LHeH/LOkq4L2kGZU2Ik2kcjfp/U/V910N+CWwbb7XKnm+gFOBxRHxk3zeb4HjI+IGpdejzAA2JL0X/oaIOCbPObBvDR/ns/kZw4GbJf0uD69dHpgVEYdI+k6+90GkaQgPiIj78/SDp5CGUvuOujMAAAGnSURBVJr1ygG0vQ2XNCevXw+cRipa/61q/tIdgPdU6jeBFYFxwLbAOXmS6MckXdvF/bcArqvcK788risfBDbS62+EXiFP8bYtafYmIuIySU/X8Jm+JOljeX3tnNZFwGvAeXn/WcD/5GdsBVxQ9eyhNTzDDHAAbXcvRcT46h05kLxQvQs4OCJmdDpv5zqmo4M0m9XLXaSlZnkm9w+SJnV5UWk2/GHdnB75uc90/g7MauU6UOvNDNIL85YDkPQOScsD1wG75zrSMcAHurj2r6R3H62Xr10l73+eNCa+4irg4MpGnq2e/Iw9876dSJNW92RF4OkcPDcg5YArOoBKLnpPUtXAc8CDee7NSr3uJr08w+z/OYBab35Fqt+8RdKdwC9IJZeLgPvzsTNI73R/g4h4EtifVFy+jdeL0H8APlZpRAK+RHpT5+2S7ub13gDfJQXgu0hF+X/2ktYrgcGS7gGOJQXwiheAzfJn2A44Ju+fAuyb03cXMLmG78QM8Fh4M7PCnAM1MyvIAdTMrCAHUDOzghxAzcwKcgA1MyvIAdTMrCAHUDOzgv4PNQHLiUMD8OoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DulFkT3bcwaq"
      },
      "source": [
        "## Part 2: Transfer Learning Models\n",
        "After training our custom CNN models we will now train our models that incorporate transfer learning. As before the hyper-parameters for each model will be defined in a chromosome. However, this chromosome differs slightly from the one used above. The values in this chromosome are as follows: ``[\"Optimizer Type\", \"Activation Function Type\", \"Number of Trainable Layers\", \"Dropout Chance\", \"Number of Dense Layers\", \"Model Type\"]``. This chromosome we be used in our GA to tune our hyper-parameters to help produce the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "id": "9mxO4RCJcwaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9e9cbb-6d3b-45f2-de80-8a0136c85241"
      },
      "source": [
        "from tensorflow.keras.layers import UpSampling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Builder function for VGG16\n",
        "def build_transfer_model_VGG(chromosome):\n",
        "    vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    activation_function=None\n",
        "    optimizer_function=None\n",
        "    dropout_chance=float(chromosome[3])/float(13)\n",
        "\n",
        "    # Determine activation type\n",
        "    if (chromosome[1]%3==0):\n",
        "        activation_function=\"relu\"\n",
        "    elif (chromosome[1]%3==1):\n",
        "        activation_function=\"tanh\"\n",
        "    elif (chromosome[1]%3==2):\n",
        "        activation_function=\"sigmoid\"\n",
        "\n",
        "    # Determine optimizer type\n",
        "    if (chromosome[0]%2==0):\n",
        "        optimizer_function=\"adam\"\n",
        "    else:\n",
        "        optimizer_function=\"sgd\"\n",
        "\n",
        "    # Create our model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add VGG16 layers\n",
        "    for layer in vgg_model.layers:\n",
        "        model.add(layer)\n",
        "\n",
        "    # Freeze the number of layers defined by the chromosome\n",
        "    for layer in model.layers[:len(model.layers)-chromosome[2]]:\n",
        "        layer.trainable= False\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Add Fully-Connected portion of the network\n",
        "    for i in range(chromosome[4]+2):\n",
        "        model.add(Dense(pow(2,chromosome[4]+1-i), activation=activation_function))\n",
        "        model.add(Dropout(dropout_chance))\n",
        "\n",
        "    # Add output layer, compile, and return model\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer_function)\n",
        "    return model\n",
        "\n",
        "# Builder function for MobileNet\n",
        "def build_transfer_model_mobile(chromosome):\n",
        "    mobile_model = MobileNetV2(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n",
        "    activation_function=None\n",
        "    optimizer_function=None\n",
        "    dropout_chance=float(chromosome[3])/float(13)\n",
        "\n",
        "    # Determine activation type\n",
        "    if (chromosome[1]%3==0):\n",
        "        activation_function=\"relu\"\n",
        "    elif (chromosome[1]%3==1):\n",
        "        activation_function=\"tanh\"\n",
        "    elif (chromosome[1]%3==2):\n",
        "        activation_function=\"sigmoid\"\n",
        "\n",
        "    # Determine optimizer type\n",
        "    if (chromosome[0]%2==0):\n",
        "        optimizer_function=\"adam\"\n",
        "    else:\n",
        "        optimizer_function=\"sgd\"\n",
        "\n",
        "    # Freeze the number of layers defined by the chromosome\n",
        "    for layer in mobile_model.layers[:len(mobile_model.layers)-chromosome[2]]: #set the number of trainable layers\n",
        "        layer.trainable= False\n",
        "    \n",
        "    # Add a some top layers\n",
        "    newOut = mobile_model.output\n",
        "    newOut = GlobalAveragePooling2D()(newOut)\n",
        "\n",
        "    # Add the fully connected portion of the network\n",
        "    for i in range(chromosome[4]+2):\n",
        "        newOut = Dense(pow(2,chromosome[4]+1-i), activation=activation_function)(newOut)\n",
        "        newOut = Dropout(dropout_chance)(newOut)\n",
        "\n",
        "    # Create the output layer\n",
        "    newOut = Dense(3, activation='softmax')(newOut) #Perm\n",
        "\n",
        "    #Create model, compile, and return it\n",
        "    model = Model(inputs=mobile_model.input, outputs=newOut)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer_function)\n",
        "    return model\n",
        "\n",
        "#Print model examples\n",
        "chromosome = [0, 0, 1, 5, 9]\n",
        "\n",
        "print('VGG16 Model:\\n')\n",
        "model = build_transfer_model_VGG(chromosome)\n",
        "print(f'{model.summary()}\\n\\n')\n",
        "\n",
        "print('MobileNet Model:\\n')\n",
        "model = build_transfer_model_mobile(chromosome)\n",
        "print(f'{model.summary()}\\n\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG16 Model:\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1024)              25691136  \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dropout_51 (Dropout)         (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dropout_52 (Dropout)         (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 1)                 3         \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 3)                 6         \n",
            "=================================================================\n",
            "Total params: 41,105,903\n",
            "Trainable params: 26,391,215\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "\n",
            "MobileNet Model:\n",
            "\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
            "__________________________________________________________________________________________________\n",
            "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
            "                                                                 block_2_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
            "                                                                 block_4_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
            "                                                                 block_5_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
            "                                                                 block_7_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
            "                                                                 block_8_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
            "                                                                 block_9_project_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
            "                                                                 block_11_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
            "                                                                 block_12_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
            "                                                                 block_14_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
            "                                                                 block_15_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_59 (Dense)                (None, 1024)         1311744     global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 1024)         0           dense_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_60 (Dense)                (None, 512)          524800      dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 512)          0           dense_60[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_61 (Dense)                (None, 256)          131328      dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 256)          0           dense_61[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_62 (Dense)                (None, 128)          32896       dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 128)          0           dense_62[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_63 (Dense)                (None, 64)           8256        dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 64)           0           dense_63[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_64 (Dense)                (None, 32)           2080        dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 32)           0           dense_64[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_65 (Dense)                (None, 16)           528         dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 16)           0           dense_65[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_66 (Dense)                (None, 8)            136         dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 8)            0           dense_66[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_67 (Dense)                (None, 4)            36          dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 4)            0           dense_67[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_68 (Dense)                (None, 2)            10          dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 2)            0           dense_68[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_69 (Dense)                (None, 1)            3           dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 1)            0           dense_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_70 (Dense)                (None, 3)            6           dropout_64[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 4,269,807\n",
            "Trainable params: 2,011,823\n",
            "Non-trainable params: 2,257,984\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSjzRk85cwar"
      },
      "source": [
        "## GA Evaluation Function\n",
        "After defining our transfer learning models, we will need to create our evaluation function. This will be very similiar to our evaluation function for our custom CNN models. As before certain items related to that model will be saved. The F1 score of that particular trained model will act as its evaluation. Chromosomes will be saved in a dictionary so as to not train the same chromosome multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PAXpEAWcwar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e741b12-4dab-4061-93be-1c92b79da9b9"
      },
      "source": [
        "#Create dictorary of already evaluated model:\n",
        "import os\n",
        "\n",
        "seen_models=None\n",
        "\n",
        "if (os.path.exists(\"/content/drive/MyDrive/CSC180_Final_Project/saved_dictionaries_transfer_learning/Evaluated_Models_Transfer_Learning.json\")):\n",
        "  seen_models=openDict(\"Evaluated_Models_Transfer_Learning.json\", path=\"/content/drive/MyDrive/CSC180_Final_Project/saved_dictionaries_transfer_learning/\")\n",
        "else:\n",
        "  seen_models={}\n",
        "\n",
        "print(seen_models)\n",
        "best_f1=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'[3, 9, 9, 9, 4, 1]': 0.3377031557765229, '[6, 5, 6, 5, 4, 6]': 0.3377031557765229, '[7, 8, 2, 7, 1, 1]': 0.3377031557765229, '[5, 7, 4, 4, 4, 6]': 0.544804381763419, '[3, 1, 6, 4, 8, 6]': 0.5334904894966727, '[8, 6, 5, 9, 0, 4]': 0.3377031557765229, '[0, 1, 5, 6, 4, 5]': 0.3377031557765229, '[1, 8, 6, 3, 1, 2]': 0.3377031557765229, '[6, 2, 0, 8, 7, 3]': 0.3377031557765229, '[8, 2, 1, 8, 6, 9]': 0.3377031557765229, '[9, 5, 1, 4, 7, 3]': 0.3377031557765229, '[1, 3, 9, 1, 3, 5]': 0.8179360998043937, '[5, 4, 3, 9, 0, 9]': 0.3377031557765229, '[1, 5, 9, 0, 2, 8]': 0.5434526534800941, '[6, 4, 3, 9, 2, 6]': 0.3896457014127283, '[5, 9, 4, 9, 5, 6]': 0.3377031557765229, '[3, 5, 8, 1, 3, 1]': 0.3377031557765229, '[7, 1, 0, 3, 4, 2]': 0.5366182170832514, '[3, 6, 8, 6, 2, 7]': 0.3377031557765229, '[9, 2, 0, 4, 5, 8]': 0.3377031557765229, '[8, 5, 8, 1, 6, 9]': 0.3377031557765229, '[3, 2, 1, 8, 3, 1]': 0.3377031557765229, '[5, 9, 9, 9, 5, 6]': 0.3377031557765229, '[3, 9, 4, 9, 4, 1]': 0.3377031557765229, '[3, 5, 6, 5, 3, 5]': 0.3377031557765229, '[6, 5, 8, 5, 4, 6]': 0.3377031557765229, '[1, 8, 2, 7, 1, 2]': 0.3377031557765229, '[7, 8, 6, 3, 1, 1]': 0.546632455891754, '[3, 6, 5, 1, 3, 1]': 0.7727706488314748, '[8, 5, 8, 9, 0, 4]': 0.3377031557765229, '[9, 5, 8, 1, 6, 9]': 0.3377031557765229, '[3, 3, 8, 1, 3, 1]': 0.7772944738995461, '[5, 9, 4, 9, 1, 6]': 0.3377031557765229, '[3, 5, 8, 1, 3, 2]': 0.3377031557765229, '[5, 9, 4, 9, 6, 9]': 0.3377031557765229, '[8, 5, 8, 1, 1, 6]': 0.8104610337049346, '[9, 5, 1, 4, 3, 3]': 0.3377031557765229, '[3, 2, 1, 8, 7, 1]': 0.3377031557765229, '[5, 2, 1, 8, 5, 6]': 0.3377031557765229, '[3, 9, 4, 9, 3, 1]': 0.3377031557765229, '[3, 0, 5, 6, 4, 5]': 0.3377031557765229, '[0, 5, 6, 5, 3, 5]': 0.3377031557765229, '[0, 1, 1, 8, 3, 5]': 0.3377031557765229, '[3, 2, 5, 6, 4, 1]': 0.3377031557765229, '[3, 2, 5, 6, 4, 5]': 0.3377031557765229, '[0, 1, 1, 8, 3, 1]': 0.3377031557765229, '[3, 2, 2, 8, 6, 9]': 0.3377031557765229, '[8, 5, 8, 1, 3, 1]': 0.3377031557765229, '[3, 9, 4, 9, 7, 1]': 0.3377031557765229, '[8, 5, 5, 1, 6, 9]': 0.3377031557765229, '[3, 2, 8, 6, 4, 1]': 0.3377031557765229, '[8, 5, 8, 1, 3, 9]': 0.3377031557765229, '[0, 1, 1, 8, 6, 5]': 0.3377031557765229, '[5, 9, 4, 9, 4, 9]': 0.3377031557765229, '[3, 2, 5, 6, 6, 5]': 0.3377031557765229, '[8, 9, 4, 9, 5, 9]': 0.3377031557765229, '[5, 0, 8, 1, 6, 9]': 0.7903884576917704, '[8, 5, 8, 4, 3, 9]': 0.3377031557765229, '[9, 5, 1, 1, 6, 3]': 0.3377031557765229, '[3, 5, 5, 6, 6, 5]': 0.3377031557765229, '[8, 2, 8, 1, 6, 9]': 0.3377031557765229, '[3, 9, 4, 6, 2, 7]': 0.3377031557765229, '[3, 6, 8, 9, 7, 1]': 0.3377031557765229, '[8, 2, 1, 8, 3, 1]': 0.3377031557765229, '[3, 9, 5, 1, 6, 9]': 0.8112770082466702, '[8, 9, 4, 9, 7, 1]': 0.3377031557765229, '[3, 5, 8, 1, 6, 9]': 0.3377031557765229, '[3, 9, 4, 9, 4, 5]': 0.3377031557765229, '[3, 2, 5, 6, 7, 1]': 0.3377031557765229, '[6, 9, 4, 9, 7, 1]': 0.3377031557765229, '[5, 2, 5, 6, 4, 5]': 0.3377031557765229, '[3, 1, 1, 8, 6, 5]': 0.3377031557765229, '[3, 5, 8, 5, 6, 5]': 0.3377031557765229, '[8, 2, 5, 6, 3, 1]': 0.3377031557765229, '[3, 2, 1, 8, 4, 1]': 0.3377031557765229, '[8, 5, 1, 8, 3, 1]': 0.3377031557765229, '[8, 2, 8, 1, 3, 9]': 0.3377031557765229, '[3, 1, 8, 0, 2, 7]': 0.8233178363071324, '[3, 9, 1, 9, 7, 1]': 0.3377031557765229, '[8, 2, 4, 8, 3, 1]': 0.3377031557765229, '[3, 9, 4, 9, 7, 6]': 0.3377031557765229, '[3, 2, 5, 6, 7, 5]': 0.3377031557765229, '[3, 9, 4, 9, 7, 2]': 0.3377031557765229, '[8, 9, 4, 8, 8, 1]': 0.3377031557765229, '[3, 5, 1, 9, 7, 1]': 0.3377031557765229, '[3, 5, 1, 9, 7, 5]': 0.3377031557765229, '[4, 4, 4, 9, 4, 1]': 0.3377031557765229, '[6, 9, 2, 3, 5, 9]': 0.5909451278849567, '[0, 3, 6, 0, 5, 3]': 0.3377031557765229, '[7, 4, 7, 3, 7, 0]': 0.5365249709849657, '[2, 6, 2, 0, 5, 6]': 0.3377031557765229, '[0, 5, 3, 5, 4, 3]': 0.3377031557765229, '[6, 5, 9, 9, 1, 6]': 0.3377031557765229, '[1, 5, 1, 1, 3, 3]': 0.3377031557765229, '[9, 6, 1, 2, 4, 8]': 0.5436928567634454, '[7, 6, 3, 1, 1, 9]': 0.3377031557765229, '[0, 3, 3, 3, 7, 0]': 0.538159644740286, '[7, 6, 3, 5, 1, 9]': 0.3377031557765229, '[0, 5, 3, 1, 4, 3]': 0.3377031557765229, '[0, 5, 9, 9, 4, 3]': 0.3377031557765229, '[6, 5, 3, 5, 1, 6]': 0.3377031557765229, '[2, 5, 2, 0, 5, 6]': 0.5379531513052739, '[7, 6, 9, 9, 1, 6]': 0.3377031557765229, '[2, 8, 3, 7, 4, 6]': 0.3377031557765229, '[0, 5, 2, 0, 5, 3]': 0.3377031557765229, '[0, 5, 7, 5, 4, 3]': 0.3377031557765229, '[0, 5, 3, 5, 4, 0]': 0.3377031557765229, '[0, 5, 3, 5, 1, 6]': 0.3377031557765229, '[6, 5, 2, 0, 5, 3]': 0.5292418482242357, '[7, 6, 9, 5, 1, 9]': 0.3377031557765229, '[7, 6, 3, 9, 1, 6]': 0.3377031557765229, '[2, 8, 3, 5, 4, 6]': 0.3377031557765229, '[0, 5, 3, 7, 1, 6]': 0.3377031557765229, '[3, 5, 3, 5, 1, 6]': 0.3377031557765229, '[0, 6, 3, 9, 1, 6]': 0.3377031557765229, '[0, 5, 3, 5, 8, 3]': 0.3377031557765229, '[3, 5, 3, 7, 1, 6]': 0.3377031557765229, '[0, 5, 3, 5, 3, 6]': 0.3377031557765229, '[0, 7, 2, 8, 1, 9]': 0.3377031557765229, '[0, 8, 2, 7, 1, 4]': 0.3377031557765229, '[3, 0, 6, 9, 5, 0]': 0.3377031557765229, '[6, 2, 3, 9, 0, 8]': 0.3377031557765229, '[0, 3, 1, 5, 3, 3]': 0.3377031557765229, '[5, 9, 3, 8, 0, 9]': 0.3377031557765229, '[7, 0, 9, 9, 7, 3]': 0.3377031557765229, '[4, 6, 0, 5, 8, 4]': 0.3377031557765229, '[8, 6, 2, 4, 5, 1]': 0.3377031557765229, '[5, 3, 6, 3, 9, 4]': 0.3377031557765229, '[6, 2, 2, 8, 0, 8]': 0.3377031557765229, '[0, 7, 3, 9, 1, 9]': 0.3377031557765229, '[5, 3, 6, 3, 7, 4]': 0.3377031557765229, '[9, 8, 2, 7, 1, 4]': 0.3377031557765229, '[0, 7, 2, 3, 1, 9]': 0.3377031557765229, '[5, 7, 6, 8, 7, 4]': 0.3377031557765229, '[5, 8, 2, 7, 1, 9]': 0.3377031557765229, '[9, 9, 3, 8, 0, 4]': 0.3377031557765229, '[9, 8, 2, 3, 7, 4]': 0.3377031557765229, '[0, 7, 2, 7, 6, 9]': 0.3377031557765229, '[6, 2, 2, 8, 0, 9]': 0.3377031557765229, '[5, 9, 3, 8, 0, 8]': 0.41454922728815413, '[5, 9, 3, 8, 0, 2]': 0.3377031557765229, '[5, 8, 2, 3, 7, 9]': 0.3377031557765229, '[0, 7, 2, 3, 7, 4]': 0.5293333424288055, '[9, 8, 2, 3, 1, 9]': 0.5408379338884391, '[9, 8, 2, 3, 4, 9]': 0.3377031557765229, '[5, 9, 3, 2, 0, 8]': 0.5933582973579808, '[1, 8, 2, 3, 1, 9]': 0.5403844215199675, '[9, 3, 2, 3, 1, 9]': 0.3377031557765229, '[0, 7, 2, 3, 7, 0]': 0.5287171139514711, '[0, 7, 3, 2, 0, 8]': 0.5344797701605678, '[5, 9, 2, 3, 7, 4]': 0.5090632954786699, '[5, 9, 3, 2, 0, 9]': 0.3377031557765229, '[5, 6, 7, 2, 6, 8]': 0.5411013875172789, '[5, 9, 3, 2, 0, 1]': 0.3377031557765229, '[4, 8, 3, 2, 1, 9]': 0.3377031557765229, '[5, 9, 2, 3, 0, 8]': 0.5355113198360291, '[4, 6, 4, 4, 8, 7]': 0.3377031557765229, '[5, 5, 4, 1, 4, 7]': 0.3377031557765229, '[9, 4, 0, 5, 5, 7]': 0.529773739853828, '[5, 3, 1, 2, 8, 2]': 0.5405687645156622, '[5, 3, 2, 5, 8, 2]': 0.3377031557765229, '[7, 7, 2, 3, 7, 9]': 0.5410090469880425, '[1, 9, 2, 5, 2, 7]': 0.3377031557765229, '[6, 1, 1, 1, 3, 3]': 0.3377031557765229, '[8, 3, 3, 2, 3, 0]': 0.7500586391007242, '[1, 3, 0, 1, 8, 7]': 0.544316381823104, '[8, 3, 0, 2, 3, 0]': 0.7753277833392039, '[1, 3, 3, 1, 8, 7]': 0.7769744224084659, '[1, 3, 3, 2, 3, 7]': 0.5300402633600215, '[8, 3, 0, 1, 8, 0]': 0.3377031557765229, '[7, 7, 2, 5, 7, 9]': 0.5397585637424956, '[1, 9, 2, 3, 2, 7]': 0.5476809626622889, '[4, 3, 3, 3, 7, 7]': 0.5330455627754511, '[7, 7, 2, 1, 8, 9]': 0.5421009251967693, '[1, 9, 2, 2, 3, 7]': 0.3377031557765229, '[8, 3, 0, 3, 2, 0]': 0.6156683291153927, '[8, 3, 3, 3, 2, 0]': 0.3377031557765229, '[8, 3, 3, 1, 8, 0]': 0.5250973944300279, '[8, 3, 3, 2, 3, 7]': 0.3377031557765229, '[1, 3, 3, 1, 8, 0]': 0.3377031557765229, '[1, 7, 3, 1, 8, 7]': 0.5453603982710357, '[8, 3, 3, 1, 8, 7]': 0.8098824210398013, '[8, 3, 3, 1, 7, 7]': 0.5299484355235159, '[1, 3, 3, 3, 2, 0]': 0.5403688819001129, '[5, 3, 3, 1, 8, 7]': 0.7839238809445681, '[9, 3, 3, 7, 8, 7]': 0.3377031557765229}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ujx4FHcwar"
      },
      "source": [
        "best=0\n",
        "\n",
        "# Evaluation function for DEAP\n",
        "def evaFitness(individual):\n",
        "    # Check if this model has been evaluated... if so return the previous score\n",
        "    model_as_string=str(individual)\n",
        "    if model_as_string in seen_models:\n",
        "        return (seen_models[model_as_string],)\n",
        "\n",
        "    # Else, create the model and train it\n",
        "    print(\"Training: \", model_as_string)\n",
        "\n",
        "    # Create checkpoints\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=4, verbose=1, mode='auto')\n",
        "    checkpointer = ModelCheckpoint(filepath='dnn/best_weights.hdf5', verbose=0, save_best_only=True)\n",
        "\n",
        "    # Get model\n",
        "    if (individual[5]%2==0):\n",
        "        model=build_transfer_model_mobile(individual)\n",
        "    else:\n",
        "        model=build_transfer_model_VGG(individual)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(x_train, y_train,     \n",
        "            batch_size=32,\n",
        "            epochs=1000,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[monitor, checkpointer])\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_weights('dnn/best_weights.hdf5')\n",
        "\n",
        "    # Score the model\n",
        "    pred = model.predict(x_test)\n",
        "    pred = np.argmax(pred,axis=1)\n",
        "    y_true = np.argmax(y_test,axis=1)\n",
        "    f1 = metrics.f1_score(y_true, pred, average='weighted')\n",
        "\n",
        "    # Save the model if it scores above a certain score\n",
        "    if (f1>0.7):\n",
        "        best=f1\n",
        "        model.save('/content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_'+str(f1)+\"_\"+model_as_string)\n",
        "\n",
        "    # Record the model as seen\n",
        "    seen_models[model_as_string]=f1\n",
        "\n",
        "    # Return evaluation\n",
        "    return (f1,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2mL3vUScwar"
      },
      "source": [
        "## Setup & Run GA\n",
        "After defining our evaluation function for our transfer learning models we are ready to create and run our GA. As before DEAP will be used to facilate this process. The same strategies we used for our custom CNN models will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H46vyxXJcwas"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from deap import algorithms, base, creator, tools\n",
        "\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWcL5ylbcwas"
      },
      "source": [
        "# Define our toolbox\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_int\", random.randint, 0, 9)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_int, n=6)\n",
        "\n",
        "#Register strategies\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"evaluate\", evaFitness)\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutUniformInt, low = 0, up = 9, indpb=0.1)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "#Statistics\n",
        "stats = tools.Statistics(key=lambda ind: ind.fitness.values)\n",
        "stats.register(\"avg\", np.mean)\n",
        "stats.register(\"max\", np.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxq30MNAcwas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c780b05c-fd79-4b21-9c15-ca59c7c02f33"
      },
      "source": [
        "#Define population and hall of fame\n",
        "pop = toolbox.population(n=20)\n",
        "hof = tools.HallOfFame(maxsize=1)\n",
        "\n",
        "#Run the GA\n",
        "pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.2, mutpb=0.7, ngen=15, stats=stats, halloffame=hof, verbose=True)\n",
        "\n",
        "#Save the dictionary\n",
        "saveDict(seen_models, \"Evaluated_Models_Transfer_Learning.json\", path=\"/content/drive/MyDrive/CSC180_Final_Project/saved_dictionaries_transfer_learning/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 0.8877 - val_loss: 0.7747\n",
            "Epoch 129/1000\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 0.8963 - val_loss: 0.7733\n",
            "Epoch 130/1000\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 0.8835 - val_loss: 0.7734\n",
            "Epoch 131/1000\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 0.8802 - val_loss: 0.7738\n",
            "Epoch 132/1000\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 0.8609 - val_loss: 0.7759\n",
            "Epoch 133/1000\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 0.8686 - val_loss: 0.7740\n",
            "Epoch 00133: early stopping\n",
            "1  \t18    \t0.485876\t0.780339\n",
            "Training:  [7, 5, 7, 1, 9, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 115ms/step - loss: 1.0969 - val_loss: 1.0610\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0569 - val_loss: 1.0405\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0399 - val_loss: 1.0324\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0437 - val_loss: 1.0287\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 1.0293 - val_loss: 1.0270\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0233 - val_loss: 1.0261\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0279 - val_loss: 1.0256\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0387 - val_loss: 1.0253\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 1.0262 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 1.0291 - val_loss: 1.0250\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0248 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 1.0287 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0334 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 1.0389 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0379 - val_loss: 1.0247\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0276 - val_loss: 1.0247\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 1.0274 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0359 - val_loss: 1.0247\n",
            "Epoch 00018: early stopping\n",
            "Training:  [7, 7, 7, 3, 1, 0]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 7s 44ms/step - loss: 1.0282 - val_loss: 0.9659\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.9316 - val_loss: 0.8909\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8856 - val_loss: 0.8541\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8541 - val_loss: 0.8041\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8511 - val_loss: 0.7679\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8326 - val_loss: 0.7468\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8158 - val_loss: 0.7347\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8078 - val_loss: 0.7210\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8001 - val_loss: 0.7120\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7944 - val_loss: 0.7060\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7727 - val_loss: 0.6995\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7792 - val_loss: 0.6961\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7679 - val_loss: 0.6927\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7629 - val_loss: 0.6944\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7802 - val_loss: 0.6853\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7719 - val_loss: 0.6876\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7732 - val_loss: 0.6813\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7584 - val_loss: 0.6795\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7550 - val_loss: 0.6799\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7587 - val_loss: 0.6780\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7557 - val_loss: 0.6776\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7558 - val_loss: 0.6774\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7423 - val_loss: 0.6783\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7553 - val_loss: 0.6719\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7573 - val_loss: 0.6731\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7680 - val_loss: 0.6734\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7562 - val_loss: 0.6729\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7607 - val_loss: 0.6732\n",
            "Epoch 00028: early stopping\n",
            "Training:  [2, 8, 6, 0, 5, 1]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 13s 103ms/step - loss: 1.0537 - val_loss: 1.0381\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0360 - val_loss: 1.0291\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0359 - val_loss: 1.0265\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0312 - val_loss: 1.0254\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0383 - val_loss: 1.0251\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0296 - val_loss: 1.0249\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0323 - val_loss: 1.0248\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0407 - val_loss: 1.0247\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 12s 102ms/step - loss: 1.0279 - val_loss: 1.0248\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0283 - val_loss: 1.0247\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0373 - val_loss: 1.0247\n",
            "Epoch 00011: early stopping\n",
            "Training:  [7, 8, 7, 1, 9, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 115ms/step - loss: 1.1232 - val_loss: 1.0751\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 1.0694 - val_loss: 1.0460\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0435 - val_loss: 1.0342\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0325 - val_loss: 1.0291\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0332 - val_loss: 1.0271\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0358 - val_loss: 1.0260\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0288 - val_loss: 1.0256\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0343 - val_loss: 1.0252\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0330 - val_loss: 1.0251\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0302 - val_loss: 1.0250\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0357 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 1.0272 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0286 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0310 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0256 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0360 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0381 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0259 - val_loss: 1.0248\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0385 - val_loss: 1.0248\n",
            "Epoch 00019: early stopping\n",
            "Training:  [4, 9, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.2798 - val_loss: 1.0699\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.1365 - val_loss: 1.0509\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.1007 - val_loss: 1.0399\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0606 - val_loss: 1.0334\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0510 - val_loss: 1.0299\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0296 - val_loss: 1.0278\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0336 - val_loss: 1.0265\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0299 - val_loss: 1.0259\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0299 - val_loss: 1.0255\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0416 - val_loss: 1.0252\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0425 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0351 - val_loss: 1.0250\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0301 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0396 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0289 - val_loss: 1.0249\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0245 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0477 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0195 - val_loss: 1.0247\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0297 - val_loss: 1.0248\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0276 - val_loss: 1.0247\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0393 - val_loss: 1.0248\n",
            "Epoch 00021: early stopping\n",
            "Training:  [7, 7, 7, 5, 5, 9]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.1010 - val_loss: 1.0396\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0700 - val_loss: 1.0292\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0460 - val_loss: 1.0246\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0349 - val_loss: 1.0242\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0364 - val_loss: 1.0219\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0489 - val_loss: 1.0203\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0305 - val_loss: 1.0183\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0354 - val_loss: 1.0159\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0285 - val_loss: 1.0191\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0301 - val_loss: 1.0129\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0403 - val_loss: 0.9968\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0237 - val_loss: 0.9798\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0230 - val_loss: 0.9699\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0123 - val_loss: 0.9613\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0110 - val_loss: 0.9469\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9937 - val_loss: 0.9306\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9905 - val_loss: 0.9113\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9853 - val_loss: 0.8952\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9685 - val_loss: 0.8783\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9704 - val_loss: 0.8608\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9647 - val_loss: 0.8418\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9519 - val_loss: 0.8294\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9342 - val_loss: 0.8163\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9364 - val_loss: 0.8366\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9329 - val_loss: 0.7939\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8937 - val_loss: 0.7766\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8984 - val_loss: 0.7726\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8827 - val_loss: 0.7572\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8768 - val_loss: 0.7508\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8873 - val_loss: 0.7489\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8873 - val_loss: 0.7398\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8762 - val_loss: 0.7370\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8742 - val_loss: 0.7326\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8784 - val_loss: 0.7295\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8732 - val_loss: 0.7265\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8590 - val_loss: 0.7284\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8786 - val_loss: 0.7239\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8625 - val_loss: 0.7225\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8780 - val_loss: 0.7191\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8734 - val_loss: 0.7200\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8745 - val_loss: 0.7166\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8273 - val_loss: 0.7145\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8747 - val_loss: 0.7219\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8582 - val_loss: 0.7113\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8484 - val_loss: 0.7107\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8557 - val_loss: 0.7117\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8647 - val_loss: 0.7095\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8508 - val_loss: 1.1615\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0505 - val_loss: 0.7249\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8736 - val_loss: 0.7177\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8483 - val_loss: 0.7266\n",
            "Epoch 00051: early stopping\n",
            "Training:  [7, 9, 0, 1, 6, 9]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 86ms/step - loss: 1.0916 - val_loss: 1.0110\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0499 - val_loss: 0.9707\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.9803 - val_loss: 1.0205\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.9404 - val_loss: 0.8824\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.8507 - val_loss: 0.8246\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.8256 - val_loss: 0.8321\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.8496 - val_loss: 0.7220\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7912 - val_loss: 0.7195\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7826 - val_loss: 0.6943\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7714 - val_loss: 0.7036\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7482 - val_loss: 0.6750\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7543 - val_loss: 0.6614\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7456 - val_loss: 0.6506\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7368 - val_loss: 0.6533\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7485 - val_loss: 0.6392\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7392 - val_loss: 0.7736\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7267 - val_loss: 0.6729\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7199 - val_loss: 0.6530\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7011 - val_loss: 0.6376\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7054 - val_loss: 0.6163\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7026 - val_loss: 0.6353\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.6977 - val_loss: 0.6256\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7091 - val_loss: 0.6308\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7094 - val_loss: 0.6113\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7022 - val_loss: 0.6086\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.6726 - val_loss: 0.6142\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7026 - val_loss: 0.6423\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.6990 - val_loss: 0.6098\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.6816 - val_loss: 0.6593\n",
            "Epoch 00029: early stopping\n",
            "Training:  [7, 9, 7, 1, 1, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0877 - val_loss: 1.0588\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0575 - val_loss: 1.0410\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0447 - val_loss: 1.0329\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0420 - val_loss: 1.0292\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0317 - val_loss: 1.0273\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0371 - val_loss: 1.0263\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0308 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0200 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0249 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0310 - val_loss: 1.0250\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0284 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0314 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0326 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0463 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0200 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0292 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0341 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0319 - val_loss: 1.0248\n",
            "Epoch 00018: early stopping\n",
            "Training:  [7, 9, 7, 1, 0, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.1319 - val_loss: 1.0595\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0559 - val_loss: 1.0413\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0443 - val_loss: 1.0332\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0313 - val_loss: 1.0294\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0375 - val_loss: 1.0275\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0402 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0267 - val_loss: 1.0259\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0373 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0318 - val_loss: 1.0253\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0317 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0306 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0334 - val_loss: 1.0250\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0312 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0358 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0397 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0360 - val_loss: 1.0247\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0346 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0261 - val_loss: 1.0247\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0405 - val_loss: 1.0247\n",
            "Epoch 00019: early stopping\n",
            "Training:  [7, 9, 7, 1, 5, 4]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 8s 45ms/step - loss: 1.0869 - val_loss: 1.0591\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0544 - val_loss: 1.0413\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0476 - val_loss: 1.0330\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0419 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0337 - val_loss: 1.0273\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0319 - val_loss: 1.0263\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0300 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0363 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0311 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0358 - val_loss: 1.0250\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0314 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0312 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0339 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0281 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0236 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0348 - val_loss: 1.0248\n",
            "Epoch 00016: early stopping\n",
            "Training:  [0, 1, 0, 4, 7, 5]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 87ms/step - loss: 1.1324 - val_loss: 1.0470\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0782 - val_loss: 1.0353\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0667 - val_loss: 1.0293\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0589 - val_loss: 1.0269\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0448 - val_loss: 1.0261\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0449 - val_loss: 1.0258\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0350 - val_loss: 1.0253\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0432 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0437 - val_loss: 1.0249\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0400 - val_loss: 1.0248\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0361 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0329 - val_loss: 1.0248\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0265 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0244 - val_loss: 1.0249\n",
            "Epoch 00014: early stopping\n",
            "Training:  [6, 9, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.2060 - val_loss: 1.0693\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.1379 - val_loss: 1.0507\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0645 - val_loss: 1.0401\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.1350 - val_loss: 1.0337\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0565 - val_loss: 1.0298\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0373 - val_loss: 1.0278\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0414 - val_loss: 1.0266\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0312 - val_loss: 1.0259\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0306 - val_loss: 1.0254\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0321 - val_loss: 1.0252\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0283 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0328 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0362 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0343 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0936 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0440 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.2151 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 3.4621 - val_loss: 1.0247\n",
            "Epoch 00018: early stopping\n",
            "2  \t18    \t0.522454\t0.780339\n",
            "Training:  [7, 9, 0, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 86ms/step - loss: 1.0872 - val_loss: 1.0591\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0560 - val_loss: 1.0412\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0455 - val_loss: 1.0330\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0322 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0330 - val_loss: 1.0274\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0318 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0328 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0265 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0341 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0297 - val_loss: 1.0250\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0243 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0292 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0408 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0399 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0270 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0230 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0261 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0368 - val_loss: 1.0247\n",
            "Epoch 00018: early stopping\n",
            "Training:  [7, 9, 7, 1, 8, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0754 - val_loss: 1.0275\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0374 - val_loss: 0.9812\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9959 - val_loss: 0.8777\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9252 - val_loss: 0.7948\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8677 - val_loss: 0.7866\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8464 - val_loss: 0.7670\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8381 - val_loss: 0.7516\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8353 - val_loss: 0.7435\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8191 - val_loss: 0.7360\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8180 - val_loss: 0.7444\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8333 - val_loss: 0.7284\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8109 - val_loss: 0.7705\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8193 - val_loss: 0.7258\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8022 - val_loss: 0.9031\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8188 - val_loss: 0.7210\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8022 - val_loss: 0.7108\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8045 - val_loss: 0.7017\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7984 - val_loss: 0.6954\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8061 - val_loss: 0.7060\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8036 - val_loss: 0.6804\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7933 - val_loss: 0.6737\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7755 - val_loss: 0.6708\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7586 - val_loss: 0.6668\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7649 - val_loss: 0.6898\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7421 - val_loss: 0.6426\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7428 - val_loss: 0.6923\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7381 - val_loss: 0.6925\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7445 - val_loss: 0.6386\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7477 - val_loss: 0.6140\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7456 - val_loss: 0.6400\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7469 - val_loss: 0.6244\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7241 - val_loss: 0.6274\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7194 - val_loss: 0.6099\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7180 - val_loss: 0.6446\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7219 - val_loss: 0.6345\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7449 - val_loss: 0.6790\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7488 - val_loss: 0.6679\n",
            "Epoch 00037: early stopping\n",
            "Training:  [7, 7, 7, 6, 1, 0]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 7s 44ms/step - loss: 1.2153 - val_loss: 1.0073\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.1296 - val_loss: 1.0090\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0793 - val_loss: 0.9771\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0179 - val_loss: 0.9876\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 1.0206 - val_loss: 0.9604\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.9962 - val_loss: 0.9618\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.9874 - val_loss: 0.9831\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 0.9920 - val_loss: 0.9892\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 38ms/step - loss: 0.9776 - val_loss: 0.9859\n",
            "Epoch 00009: early stopping\n",
            "Training:  [2, 9, 7, 1, 2, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 114ms/step - loss: 1.0909 - val_loss: 1.0704\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0672 - val_loss: 1.0521\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0505 - val_loss: 1.0405\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0359 - val_loss: 1.0337\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0390 - val_loss: 1.0298\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0364 - val_loss: 1.0278\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0373 - val_loss: 1.0266\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0323 - val_loss: 1.0259\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0286 - val_loss: 1.0254\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0381 - val_loss: 1.0252\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0326 - val_loss: 1.0251\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0267 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0271 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0206 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0434 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0296 - val_loss: 1.0247\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0358 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0437 - val_loss: 1.0247\n",
            "Epoch 00018: early stopping\n",
            "Training:  [7, 9, 7, 1, 5, 9]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0870 - val_loss: 1.0592\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0545 - val_loss: 1.0414\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0405 - val_loss: 1.0332\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0363 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0347 - val_loss: 1.0274\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0301 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0301 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0260 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0357 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0352 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0419 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0315 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0269 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0330 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0331 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0299 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0293 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0399 - val_loss: 1.0248\n",
            "Epoch 00018: early stopping\n",
            "3  \t14    \t0.652785\t0.780339\n",
            "Training:  [8, 9, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0903 - val_loss: 1.0690\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0658 - val_loss: 1.0506\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0513 - val_loss: 1.0396\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0448 - val_loss: 1.0334\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0332 - val_loss: 1.0296\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0349 - val_loss: 1.0277\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0370 - val_loss: 1.0265\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0390 - val_loss: 1.0257\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0347 - val_loss: 1.0254\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0374 - val_loss: 1.0252\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0385 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0334 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0328 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0286 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0330 - val_loss: 1.0247\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0278 - val_loss: 1.0247\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0310 - val_loss: 1.0248\n",
            "Epoch 00017: early stopping\n",
            "Training:  [5, 9, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0875 - val_loss: 1.0590\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0600 - val_loss: 1.0409\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0485 - val_loss: 1.0329\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0334 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0292 - val_loss: 1.0275\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0323 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0347 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0308 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0479 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0267 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0220 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0268 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0277 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0268 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0330 - val_loss: 1.0247\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0410 - val_loss: 1.0247\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0352 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0365 - val_loss: 1.0247\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0225 - val_loss: 1.0248\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0252 - val_loss: 1.0247\n",
            "Epoch 00020: early stopping\n",
            "Training:  [1, 9, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0864 - val_loss: 1.0494\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0449 - val_loss: 1.0118\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0002 - val_loss: 0.8506\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9044 - val_loss: 0.7991\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8764 - val_loss: 0.7910\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8726 - val_loss: 0.7825\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8558 - val_loss: 0.7589\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8471 - val_loss: 0.7538\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8409 - val_loss: 0.7408\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8312 - val_loss: 0.7520\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8336 - val_loss: 0.7268\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8369 - val_loss: 0.7308\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8246 - val_loss: 0.7205\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8246 - val_loss: 0.7243\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8252 - val_loss: 0.7068\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8074 - val_loss: 0.7021\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8225 - val_loss: 0.7890\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8304 - val_loss: 0.7066\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8186 - val_loss: 0.6794\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8076 - val_loss: 0.7034\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7998 - val_loss: 0.6668\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7988 - val_loss: 0.6948\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7840 - val_loss: 0.8178\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8030 - val_loss: 0.6388\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7714 - val_loss: 0.6241\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7829 - val_loss: 0.6492\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7725 - val_loss: 0.6080\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7712 - val_loss: 0.6007\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7356 - val_loss: 0.5936\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7388 - val_loss: 0.6303\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7315 - val_loss: 0.6410\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7263 - val_loss: 0.6329\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7018 - val_loss: 0.6049\n",
            "Epoch 00033: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.7905434387679098_[1, 9, 7, 1, 5, 7]/assets\n",
            "Training:  [7, 9, 7, 9, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 114ms/step - loss: 1.0874 - val_loss: 1.0589\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0562 - val_loss: 1.0415\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0393 - val_loss: 1.0333\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0340 - val_loss: 1.0296\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0320 - val_loss: 1.0275\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0322 - val_loss: 1.0265\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0294 - val_loss: 1.0259\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0379 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0308 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0269 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0372 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0277 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0257 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0305 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0328 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0320 - val_loss: 1.0247\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0304 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0407 - val_loss: 1.0247\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0303 - val_loss: 1.0247\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0355 - val_loss: 1.0247\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0286 - val_loss: 1.0247\n",
            "Epoch 00021: early stopping\n",
            "Training:  [7, 7, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0381 - val_loss: 0.7721\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8199 - val_loss: 0.9351\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7774 - val_loss: 0.7170\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7562 - val_loss: 0.6997\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7388 - val_loss: 0.7446\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7423 - val_loss: 0.7093\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7388 - val_loss: 0.6864\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7293 - val_loss: 0.6814\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7295 - val_loss: 0.6738\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7249 - val_loss: 0.6715\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7063 - val_loss: 0.6770\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6984 - val_loss: 0.6694\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6911 - val_loss: 0.6607\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6971 - val_loss: 0.6644\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6882 - val_loss: 0.6599\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6983 - val_loss: 0.6565\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6860 - val_loss: 0.6604\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6974 - val_loss: 0.6546\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6978 - val_loss: 0.6531\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6918 - val_loss: 0.6471\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6973 - val_loss: 0.6517\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6858 - val_loss: 0.6520\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6874 - val_loss: 0.6474\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6967 - val_loss: 0.6417\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6678 - val_loss: 0.6422\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6639 - val_loss: 0.6420\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6731 - val_loss: 0.6463\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6831 - val_loss: 0.6455\n",
            "Epoch 00028: early stopping\n",
            "Training:  [7, 0, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0879 - val_loss: 1.0588\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0584 - val_loss: 1.0411\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0465 - val_loss: 1.0330\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0378 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0323 - val_loss: 1.0274\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0343 - val_loss: 1.0263\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0320 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0277 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0326 - val_loss: 1.0253\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0318 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0367 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0352 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0437 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0310 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0346 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0351 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0435 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0385 - val_loss: 1.0248\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0262 - val_loss: 1.0248\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0288 - val_loss: 1.0247\n",
            "Epoch 00020: early stopping\n",
            "Training:  [7, 3, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0868 - val_loss: 1.0592\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0606 - val_loss: 1.0409\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0374 - val_loss: 1.0330\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0404 - val_loss: 1.0292\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0305 - val_loss: 1.0273\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0238 - val_loss: 1.0263\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0194 - val_loss: 1.0257\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0203 - val_loss: 1.0253\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0255 - val_loss: 1.0251\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0211 - val_loss: 1.0249\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0210 - val_loss: 1.0247\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0118 - val_loss: 1.0245\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0069 - val_loss: 0.7785\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8224 - val_loss: 0.7582\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8100 - val_loss: 0.7086\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8090 - val_loss: 0.6867\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7721 - val_loss: 0.6722\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7704 - val_loss: 0.6932\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7710 - val_loss: 0.6432\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7583 - val_loss: 0.6351\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7406 - val_loss: 0.6385\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7384 - val_loss: 0.6097\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7144 - val_loss: 0.5894\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7100 - val_loss: 0.5828\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6852 - val_loss: 0.5841\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6818 - val_loss: 0.5554\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6809 - val_loss: 0.5480\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6769 - val_loss: 0.5610\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6748 - val_loss: 0.5440\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6733 - val_loss: 0.5330\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6744 - val_loss: 0.5289\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6841 - val_loss: 0.5620\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6866 - val_loss: 0.5220\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6479 - val_loss: 0.5285\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6831 - val_loss: 0.5198\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6629 - val_loss: 0.5275\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6469 - val_loss: 0.5436\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6634 - val_loss: 0.5201\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6507 - val_loss: 0.5088\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6565 - val_loss: 0.5223\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6348 - val_loss: 0.5061\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6684 - val_loss: 0.5008\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6384 - val_loss: 0.4871\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6538 - val_loss: 0.5125\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6458 - val_loss: 0.5455\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6288 - val_loss: 0.4910\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6223 - val_loss: 0.4992\n",
            "Epoch 00047: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.8217909837519634_[7, 3, 7, 1, 5, 7]/assets\n",
            "4  \t14    \t0.660389\t0.821791\n",
            "Training:  [7, 6, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0882 - val_loss: 1.0587\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0544 - val_loss: 1.0413\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0385 - val_loss: 1.0334\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0371 - val_loss: 1.0295\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0356 - val_loss: 1.0275\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0330 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0370 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0334 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0340 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0338 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0381 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0342 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0308 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0234 - val_loss: 1.0249\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0305 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0285 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0341 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0306 - val_loss: 1.0248\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0288 - val_loss: 1.0247\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0339 - val_loss: 1.0247\n",
            "Epoch 00020: early stopping\n",
            "Training:  [0, 6, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 115ms/step - loss: 1.1340 - val_loss: 1.0709\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0674 - val_loss: 1.0523\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0527 - val_loss: 1.0406\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0402 - val_loss: 1.0341\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0322 - val_loss: 1.0300\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0372 - val_loss: 1.0279\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0307 - val_loss: 1.0267\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0313 - val_loss: 1.0259\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0260 - val_loss: 1.0254\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0300 - val_loss: 1.0252\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0384 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0332 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0354 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0297 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0328 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0326 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0291 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0349 - val_loss: 1.0248\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0251 - val_loss: 1.0247\n",
            "Epoch 00019: early stopping\n",
            "Training:  [7, 9, 7, 8, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 44000549746.9274 - val_loss: 1.0600\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0581 - val_loss: 1.0415\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0467 - val_loss: 1.0333\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0366 - val_loss: 1.0294\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0321 - val_loss: 1.0274\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0397 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0401 - val_loss: 1.0257\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0430 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0370 - val_loss: 1.0251\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0316 - val_loss: 1.0250\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0351 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0365 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0430 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0406 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0244 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0324 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0201 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0353 - val_loss: 1.0248\n",
            "Epoch 00018: early stopping\n",
            "Training:  [8, 9, 7, 1, 3, 2]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 8s 47ms/step - loss: 1.0909 - val_loss: 1.0702\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0669 - val_loss: 1.0517\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0520 - val_loss: 1.0403\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0437 - val_loss: 1.0340\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0418 - val_loss: 1.0299\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0333 - val_loss: 1.0278\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0373 - val_loss: 1.0266\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0391 - val_loss: 1.0258\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0352 - val_loss: 1.0254\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0238 - val_loss: 1.0252\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0377 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0331 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0251 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0302 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0292 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0337 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0388 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0295 - val_loss: 1.0247\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0355 - val_loss: 1.0248\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0302 - val_loss: 1.0247\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0270 - val_loss: 1.0247\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0275 - val_loss: 1.0247\n",
            "Epoch 00022: early stopping\n",
            "Training:  [7, 9, 7, 3, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0936 - val_loss: 1.0586\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0594 - val_loss: 1.0411\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0426 - val_loss: 1.0331\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0402 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0319 - val_loss: 1.0274\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0242 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0244 - val_loss: 1.0259\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0372 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0401 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0267 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0346 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0403 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0395 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0247 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0326 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0263 - val_loss: 1.0247\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0336 - val_loss: 1.0247\n",
            "Epoch 00017: early stopping\n",
            "Training:  [7, 9, 8, 1, 3, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 120ms/step - loss: 1.0872 - val_loss: 1.0592\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0561 - val_loss: 1.0413\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0419 - val_loss: 1.0332\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0366 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0319 - val_loss: 1.0274\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0348 - val_loss: 1.0263\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0422 - val_loss: 1.0257\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0215 - val_loss: 1.0253\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0276 - val_loss: 1.0239\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0241 - val_loss: 1.0216\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0206 - val_loss: 0.9970\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.9465 - val_loss: 0.9013\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.8537 - val_loss: 0.7778\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.8208 - val_loss: 0.7551\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.8194 - val_loss: 0.7320\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7809 - val_loss: 0.7094\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7706 - val_loss: 0.6881\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7652 - val_loss: 0.6743\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7464 - val_loss: 0.6599\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7465 - val_loss: 0.6629\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7483 - val_loss: 0.6381\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7104 - val_loss: 0.6564\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7329 - val_loss: 0.6247\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7098 - val_loss: 0.6141\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6976 - val_loss: 0.5991\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6621 - val_loss: 0.5873\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6559 - val_loss: 0.6238\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6648 - val_loss: 0.5674\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6391 - val_loss: 0.5491\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6265 - val_loss: 0.5407\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6276 - val_loss: 0.5237\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6025 - val_loss: 0.5353\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5981 - val_loss: 0.5602\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6321 - val_loss: 0.5214\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5965 - val_loss: 0.5434\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5998 - val_loss: 0.5122\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6056 - val_loss: 0.5247\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6080 - val_loss: 0.5033\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5912 - val_loss: 0.5228\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5913 - val_loss: 0.5183\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5830 - val_loss: 0.4945\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6054 - val_loss: 0.4979\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5817 - val_loss: 0.5078\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5820 - val_loss: 0.4940\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5707 - val_loss: 0.4874\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5698 - val_loss: 0.4928\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5875 - val_loss: 0.4941\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5719 - val_loss: 0.5031\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5546 - val_loss: 0.4823\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5633 - val_loss: 0.4810\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5393 - val_loss: 0.8291\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5701 - val_loss: 0.5727\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5533 - val_loss: 0.4937\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5369 - val_loss: 0.4743\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5418 - val_loss: 0.4718\n",
            "Epoch 56/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5348 - val_loss: 0.5025\n",
            "Epoch 57/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5342 - val_loss: 0.5307\n",
            "Epoch 58/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5581 - val_loss: 0.4665\n",
            "Epoch 59/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5293 - val_loss: 0.4761\n",
            "Epoch 60/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5288 - val_loss: 0.5005\n",
            "Epoch 61/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5002 - val_loss: 0.4853\n",
            "Epoch 62/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5151 - val_loss: 0.4637\n",
            "Epoch 63/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5005 - val_loss: 0.5051\n",
            "Epoch 64/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5186 - val_loss: 0.5440\n",
            "Epoch 65/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5243 - val_loss: 0.4676\n",
            "Epoch 66/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5496 - val_loss: 0.4766\n",
            "Epoch 00066: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.8059207732246773_[7, 9, 8, 1, 3, 7]/assets\n",
            "5  \t15    \t0.678197\t0.821791\n",
            "Training:  [7, 3, 3, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 12s 92ms/step - loss: 1.0917 - val_loss: 1.0587\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0575 - val_loss: 1.0411\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0395 - val_loss: 1.0330\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0346 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0311 - val_loss: 1.0275\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0355 - val_loss: 1.0265\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0339 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0383 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0299 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0366 - val_loss: 0.9731\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9842 - val_loss: 0.8831\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9137 - val_loss: 0.8431\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8761 - val_loss: 0.8060\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8577 - val_loss: 0.7913\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8634 - val_loss: 0.7677\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8590 - val_loss: 0.7523\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8504 - val_loss: 0.7470\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8151 - val_loss: 0.7373\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8022 - val_loss: 0.7263\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8093 - val_loss: 0.7222\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7850 - val_loss: 0.7289\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7954 - val_loss: 0.7267\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7957 - val_loss: 0.7135\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7723 - val_loss: 0.7074\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7534 - val_loss: 0.6988\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7642 - val_loss: 0.6956\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7603 - val_loss: 0.6909\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7689 - val_loss: 0.6911\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7455 - val_loss: 0.6881\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7492 - val_loss: 0.6912\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7550 - val_loss: 0.6863\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7540 - val_loss: 0.6773\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7616 - val_loss: 0.6754\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7541 - val_loss: 0.6887\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7429 - val_loss: 0.6784\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7577 - val_loss: 0.6762\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7332 - val_loss: 0.6942\n",
            "Epoch 00037: early stopping\n",
            "Training:  [7, 1, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0940 - val_loss: 1.0198\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0185 - val_loss: 0.9567\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9623 - val_loss: 0.9627\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9278 - val_loss: 0.8824\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8825 - val_loss: 0.8366\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8565 - val_loss: 0.8016\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8305 - val_loss: 0.7982\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8158 - val_loss: 0.7839\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8039 - val_loss: 0.7551\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7867 - val_loss: 0.7287\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7722 - val_loss: 0.7114\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7492 - val_loss: 0.6995\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7358 - val_loss: 0.6797\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7204 - val_loss: 0.6690\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7006 - val_loss: 0.6530\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7032 - val_loss: 0.6461\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6907 - val_loss: 0.6527\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6733 - val_loss: 0.6410\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6899 - val_loss: 0.6336\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6687 - val_loss: 0.6377\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6790 - val_loss: 0.6161\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6664 - val_loss: 0.6104\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6609 - val_loss: 0.6089\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6626 - val_loss: 0.6008\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6626 - val_loss: 0.5981\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6475 - val_loss: 0.5960\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6565 - val_loss: 0.5951\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6437 - val_loss: 0.5849\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6454 - val_loss: 0.5785\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6333 - val_loss: 0.5740\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6392 - val_loss: 0.5729\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6542 - val_loss: 0.5764\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6312 - val_loss: 0.5666\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6195 - val_loss: 0.5710\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6319 - val_loss: 0.5664\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6392 - val_loss: 0.5604\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6345 - val_loss: 0.5557\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6282 - val_loss: 0.5513\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6115 - val_loss: 0.5541\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6187 - val_loss: 0.5575\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6182 - val_loss: 0.5526\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6213 - val_loss: 0.5388\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5969 - val_loss: 0.5457\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6106 - val_loss: 0.5512\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6098 - val_loss: 0.5370\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6054 - val_loss: 0.5382\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6107 - val_loss: 0.5433\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5949 - val_loss: 0.5297\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6010 - val_loss: 0.5262\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5949 - val_loss: 0.5258\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5729 - val_loss: 0.5285\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5960 - val_loss: 0.5379\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6055 - val_loss: 0.5268\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5983 - val_loss: 0.5252\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5913 - val_loss: 0.5255\n",
            "Epoch 56/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6113 - val_loss: 0.5178\n",
            "Epoch 57/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5848 - val_loss: 0.5146\n",
            "Epoch 58/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5754 - val_loss: 0.5148\n",
            "Epoch 59/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5785 - val_loss: 0.5106\n",
            "Epoch 60/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5696 - val_loss: 0.5129\n",
            "Epoch 61/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.5628 - val_loss: 0.5142\n",
            "Epoch 62/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.5544 - val_loss: 0.5214\n",
            "Epoch 63/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.5686 - val_loss: 0.5125\n",
            "Epoch 00063: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.8266718456410842_[7, 1, 7, 1, 5, 7]/assets\n",
            "6  \t15    \t0.763891\t0.826672\n",
            "Training:  [1, 9, 9, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 121ms/step - loss: 1.0875 - val_loss: 1.0592\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0584 - val_loss: 1.0413\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0415 - val_loss: 1.0331\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0371 - val_loss: 1.0292\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0346 - val_loss: 1.0274\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0299 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0405 - val_loss: 1.0257\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0326 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0316 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0407 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0306 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0317 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0371 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0411 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0327 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0342 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0426 - val_loss: 1.0247\n",
            "Epoch 00017: early stopping\n",
            "Training:  [9, 9, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 114ms/step - loss: 1.0868 - val_loss: 1.0592\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0529 - val_loss: 1.0415\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0444 - val_loss: 1.0333\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0374 - val_loss: 1.0294\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0330 - val_loss: 1.0275\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0383 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0336 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0367 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0333 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0327 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0422 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0373 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0290 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0273 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0300 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0342 - val_loss: 1.0247\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0279 - val_loss: 1.0247\n",
            "Epoch 00017: early stopping\n",
            "Training:  [7, 3, 9, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 122ms/step - loss: 0.9670 - val_loss: 0.7847\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.8390 - val_loss: 0.7671\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.8136 - val_loss: 0.7593\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.8003 - val_loss: 0.7393\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7931 - val_loss: 0.7241\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7871 - val_loss: 0.7125\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7772 - val_loss: 0.7057\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7675 - val_loss: 0.7321\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7621 - val_loss: 0.6879\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7588 - val_loss: 0.6855\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7575 - val_loss: 0.6791\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7315 - val_loss: 0.6948\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7500 - val_loss: 0.6579\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7378 - val_loss: 0.6607\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7314 - val_loss: 0.6343\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7227 - val_loss: 0.6367\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7132 - val_loss: 0.6076\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.6872 - val_loss: 0.6016\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6780 - val_loss: 0.5744\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6821 - val_loss: 0.5621\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6495 - val_loss: 0.6253\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6673 - val_loss: 0.5678\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6667 - val_loss: 0.6002\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6402 - val_loss: 0.5369\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6367 - val_loss: 0.5385\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6411 - val_loss: 0.5334\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6509 - val_loss: 0.6052\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6537 - val_loss: 0.5398\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6398 - val_loss: 0.5279\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6321 - val_loss: 0.7687\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6406 - val_loss: 0.5318\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6447 - val_loss: 0.5133\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6053 - val_loss: 0.5311\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6169 - val_loss: 0.5104\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6141 - val_loss: 0.5259\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6220 - val_loss: 0.5397\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6201 - val_loss: 0.6206\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5998 - val_loss: 0.5074\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6260 - val_loss: 0.4976\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5994 - val_loss: 0.5994\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5913 - val_loss: 0.4903\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6074 - val_loss: 0.5008\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.5784 - val_loss: 0.5376\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6289 - val_loss: 0.4967\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6060 - val_loss: 0.6301\n",
            "Epoch 00045: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.8211332235656574_[7, 3, 9, 1, 5, 7]/assets\n",
            "Training:  [7, 1, 9, 1, 6, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 120ms/step - loss: 1.0421 - val_loss: 0.9081\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.9072 - val_loss: 0.8201\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.8309 - val_loss: 0.7647\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.7916 - val_loss: 0.7373\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7603 - val_loss: 0.7083\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7346 - val_loss: 0.7036\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7324 - val_loss: 0.6861\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7243 - val_loss: 0.6826\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7162 - val_loss: 0.6769\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7134 - val_loss: 0.6717\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6970 - val_loss: 0.6637\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7251 - val_loss: 0.6791\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6948 - val_loss: 0.6634\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6888 - val_loss: 0.6629\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6911 - val_loss: 0.6562\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.6960 - val_loss: 0.6554\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6824 - val_loss: 0.6524\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6865 - val_loss: 0.6625\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.6924 - val_loss: 0.6468\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.6869 - val_loss: 0.6491\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.6730 - val_loss: 0.6458\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6784 - val_loss: 0.6487\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6831 - val_loss: 0.6492\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6791 - val_loss: 0.6514\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6757 - val_loss: 0.6440\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6807 - val_loss: 0.6509\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6894 - val_loss: 0.6472\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.6713 - val_loss: 0.6398\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.6912 - val_loss: 0.6440\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6701 - val_loss: 0.6410\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6722 - val_loss: 0.6466\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 0.6850 - val_loss: 0.6419\n",
            "Epoch 00032: early stopping\n",
            "Training:  [1, 9, 8, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 120ms/step - loss: 1.0868 - val_loss: 1.0592\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0557 - val_loss: 1.0415\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0504 - val_loss: 1.0332\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0429 - val_loss: 1.0292\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0319 - val_loss: 1.0273\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0310 - val_loss: 1.0263\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0225 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0292 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0316 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0367 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0267 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0305 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0391 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0224 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0281 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0254 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0339 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0267 - val_loss: 1.0248\n",
            "Epoch 00018: early stopping\n",
            "Training:  [9, 3, 7, 1, 2, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0869 - val_loss: 1.0579\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0097 - val_loss: 0.8223\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8564 - val_loss: 0.8300\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8259 - val_loss: 0.8254\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7965 - val_loss: 0.7102\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7799 - val_loss: 0.6896\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7682 - val_loss: 0.6986\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7342 - val_loss: 0.7224\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7323 - val_loss: 0.6703\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7368 - val_loss: 0.6384\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7249 - val_loss: 0.7288\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7252 - val_loss: 0.6200\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7219 - val_loss: 0.6365\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6974 - val_loss: 0.6270\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7073 - val_loss: 0.6131\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7050 - val_loss: 0.6112\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7075 - val_loss: 0.6143\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6852 - val_loss: 0.6074\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6900 - val_loss: 0.5942\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6802 - val_loss: 0.6039\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6843 - val_loss: 0.6021\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6820 - val_loss: 0.6096\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6291 - val_loss: 0.5591\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6412 - val_loss: 0.5366\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6622 - val_loss: 0.5756\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6571 - val_loss: 0.5390\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6478 - val_loss: 0.5305\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6698 - val_loss: 0.5396\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6078 - val_loss: 0.5150\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6231 - val_loss: 0.5384\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6335 - val_loss: 0.5140\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6375 - val_loss: 0.5071\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6359 - val_loss: 0.5199\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6144 - val_loss: 0.5120\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6182 - val_loss: 0.5086\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5976 - val_loss: 0.5151\n",
            "Epoch 00036: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.793127019601349_[9, 3, 7, 1, 2, 7]/assets\n",
            "Training:  [7, 3, 9, 1, 5, 4]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 8s 45ms/step - loss: 1.0872 - val_loss: 1.0590\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0582 - val_loss: 1.0413\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0405 - val_loss: 1.0333\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0390 - val_loss: 1.0294\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0302 - val_loss: 1.0274\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0379 - val_loss: 1.0264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 1.0272 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0272 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0230 - val_loss: 1.0252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0277 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0282 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0311 - val_loss: 1.0250\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0365 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0222 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0296 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0222 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0407 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 1.0274 - val_loss: 1.0248\n",
            "Epoch 00018: early stopping\n",
            "7  \t15    \t0.706266\t0.826672\n",
            "Training:  [7, 3, 5, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0720 - val_loss: 0.9018\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.9139 - val_loss: 0.8104\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.8488 - val_loss: 0.7624\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.8079 - val_loss: 0.7278\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7885 - val_loss: 0.7534\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7777 - val_loss: 0.6950\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7690 - val_loss: 0.7122\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7499 - val_loss: 0.6526\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7440 - val_loss: 0.6488\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7295 - val_loss: 0.6462\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7226 - val_loss: 0.6389\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7278 - val_loss: 0.6176\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7099 - val_loss: 0.6801\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7101 - val_loss: 0.6155\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7164 - val_loss: 0.5835\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6734 - val_loss: 0.5746\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6585 - val_loss: 0.5776\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6915 - val_loss: 0.5901\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6597 - val_loss: 0.5804\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6566 - val_loss: 0.5539\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6673 - val_loss: 0.5538\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6598 - val_loss: 0.5725\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6433 - val_loss: 0.5373\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6369 - val_loss: 0.5373\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6555 - val_loss: 0.5350\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6509 - val_loss: 0.5229\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6516 - val_loss: 0.5151\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6426 - val_loss: 0.5111\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6162 - val_loss: 0.5245\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6394 - val_loss: 0.5576\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6092 - val_loss: 0.5121\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6255 - val_loss: 0.4975\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5959 - val_loss: 0.4994\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6127 - val_loss: 0.4965\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6161 - val_loss: 0.5117\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5950 - val_loss: 0.5034\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6006 - val_loss: 0.4934\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6195 - val_loss: 0.5248\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5814 - val_loss: 0.5427\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5961 - val_loss: 0.4891\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5813 - val_loss: 0.4949\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5961 - val_loss: 0.4841\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5751 - val_loss: 0.4987\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6214 - val_loss: 0.5439\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5667 - val_loss: 0.4852\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.5747 - val_loss: 0.5107\n",
            "Epoch 00046: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.8055070878025405_[7, 3, 5, 1, 5, 7]/assets\n",
            "Training:  [7, 3, 0, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 87ms/step - loss: 1.0876 - val_loss: 1.0221\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0355 - val_loss: 0.8962\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.9559 - val_loss: 0.8275\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.8964 - val_loss: 0.8424\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.9084 - val_loss: 0.7722\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.8474 - val_loss: 0.7436\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.8156 - val_loss: 0.7288\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.8312 - val_loss: 0.7161\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.8138 - val_loss: 0.7139\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.8133 - val_loss: 0.7419\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7813 - val_loss: 0.6993\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.8033 - val_loss: 0.7265\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7883 - val_loss: 0.6877\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7838 - val_loss: 0.6854\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7741 - val_loss: 0.6744\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7750 - val_loss: 0.6827\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7696 - val_loss: 0.6683\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7568 - val_loss: 0.6927\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7429 - val_loss: 0.7063\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7564 - val_loss: 0.6648\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7235 - val_loss: 0.7555\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7270 - val_loss: 0.6600\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7339 - val_loss: 0.6536\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7278 - val_loss: 0.6353\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7101 - val_loss: 0.6528\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.7238 - val_loss: 0.6306\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7331 - val_loss: 0.6629\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7144 - val_loss: 0.6368\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6812 - val_loss: 0.6166\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7061 - val_loss: 0.6113\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7016 - val_loss: 0.6270\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6887 - val_loss: 0.6221\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6995 - val_loss: 0.6075\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6923 - val_loss: 0.6069\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6987 - val_loss: 0.6135\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.7007 - val_loss: 0.6438\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6948 - val_loss: 0.6102\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6920 - val_loss: 0.5951\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6707 - val_loss: 0.5915\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6819 - val_loss: 0.6008\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6970 - val_loss: 0.6073\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6976 - val_loss: 0.5954\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.6797 - val_loss: 0.5972\n",
            "Epoch 00043: early stopping\n",
            "Training:  [8, 3, 2, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 89ms/step - loss: 1.0823 - val_loss: 0.9442\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.9553 - val_loss: 0.8969\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.9227 - val_loss: 0.8668\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8894 - val_loss: 0.8371\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8914 - val_loss: 0.7973\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8838 - val_loss: 0.8104\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8436 - val_loss: 0.8273\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8600 - val_loss: 0.7876\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8418 - val_loss: 0.7732\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8476 - val_loss: 0.7748\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8644 - val_loss: 0.7785\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8375 - val_loss: 0.7742\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 10s 87ms/step - loss: 0.8287 - val_loss: 0.7982\n",
            "Epoch 00013: early stopping\n",
            "Training:  [9, 3, 7, 1, 6, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0422 - val_loss: 0.9453\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9550 - val_loss: 0.8500\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9037 - val_loss: 0.8045\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8819 - val_loss: 0.7777\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8451 - val_loss: 0.7507\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8352 - val_loss: 0.7393\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8454 - val_loss: 0.7362\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8332 - val_loss: 0.7293\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8151 - val_loss: 0.7145\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8047 - val_loss: 0.7083\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7960 - val_loss: 0.7246\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7829 - val_loss: 0.6791\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7673 - val_loss: 0.6675\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7780 - val_loss: 0.6677\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7658 - val_loss: 0.6593\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7827 - val_loss: 0.6564\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7508 - val_loss: 0.6321\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7506 - val_loss: 0.6229\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7222 - val_loss: 0.6525\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7022 - val_loss: 0.7370\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7370 - val_loss: 0.6787\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7270 - val_loss: 0.6844\n",
            "Epoch 00022: early stopping\n",
            "Training:  [5, 1, 7, 2, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0702 - val_loss: 0.9452\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9617 - val_loss: 0.8923\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9232 - val_loss: 0.8545\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8892 - val_loss: 0.8184\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8700 - val_loss: 0.7886\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8599 - val_loss: 0.7740\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8314 - val_loss: 0.7671\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8212 - val_loss: 0.7230\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7936 - val_loss: 0.7175\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7704 - val_loss: 0.7031\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7740 - val_loss: 0.6943\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7592 - val_loss: 0.6952\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7502 - val_loss: 0.6874\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7673 - val_loss: 0.6828\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7562 - val_loss: 0.6782\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7509 - val_loss: 0.6759\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7378 - val_loss: 0.6721\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7478 - val_loss: 0.6766\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7509 - val_loss: 0.6688\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7372 - val_loss: 0.6736\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7220 - val_loss: 0.6688\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7313 - val_loss: 0.6701\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7388 - val_loss: 0.6697\n",
            "Epoch 00023: early stopping\n",
            "8  \t14    \t0.760545\t0.826672\n",
            "Training:  [7, 3, 1, 7, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 87ms/step - loss: 1.1104 - val_loss: 1.0590\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0588 - val_loss: 1.0411\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0434 - val_loss: 1.0331\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0340 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0372 - val_loss: 1.0273\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0289 - val_loss: 1.0263\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0250 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0268 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0273 - val_loss: 1.0253\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0355 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0414 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0395 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0255 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0294 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0381 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0360 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0191 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0295 - val_loss: 1.0248\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0325 - val_loss: 1.0248\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0333 - val_loss: 1.0247\n",
            "Epoch 00020: early stopping\n",
            "Training:  [7, 1, 7, 1, 4, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0846 - val_loss: 1.0238\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0366 - val_loss: 0.9769\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9913 - val_loss: 0.9313\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9476 - val_loss: 0.8684\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8865 - val_loss: 0.8303\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8739 - val_loss: 0.8082\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8360 - val_loss: 0.8127\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8198 - val_loss: 0.7814\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8069 - val_loss: 0.7648\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7963 - val_loss: 0.7454\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7836 - val_loss: 0.7310\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7690 - val_loss: 0.7149\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7408 - val_loss: 0.7075\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7409 - val_loss: 0.6828\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7313 - val_loss: 0.6711\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7107 - val_loss: 0.6615\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7020 - val_loss: 0.6616\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7006 - val_loss: 0.6408\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6878 - val_loss: 0.6327\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6857 - val_loss: 0.6243\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6809 - val_loss: 0.6189\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6635 - val_loss: 0.6147\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6734 - val_loss: 0.6147\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6645 - val_loss: 0.6053\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6589 - val_loss: 0.5974\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6582 - val_loss: 0.5912\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6492 - val_loss: 0.5961\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6455 - val_loss: 0.5975\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6483 - val_loss: 0.5944\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6435 - val_loss: 0.5798\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6434 - val_loss: 0.5766\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6292 - val_loss: 0.5673\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6179 - val_loss: 0.5643\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6272 - val_loss: 0.5627\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6354 - val_loss: 0.5614\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6173 - val_loss: 0.5616\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6335 - val_loss: 0.5622\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6097 - val_loss: 0.5575\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6139 - val_loss: 0.5544\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6046 - val_loss: 0.5507\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6073 - val_loss: 0.5387\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5971 - val_loss: 0.5403\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6197 - val_loss: 0.5429\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6101 - val_loss: 0.5409\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6062 - val_loss: 0.5392\n",
            "Epoch 00045: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.8060397175816625_[7, 1, 7, 1, 4, 7]/assets\n",
            "Training:  [1, 1, 7, 2, 2, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0501 - val_loss: 0.9163\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9241 - val_loss: 0.8190\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8634 - val_loss: 0.7783\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8219 - val_loss: 0.7533\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7997 - val_loss: 0.7465\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7958 - val_loss: 0.7220\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7790 - val_loss: 0.7153\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7789 - val_loss: 0.7035\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7636 - val_loss: 0.6994\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7713 - val_loss: 0.6946\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7467 - val_loss: 0.6922\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7436 - val_loss: 0.7784\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7618 - val_loss: 0.6859\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7515 - val_loss: 0.6821\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7435 - val_loss: 0.6835\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7378 - val_loss: 0.6754\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7362 - val_loss: 0.6706\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7324 - val_loss: 0.6689\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7390 - val_loss: 0.6683\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7335 - val_loss: 0.6642\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7182 - val_loss: 0.6659\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7388 - val_loss: 0.6639\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7469 - val_loss: 0.6658\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7340 - val_loss: 0.6617\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7145 - val_loss: 0.6779\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7391 - val_loss: 0.6578\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7255 - val_loss: 0.6606\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7144 - val_loss: 0.6584\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7254 - val_loss: 0.6587\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7352 - val_loss: 0.6592\n",
            "Epoch 00030: early stopping\n",
            "Training:  [9, 7, 7, 1, 2, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0389 - val_loss: 1.0226\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0241 - val_loss: 0.8952\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9038 - val_loss: 0.8477\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8457 - val_loss: 0.8058\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8173 - val_loss: 0.7650\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7714 - val_loss: 0.7994\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7752 - val_loss: 0.7352\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7515 - val_loss: 0.7222\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7463 - val_loss: 0.6992\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7339 - val_loss: 0.7006\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7298 - val_loss: 0.6956\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7062 - val_loss: 0.6886\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7167 - val_loss: 0.6914\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7050 - val_loss: 0.6768\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6976 - val_loss: 0.6891\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7056 - val_loss: 0.6722\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6977 - val_loss: 0.6776\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7001 - val_loss: 0.6649\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6773 - val_loss: 0.6668\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7000 - val_loss: 0.6722\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6893 - val_loss: 0.6607\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6819 - val_loss: 0.6598\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6893 - val_loss: 0.6575\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6854 - val_loss: 0.6566\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6910 - val_loss: 0.6621\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6854 - val_loss: 0.6543\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6909 - val_loss: 0.6547\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6808 - val_loss: 0.6484\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6820 - val_loss: 0.6496\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6840 - val_loss: 0.6500\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6729 - val_loss: 0.6490\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6894 - val_loss: 0.6450\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6833 - val_loss: 0.6421\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6749 - val_loss: 0.6424\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6682 - val_loss: 0.6485\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6814 - val_loss: 0.6461\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6708 - val_loss: 0.6522\n",
            "Epoch 00037: early stopping\n",
            "Training:  [7, 1, 7, 6, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.1476 - val_loss: 1.0752\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0873 - val_loss: 1.0552\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0806 - val_loss: 1.0431\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0548 - val_loss: 1.0369\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0548 - val_loss: 1.0334\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0474 - val_loss: 1.0300\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0388 - val_loss: 1.0283\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0459 - val_loss: 1.0276\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0355 - val_loss: 1.0267\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0416 - val_loss: 1.0263\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0260 - val_loss: 1.0259\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0348 - val_loss: 1.0256\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0420 - val_loss: 1.0254\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0332 - val_loss: 1.0253\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0246 - val_loss: 1.0251\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0257 - val_loss: 1.0249\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0268 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0352 - val_loss: 1.0248\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0341 - val_loss: 1.0248\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0373 - val_loss: 1.0248\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0270 - val_loss: 1.0248\n",
            "Epoch 00021: early stopping\n",
            "Training:  [7, 1, 7, 1, 9, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 115ms/step - loss: 1.0146 - val_loss: 0.8260\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.8412 - val_loss: 0.7783\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.8153 - val_loss: 0.7535\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7944 - val_loss: 0.7407\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7697 - val_loss: 0.7233\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7638 - val_loss: 0.7189\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7508 - val_loss: 0.7028\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7325 - val_loss: 0.6981\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7272 - val_loss: 0.6904\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7306 - val_loss: 0.6852\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7249 - val_loss: 0.6860\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7252 - val_loss: 0.6807\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7179 - val_loss: 0.6760\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7332 - val_loss: 0.6719\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6983 - val_loss: 0.6695\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7174 - val_loss: 0.6646\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6933 - val_loss: 0.6599\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.7034 - val_loss: 0.6597\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6903 - val_loss: 0.6600\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6910 - val_loss: 0.6638\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6973 - val_loss: 0.6568\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6957 - val_loss: 0.6616\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6910 - val_loss: 0.6520\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 0.6844 - val_loss: 0.6622\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6949 - val_loss: 0.6495\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6923 - val_loss: 0.6503\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6779 - val_loss: 0.6425\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6895 - val_loss: 0.6450\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6799 - val_loss: 0.6446\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 0.6811 - val_loss: 0.6519\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 113ms/step - loss: 0.7061 - val_loss: 0.6460\n",
            "Epoch 00031: early stopping\n",
            "Training:  [7, 3, 9, 8, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 120ms/step - loss: 1.0889 - val_loss: 1.0580\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0695 - val_loss: 1.0412\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0429 - val_loss: 1.0334\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0386 - val_loss: 1.0306\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0387 - val_loss: 1.0276\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0206 - val_loss: 1.0267\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0370 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0457 - val_loss: 1.0254\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0223 - val_loss: 1.0251\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0300 - val_loss: 1.0253\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0312 - val_loss: 1.0252\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0371 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0237 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0305 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0336 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0375 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0308 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0364 - val_loss: 1.0247\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0347 - val_loss: 1.0247\n",
            "Epoch 00019: early stopping\n",
            "9  \t18    \t0.708872\t0.826672\n",
            "Training:  [0, 1, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0439 - val_loss: 1.0261\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0466 - val_loss: 1.0251\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0422 - val_loss: 1.0250\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0385 - val_loss: 1.0253\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0337 - val_loss: 1.0247\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0341 - val_loss: 1.0246\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0310 - val_loss: 1.0246\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0303 - val_loss: 1.0247\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0520 - val_loss: 1.0246\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0303 - val_loss: 1.0246\n",
            "Epoch 00010: early stopping\n",
            "Training:  [7, 3, 7, 3, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0865 - val_loss: 1.0593\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0576 - val_loss: 1.0413\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0488 - val_loss: 1.0331\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0360 - val_loss: 1.0293\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0328 - val_loss: 1.0275\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0338 - val_loss: 1.0265\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0296 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0292 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0230 - val_loss: 1.0253\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0289 - val_loss: 1.0251\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0313 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0297 - val_loss: 1.0250\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0347 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0295 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0206 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0329 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0235 - val_loss: 1.0248\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0364 - val_loss: 1.0247\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0306 - val_loss: 1.0247\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0350 - val_loss: 1.0247\n",
            "Epoch 00020: early stopping\n",
            "Training:  [7, 1, 1, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 86ms/step - loss: 1.1381 - val_loss: 1.0632\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0959 - val_loss: 1.0402\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0690 - val_loss: 1.0253\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0489 - val_loss: 1.0121\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0511 - val_loss: 1.0001\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0335 - val_loss: 1.0339\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0286 - val_loss: 0.9881\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0263 - val_loss: 1.0476\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0188 - val_loss: 0.9835\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0084 - val_loss: 0.9401\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0011 - val_loss: 0.9295\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0005 - val_loss: 0.9274\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 0.9741 - val_loss: 0.9590\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.9864 - val_loss: 0.9547\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.9862 - val_loss: 0.9275\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 0.9732 - val_loss: 0.9356\n",
            "Epoch 00016: early stopping\n",
            "Training:  [7, 1, 7, 7, 1, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0840 - val_loss: 1.0340\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0582 - val_loss: 1.0276\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0519 - val_loss: 1.0202\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0448 - val_loss: 1.0190\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0165 - val_loss: 0.9538\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0036 - val_loss: 0.9294\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9829 - val_loss: 0.9157\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9784 - val_loss: 0.8889\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9681 - val_loss: 0.8741\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9618 - val_loss: 0.8598\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9608 - val_loss: 0.8757\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9806 - val_loss: 0.8449\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9426 - val_loss: 0.8304\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9437 - val_loss: 0.8464\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9459 - val_loss: 0.8101\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9350 - val_loss: 0.8060\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9255 - val_loss: 0.7962\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9267 - val_loss: 0.7887\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9358 - val_loss: 0.7860\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9290 - val_loss: 0.7832\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9245 - val_loss: 0.7798\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9240 - val_loss: 0.7858\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9378 - val_loss: 0.7740\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9378 - val_loss: 0.7720\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9342 - val_loss: 0.7755\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9368 - val_loss: 0.7700\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9250 - val_loss: 0.7760\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9362 - val_loss: 0.7662\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9183 - val_loss: 0.7694\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9521 - val_loss: 0.7661\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9131 - val_loss: 0.7644\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9455 - val_loss: 0.7596\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9339 - val_loss: 0.7639\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9358 - val_loss: 0.7581\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9291 - val_loss: 0.7564\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9356 - val_loss: 0.7572\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9136 - val_loss: 0.7557\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9229 - val_loss: 0.7547\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9415 - val_loss: 0.7540\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9281 - val_loss: 0.7614\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9250 - val_loss: 0.7536\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9368 - val_loss: 0.7533\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9085 - val_loss: 0.7536\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9325 - val_loss: 0.7529\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9276 - val_loss: 0.7536\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9187 - val_loss: 0.7534\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9217 - val_loss: 0.7538\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9312 - val_loss: 0.7553\n",
            "Epoch 00048: early stopping\n",
            "Training:  [7, 1, 7, 1, 2, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0393 - val_loss: 0.8997\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9062 - val_loss: 0.8235\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8376 - val_loss: 0.7804\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8079 - val_loss: 0.7549\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7635 - val_loss: 0.7341\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7638 - val_loss: 0.7264\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7523 - val_loss: 0.7108\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7311 - val_loss: 0.7054\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7305 - val_loss: 0.7022\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7273 - val_loss: 0.6829\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7087 - val_loss: 0.6778\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7044 - val_loss: 0.6765\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6997 - val_loss: 0.6757\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7189 - val_loss: 0.6903\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7113 - val_loss: 0.6728\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6998 - val_loss: 0.6640\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7002 - val_loss: 0.6625\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6929 - val_loss: 0.6604\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6917 - val_loss: 0.6606\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6967 - val_loss: 0.6625\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6869 - val_loss: 0.6625\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7037 - val_loss: 0.6636\n",
            "Epoch 00022: early stopping\n",
            "10 \t16    \t0.678608\t0.826672\n",
            "Training:  [7, 1, 1, 2, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 11s 86ms/step - loss: 1.0680 - val_loss: 1.0372\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0573 - val_loss: 1.0302\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0519 - val_loss: 1.0268\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0505 - val_loss: 1.0258\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0435 - val_loss: 1.0254\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 10s 85ms/step - loss: 1.0459 - val_loss: 1.0251\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0523 - val_loss: 1.0248\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0396 - val_loss: 1.0248\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0446 - val_loss: 1.0247\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0489 - val_loss: 1.0247\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0419 - val_loss: 1.0244\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0355 - val_loss: 1.0243\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0419 - val_loss: 1.0247\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0410 - val_loss: 1.0246\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 10s 84ms/step - loss: 1.0427 - val_loss: 1.0245\n",
            "Epoch 00015: early stopping\n",
            "Training:  [7, 1, 8, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 120ms/step - loss: 1.0545 - val_loss: 0.9022\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.9150 - val_loss: 0.8384\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.8615 - val_loss: 0.8530\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.8141 - val_loss: 0.7595\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7878 - val_loss: 0.7347\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7606 - val_loss: 0.7187\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7490 - val_loss: 0.7059\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7439 - val_loss: 0.6994\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7324 - val_loss: 0.6904\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7286 - val_loss: 0.6853\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7111 - val_loss: 0.6805\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6973 - val_loss: 0.6799\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7036 - val_loss: 0.6729\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.7052 - val_loss: 0.6664\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6964 - val_loss: 0.6663\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6995 - val_loss: 0.6666\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6926 - val_loss: 0.6645\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6823 - val_loss: 0.6608\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6897 - val_loss: 0.6574\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6905 - val_loss: 0.6546\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6868 - val_loss: 0.6560\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6740 - val_loss: 0.6545\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6859 - val_loss: 0.6477\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6810 - val_loss: 0.6522\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6943 - val_loss: 0.6490\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6795 - val_loss: 0.6431\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6726 - val_loss: 0.6501\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6860 - val_loss: 0.6463\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6674 - val_loss: 0.6380\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6976 - val_loss: 0.6373\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6807 - val_loss: 0.6375\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6696 - val_loss: 0.6360\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6768 - val_loss: 0.6354\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6783 - val_loss: 0.6417\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6628 - val_loss: 0.6379\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6656 - val_loss: 0.6409\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 0.6793 - val_loss: 0.6393\n",
            "Epoch 00037: early stopping\n",
            "Training:  [7, 5, 9, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 120ms/step - loss: 1.2415 - val_loss: 1.1171\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.1030 - val_loss: 1.0567\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0524 - val_loss: 1.0364\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0361 - val_loss: 1.0294\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0348 - val_loss: 1.0270\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0342 - val_loss: 1.0260\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0417 - val_loss: 1.0254\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0424 - val_loss: 1.0252\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0353 - val_loss: 1.0251\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0298 - val_loss: 1.0250\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0436 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0401 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0306 - val_loss: 1.0248\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0381 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0386 - val_loss: 1.0248\n",
            "Epoch 00015: early stopping\n",
            "Training:  [2, 9, 7, 1, 5, 3]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0912 - val_loss: 1.0697\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0707 - val_loss: 1.0513\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0500 - val_loss: 1.0398\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0421 - val_loss: 1.0337\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0408 - val_loss: 1.0300\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0426 - val_loss: 1.0278\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0314 - val_loss: 1.0265\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0270 - val_loss: 1.0258\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0294 - val_loss: 1.0255\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0395 - val_loss: 1.0252\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0256 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0315 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0350 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0392 - val_loss: 1.0248\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0445 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0322 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0252 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0244 - val_loss: 1.0248\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0382 - val_loss: 1.0248\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0365 - val_loss: 1.0248\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0299 - val_loss: 1.0248\n",
            "Epoch 00021: early stopping\n",
            "Training:  [4, 1, 8, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 121ms/step - loss: 1.1047 - val_loss: 1.0499\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0441 - val_loss: 1.0354\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0520 - val_loss: 1.0299\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0451 - val_loss: 1.0272\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0418 - val_loss: 1.0260\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 118ms/step - loss: 1.0343 - val_loss: 1.0254\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0398 - val_loss: 1.0250\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0323 - val_loss: 1.0249\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0370 - val_loss: 1.0247\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0360 - val_loss: 1.0247\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0347 - val_loss: 1.0248\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0424 - val_loss: 1.0249\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 119ms/step - loss: 1.0334 - val_loss: 1.0249\n",
            "Epoch 00013: early stopping\n",
            "Training:  [7, 1, 3, 3, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 12s 92ms/step - loss: 1.1129 - val_loss: 1.0433\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0803 - val_loss: 1.0333\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0824 - val_loss: 1.0299\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0474 - val_loss: 1.0275\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0530 - val_loss: 1.0244\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0448 - val_loss: 1.0232\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0544 - val_loss: 1.0191\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0436 - val_loss: 1.0140\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0357 - val_loss: 0.9865\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 1.0171 - val_loss: 0.9681\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9913 - val_loss: 0.9732\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9913 - val_loss: 0.9236\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9613 - val_loss: 0.8999\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9388 - val_loss: 0.8651\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9199 - val_loss: 0.8320\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9056 - val_loss: 0.8066\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8803 - val_loss: 0.7820\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8576 - val_loss: 0.7679\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8500 - val_loss: 0.7517\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8185 - val_loss: 0.7393\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8141 - val_loss: 0.7319\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8032 - val_loss: 0.7209\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8119 - val_loss: 0.7132\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8037 - val_loss: 0.7073\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7960 - val_loss: 0.7011\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7889 - val_loss: 0.7086\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7897 - val_loss: 0.7040\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8022 - val_loss: 0.6978\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7814 - val_loss: 0.7093\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7948 - val_loss: 0.6908\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7860 - val_loss: 0.6864\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7728 - val_loss: 0.6873\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7865 - val_loss: 0.6836\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7841 - val_loss: 0.6827\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7819 - val_loss: 0.6773\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7793 - val_loss: 0.6816\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7672 - val_loss: 0.6754\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7622 - val_loss: 0.6809\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7795 - val_loss: 0.6727\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7610 - val_loss: 0.6715\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7791 - val_loss: 0.6746\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7612 - val_loss: 0.6861\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7725 - val_loss: 0.6750\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7620 - val_loss: 0.6758\n",
            "Epoch 00044: early stopping\n",
            "11 \t16    \t0.699988\t0.826672\n",
            "Training:  [3, 1, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.0725 - val_loss: 1.0455\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0514 - val_loss: 1.0264\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0374 - val_loss: 1.0083\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0175 - val_loss: 0.9769\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9952 - val_loss: 0.9217\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9408 - val_loss: 0.8627\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9147 - val_loss: 0.8427\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8558 - val_loss: 0.8205\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8469 - val_loss: 0.8123\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8280 - val_loss: 0.7740\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8094 - val_loss: 0.7647\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7951 - val_loss: 0.7397\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7700 - val_loss: 0.7338\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7577 - val_loss: 0.7266\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7580 - val_loss: 0.7086\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7443 - val_loss: 0.7076\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7129 - val_loss: 0.7021\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7150 - val_loss: 0.7110\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7199 - val_loss: 0.6875\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7024 - val_loss: 0.6809\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7178 - val_loss: 0.6839\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7111 - val_loss: 0.6767\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7057 - val_loss: 0.6723\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7004 - val_loss: 0.6648\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7108 - val_loss: 0.6702\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6924 - val_loss: 0.6623\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6838 - val_loss: 0.6600\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6815 - val_loss: 0.6643\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6828 - val_loss: 0.6542\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6636 - val_loss: 0.6618\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6813 - val_loss: 0.6491\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6712 - val_loss: 0.6472\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6644 - val_loss: 0.6485\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6669 - val_loss: 0.6471\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6752 - val_loss: 0.6427\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6692 - val_loss: 0.6424\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6649 - val_loss: 0.6397\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6612 - val_loss: 0.6454\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6646 - val_loss: 0.6423\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6646 - val_loss: 0.6392\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6604 - val_loss: 0.6455\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6558 - val_loss: 0.6428\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6562 - val_loss: 0.6372\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6572 - val_loss: 0.6416\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6701 - val_loss: 0.6412\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6583 - val_loss: 0.6420\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6595 - val_loss: 0.6352\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6659 - val_loss: 0.6439\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6776 - val_loss: 0.6400\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6631 - val_loss: 0.6433\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6587 - val_loss: 0.6410\n",
            "Epoch 00051: early stopping\n",
            "Training:  [5, 1, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.1938 - val_loss: 0.9891\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0179 - val_loss: 0.9030\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9480 - val_loss: 0.8456\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8883 - val_loss: 0.7974\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8497 - val_loss: 0.8082\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8090 - val_loss: 0.7619\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7801 - val_loss: 0.7496\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7770 - val_loss: 0.7180\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7542 - val_loss: 0.6991\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7382 - val_loss: 0.6862\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7322 - val_loss: 0.6798\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7162 - val_loss: 0.6613\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7028 - val_loss: 0.6518\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6984 - val_loss: 0.6452\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6917 - val_loss: 0.6401\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6832 - val_loss: 0.6272\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6844 - val_loss: 0.6206\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6741 - val_loss: 0.6177\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6791 - val_loss: 0.6140\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6734 - val_loss: 0.6084\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6518 - val_loss: 0.6075\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6672 - val_loss: 0.5923\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6571 - val_loss: 0.6023\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6455 - val_loss: 0.5935\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6452 - val_loss: 0.5835\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6493 - val_loss: 0.5829\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6470 - val_loss: 0.5722\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6317 - val_loss: 0.5667\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6408 - val_loss: 0.5695\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6376 - val_loss: 0.5630\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6354 - val_loss: 0.5604\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6333 - val_loss: 0.5602\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6150 - val_loss: 0.5629\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6228 - val_loss: 0.5503\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6304 - val_loss: 0.5493\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6218 - val_loss: 0.5525\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6144 - val_loss: 0.5480\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6066 - val_loss: 0.5592\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6195 - val_loss: 0.5427\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6184 - val_loss: 0.5433\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6295 - val_loss: 0.5391\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5950 - val_loss: 0.5387\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.6112 - val_loss: 0.5366\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5893 - val_loss: 0.5318\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5986 - val_loss: 0.5347\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6055 - val_loss: 0.5271\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5894 - val_loss: 0.5352\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5918 - val_loss: 0.5451\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5934 - val_loss: 0.5265\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5900 - val_loss: 0.5219\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5821 - val_loss: 0.5221\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5876 - val_loss: 0.5243\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5835 - val_loss: 0.5188\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5968 - val_loss: 0.5188\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5884 - val_loss: 0.5201\n",
            "Epoch 56/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5838 - val_loss: 0.5122\n",
            "Epoch 57/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5793 - val_loss: 0.5165\n",
            "Epoch 58/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5804 - val_loss: 0.5130\n",
            "Epoch 59/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5681 - val_loss: 0.5293\n",
            "Epoch 60/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5802 - val_loss: 0.5078\n",
            "Epoch 61/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5732 - val_loss: 0.5090\n",
            "Epoch 62/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5845 - val_loss: 0.5054\n",
            "Epoch 63/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.5672 - val_loss: 0.5230\n",
            "Epoch 64/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5520 - val_loss: 0.5063\n",
            "Epoch 65/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5637 - val_loss: 0.5084\n",
            "Epoch 66/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.5718 - val_loss: 0.5172\n",
            "Epoch 00066: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.8002100168080302_[5, 1, 7, 1, 5, 7]/assets\n",
            "Training:  [7, 1, 6, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 13s 103ms/step - loss: 1.0204 - val_loss: 0.8331\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8734 - val_loss: 0.7881\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8109 - val_loss: 0.7671\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7895 - val_loss: 0.7438\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7801 - val_loss: 0.7318\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7568 - val_loss: 0.7255\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7574 - val_loss: 0.7106\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 0.7353 - val_loss: 0.7010\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7375 - val_loss: 0.6992\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7252 - val_loss: 0.6934\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7146 - val_loss: 0.6889\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7099 - val_loss: 0.6879\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7046 - val_loss: 0.6940\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7070 - val_loss: 0.6753\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7098 - val_loss: 0.6740\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7073 - val_loss: 0.6717\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6971 - val_loss: 0.6645\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6980 - val_loss: 0.6654\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.7065 - val_loss: 0.6666\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6993 - val_loss: 0.6715\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6844 - val_loss: 0.6640\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6971 - val_loss: 0.6624\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6858 - val_loss: 0.6611\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6891 - val_loss: 0.6546\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6776 - val_loss: 0.6539\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 12s 100ms/step - loss: 0.6828 - val_loss: 0.6484\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6684 - val_loss: 0.6588\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6752 - val_loss: 0.6522\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6753 - val_loss: 0.6473\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6826 - val_loss: 0.6534\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6725 - val_loss: 0.6494\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6626 - val_loss: 0.6472\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6850 - val_loss: 0.6434\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6740 - val_loss: 0.6429\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6644 - val_loss: 0.6502\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6821 - val_loss: 0.6426\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6624 - val_loss: 0.6415\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6772 - val_loss: 0.6415\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6817 - val_loss: 0.6440\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6719 - val_loss: 0.6438\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.6718 - val_loss: 0.6453\n",
            "Epoch 00041: early stopping\n",
            "Training:  [7, 1, 7, 8, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.1370 - val_loss: 1.0659\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0613 - val_loss: 1.0424\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0583 - val_loss: 1.0331\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0468 - val_loss: 1.0297\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0389 - val_loss: 1.0278\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0416 - val_loss: 1.0267\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0434 - val_loss: 1.0258\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0364 - val_loss: 1.0255\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0401 - val_loss: 1.0253\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0351 - val_loss: 1.0252\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0227 - val_loss: 1.0250\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0334 - val_loss: 1.0250\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0338 - val_loss: 1.0249\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0283 - val_loss: 1.0249\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0227 - val_loss: 1.0248\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0255 - val_loss: 1.0248\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0417 - val_loss: 1.0247\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0357 - val_loss: 1.0248\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0306 - val_loss: 1.0247\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0347 - val_loss: 1.0248\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0313 - val_loss: 1.0248\n",
            "Epoch 00021: early stopping\n",
            "Training:  [7, 1, 4, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 12s 96ms/step - loss: 1.0243 - val_loss: 0.8349\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.8465 - val_loss: 0.8482\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.8186 - val_loss: 0.7538\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7728 - val_loss: 0.7439\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7653 - val_loss: 0.7231\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7494 - val_loss: 0.7112\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7336 - val_loss: 0.6999\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7286 - val_loss: 0.7193\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7368 - val_loss: 0.6911\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7173 - val_loss: 0.6840\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7088 - val_loss: 0.6797\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7040 - val_loss: 0.6760\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6967 - val_loss: 0.6772\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7035 - val_loss: 0.6781\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 11s 93ms/step - loss: 0.7010 - val_loss: 0.6746\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6986 - val_loss: 0.6727\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6938 - val_loss: 0.6696\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.7012 - val_loss: 0.6653\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6865 - val_loss: 0.6612\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 11s 93ms/step - loss: 0.6848 - val_loss: 0.6577\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6815 - val_loss: 0.6620\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6896 - val_loss: 0.6617\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6708 - val_loss: 0.6640\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6849 - val_loss: 0.6557\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6919 - val_loss: 0.6543\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 11s 93ms/step - loss: 0.6699 - val_loss: 0.6522\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6701 - val_loss: 0.6549\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 11s 93ms/step - loss: 0.6756 - val_loss: 0.6495\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6796 - val_loss: 0.6495\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6741 - val_loss: 0.6520\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 11s 93ms/step - loss: 0.6775 - val_loss: 0.6491\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6828 - val_loss: 0.6450\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6705 - val_loss: 0.6446\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 11s 93ms/step - loss: 0.6599 - val_loss: 0.6473\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6871 - val_loss: 0.6444\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6787 - val_loss: 0.6628\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 11s 93ms/step - loss: 0.6763 - val_loss: 0.6509\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6795 - val_loss: 0.6497\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 11s 94ms/step - loss: 0.6640 - val_loss: 0.6455\n",
            "Epoch 00039: early stopping\n",
            "Training:  [1, 1, 3, 3, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 12s 92ms/step - loss: 1.0699 - val_loss: 1.0049\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9923 - val_loss: 0.8860\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.9321 - val_loss: 0.8340\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8936 - val_loss: 0.7984\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8668 - val_loss: 0.7671\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8545 - val_loss: 0.7520\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8228 - val_loss: 0.7344\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8227 - val_loss: 0.7225\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8024 - val_loss: 0.7133\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8063 - val_loss: 0.7091\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7932 - val_loss: 0.7055\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.8008 - val_loss: 0.6985\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7827 - val_loss: 0.6971\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7801 - val_loss: 0.6927\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7704 - val_loss: 0.6901\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7920 - val_loss: 0.6913\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7724 - val_loss: 0.6872\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7792 - val_loss: 0.6834\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7847 - val_loss: 0.6883\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7771 - val_loss: 0.6900\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7729 - val_loss: 0.6819\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7646 - val_loss: 0.6791\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7619 - val_loss: 0.6787\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7645 - val_loss: 0.6773\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7659 - val_loss: 0.6806\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7706 - val_loss: 0.6731\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7712 - val_loss: 0.6766\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7625 - val_loss: 0.6966\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7738 - val_loss: 0.6810\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 11s 90ms/step - loss: 0.7643 - val_loss: 0.6852\n",
            "Epoch 00030: early stopping\n",
            "Training:  [7, 1, 6, 1, 9, 5]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 105ms/step - loss: 1.0533 - val_loss: 1.0330\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 1.0381 - val_loss: 0.9907\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.9898 - val_loss: 0.9381\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.9588 - val_loss: 0.8912\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.9029 - val_loss: 0.8451\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 12s 104ms/step - loss: 0.8505 - val_loss: 0.8052\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 12s 104ms/step - loss: 0.8254 - val_loss: 0.7712\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 12s 104ms/step - loss: 0.8056 - val_loss: 0.7530\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 12s 104ms/step - loss: 0.7719 - val_loss: 0.7370\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 12s 104ms/step - loss: 0.7599 - val_loss: 0.7176\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7535 - val_loss: 0.7068\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 12s 104ms/step - loss: 0.7258 - val_loss: 0.7058\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7242 - val_loss: 0.6932\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7034 - val_loss: 0.6912\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7069 - val_loss: 0.6830\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7005 - val_loss: 0.6775\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7094 - val_loss: 0.6708\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7077 - val_loss: 0.6674\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6870 - val_loss: 0.6641\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7009 - val_loss: 0.6685\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.7002 - val_loss: 0.6652\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6893 - val_loss: 0.6543\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6854 - val_loss: 0.6567\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6797 - val_loss: 0.6550\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6694 - val_loss: 0.6501\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6840 - val_loss: 0.6505\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6794 - val_loss: 0.6465\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6733 - val_loss: 0.6482\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6744 - val_loss: 0.6413\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6697 - val_loss: 0.6450\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6690 - val_loss: 0.6444\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6675 - val_loss: 0.6399\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6785 - val_loss: 0.6399\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6654 - val_loss: 0.6396\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6500 - val_loss: 0.6476\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6854 - val_loss: 0.6400\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6673 - val_loss: 0.6402\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6806 - val_loss: 0.6380\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6743 - val_loss: 0.6377\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6571 - val_loss: 0.6354\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6603 - val_loss: 0.6348\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6620 - val_loss: 0.6366\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6526 - val_loss: 0.6335\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 12s 104ms/step - loss: 0.6596 - val_loss: 0.6382\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6663 - val_loss: 0.6360\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6686 - val_loss: 0.6409\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 12s 103ms/step - loss: 0.6705 - val_loss: 0.6367\n",
            "Epoch 00047: early stopping\n",
            "Training:  [7, 1, 7, 4, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.2217 - val_loss: 1.1187\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.1229 - val_loss: 1.0752\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0887 - val_loss: 1.0568\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0613 - val_loss: 1.0303\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0566 - val_loss: 1.0298\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0375 - val_loss: 1.0183\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0452 - val_loss: 0.9603\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 1.0171 - val_loss: 0.9328\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9990 - val_loss: 0.9176\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9783 - val_loss: 0.8994\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9801 - val_loss: 0.8806\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9543 - val_loss: 0.8617\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9378 - val_loss: 0.8425\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9130 - val_loss: 0.8163\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9046 - val_loss: 0.8012\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.9047 - val_loss: 0.7839\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8920 - val_loss: 0.7668\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8613 - val_loss: 0.7587\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8725 - val_loss: 0.7611\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8584 - val_loss: 0.7472\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8595 - val_loss: 0.7324\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8405 - val_loss: 0.7409\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8543 - val_loss: 0.7224\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8495 - val_loss: 0.7250\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8338 - val_loss: 0.7150\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8383 - val_loss: 0.7098\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8296 - val_loss: 0.7064\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8399 - val_loss: 0.7040\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8315 - val_loss: 0.7319\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8535 - val_loss: 0.7028\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8456 - val_loss: 0.7164\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8543 - val_loss: 0.7027\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7947 - val_loss: 0.6981\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8230 - val_loss: 0.6946\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8141 - val_loss: 0.6963\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8170 - val_loss: 0.6926\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8068 - val_loss: 0.6914\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8241 - val_loss: 0.6876\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8297 - val_loss: 0.6902\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.8273 - val_loss: 0.6911\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8362 - val_loss: 0.6876\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8057 - val_loss: 0.6886\n",
            "Epoch 00042: early stopping\n",
            "Training:  [7, 1, 7, 1, 5, 4]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 8s 45ms/step - loss: 1.0279 - val_loss: 0.9643\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.9313 - val_loss: 0.9075\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8825 - val_loss: 0.8469\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8391 - val_loss: 0.8036\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8036 - val_loss: 0.7630\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7775 - val_loss: 0.7471\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7728 - val_loss: 0.7260\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7474 - val_loss: 0.7152\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7361 - val_loss: 0.7012\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7042 - val_loss: 0.6949\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7170 - val_loss: 0.6913\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7057 - val_loss: 0.6882\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7121 - val_loss: 0.6838\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6895 - val_loss: 0.6750\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6939 - val_loss: 0.6752\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6859 - val_loss: 0.6695\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7031 - val_loss: 0.6694\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6925 - val_loss: 0.6682\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6922 - val_loss: 0.6675\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6810 - val_loss: 0.6679\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6927 - val_loss: 0.6588\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6759 - val_loss: 0.6617\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6889 - val_loss: 0.6611\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6704 - val_loss: 0.6557\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6880 - val_loss: 0.6610\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6848 - val_loss: 0.6615\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6651 - val_loss: 0.6622\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6715 - val_loss: 0.6578\n",
            "Epoch 00028: early stopping\n",
            "12 \t16    \t0.686591\t0.826672\n",
            "Training:  [7, 1, 7, 1, 5, 6]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 8s 45ms/step - loss: 1.0348 - val_loss: 1.0045\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.9651 - val_loss: 0.9846\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.9029 - val_loss: 0.9437\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8659 - val_loss: 0.9236\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8327 - val_loss: 0.9088\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.8026 - val_loss: 0.7981\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7839 - val_loss: 0.8180\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7489 - val_loss: 0.8078\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7535 - val_loss: 0.7203\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7365 - val_loss: 0.7481\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7257 - val_loss: 0.7206\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7252 - val_loss: 0.7071\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7239 - val_loss: 0.6951\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7054 - val_loss: 0.6857\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7122 - val_loss: 0.6941\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7054 - val_loss: 0.6790\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6955 - val_loss: 0.6722\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6940 - val_loss: 0.6711\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6813 - val_loss: 0.6699\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6935 - val_loss: 0.6758\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6793 - val_loss: 0.6636\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6912 - val_loss: 0.6684\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6996 - val_loss: 0.6620\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6782 - val_loss: 0.6678\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6765 - val_loss: 0.6604\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6745 - val_loss: 0.6626\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6671 - val_loss: 0.6638\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6722 - val_loss: 0.6554\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6599 - val_loss: 0.6556\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6871 - val_loss: 0.6805\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6574 - val_loss: 0.6524\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6726 - val_loss: 0.6592\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6711 - val_loss: 0.6531\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6753 - val_loss: 0.6580\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6776 - val_loss: 0.6538\n",
            "Epoch 00035: early stopping\n",
            "Training:  [4, 1, 7, 1, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0515 - val_loss: 1.0292\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0412 - val_loss: 1.0259\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0382 - val_loss: 1.0249\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0286 - val_loss: 1.0249\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0409 - val_loss: 1.0248\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0336 - val_loss: 1.0247\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0336 - val_loss: 1.0247\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0297 - val_loss: 1.0247\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0445 - val_loss: 1.0246\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0302 - val_loss: 1.0241\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0071 - val_loss: 0.8903\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8810 - val_loss: 0.8267\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8844 - val_loss: 0.8153\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8274 - val_loss: 0.8288\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7948 - val_loss: 0.7481\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7656 - val_loss: 0.7336\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7435 - val_loss: 0.7212\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7436 - val_loss: 0.8034\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7569 - val_loss: 0.7065\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7102 - val_loss: 0.7000\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7139 - val_loss: 0.6971\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7124 - val_loss: 0.6855\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6997 - val_loss: 0.6837\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7142 - val_loss: 1.0550\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7333 - val_loss: 0.6810\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7057 - val_loss: 0.6773\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6960 - val_loss: 0.6745\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6973 - val_loss: 0.6694\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7038 - val_loss: 0.6791\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7010 - val_loss: 0.6706\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7049 - val_loss: 0.6640\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6893 - val_loss: 0.6808\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7086 - val_loss: 0.6650\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6913 - val_loss: 0.6679\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6962 - val_loss: 0.6645\n",
            "Epoch 00035: early stopping\n",
            "Training:  [8, 1, 7, 6, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.2190 - val_loss: 1.0528\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.1335 - val_loss: 1.0422\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0931 - val_loss: 1.0305\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0831 - val_loss: 1.0283\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0770 - val_loss: 1.0261\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0618 - val_loss: 1.0253\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0569 - val_loss: 1.0249\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0427 - val_loss: 1.0247\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0297 - val_loss: 1.0247\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0246 - val_loss: 1.0248\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0369 - val_loss: 1.0249\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0311 - val_loss: 1.0250\n",
            "Epoch 00012: early stopping\n",
            "Training:  [7, 1, 6, 5, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 13s 102ms/step - loss: 1.1482 - val_loss: 1.0484\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0849 - val_loss: 1.0357\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0775 - val_loss: 1.0308\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0543 - val_loss: 1.0286\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0408 - val_loss: 1.0251\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0418 - val_loss: 1.0229\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0527 - val_loss: 0.9922\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0312 - val_loss: 0.9555\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0101 - val_loss: 0.9252\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 1.0165 - val_loss: 0.9042\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9927 - val_loss: 0.9000\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9822 - val_loss: 0.8727\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9666 - val_loss: 0.8588\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9614 - val_loss: 0.8519\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9592 - val_loss: 0.8240\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9269 - val_loss: 0.8118\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9310 - val_loss: 0.7950\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9325 - val_loss: 0.7999\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9252 - val_loss: 0.7793\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9127 - val_loss: 0.7676\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 12s 102ms/step - loss: 0.9033 - val_loss: 0.7607\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8982 - val_loss: 0.7600\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9021 - val_loss: 0.7488\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8988 - val_loss: 0.7529\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.9014 - val_loss: 0.7447\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8926 - val_loss: 0.7397\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8862 - val_loss: 0.7445\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8785 - val_loss: 0.7327\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8811 - val_loss: 0.7263\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8820 - val_loss: 0.7231\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8700 - val_loss: 0.7222\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8714 - val_loss: 0.7247\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8748 - val_loss: 0.7248\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8809 - val_loss: 0.7221\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 12s 102ms/step - loss: 0.8657 - val_loss: 0.7174\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8633 - val_loss: 0.7156\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8665 - val_loss: 0.7224\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 12s 102ms/step - loss: 0.8634 - val_loss: 0.7226\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8633 - val_loss: 0.7109\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 12s 102ms/step - loss: 0.8466 - val_loss: 0.7111\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8469 - val_loss: 0.7110\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8610 - val_loss: 0.7095\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8546 - val_loss: 0.7108\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8780 - val_loss: 0.7075\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8417 - val_loss: 0.7062\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8645 - val_loss: 0.7094\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8687 - val_loss: 0.7096\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8308 - val_loss: 0.7049\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8522 - val_loss: 0.7096\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8396 - val_loss: 0.7036\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 12s 102ms/step - loss: 0.8543 - val_loss: 0.7059\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8601 - val_loss: 0.7057\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8473 - val_loss: 0.7032\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8553 - val_loss: 0.7187\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8939 - val_loss: 0.7412\n",
            "Epoch 56/1000\n",
            "120/120 [==============================] - 12s 102ms/step - loss: 0.8771 - val_loss: 0.7492\n",
            "Epoch 57/1000\n",
            "120/120 [==============================] - 12s 101ms/step - loss: 0.8925 - val_loss: 0.7130\n",
            "Epoch 00057: early stopping\n",
            "Training:  [7, 1, 7, 1, 6, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0656 - val_loss: 0.9723\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9737 - val_loss: 0.9135\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9242 - val_loss: 0.8661\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8726 - val_loss: 0.8236\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8346 - val_loss: 0.7824\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8012 - val_loss: 0.7627\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7819 - val_loss: 0.7353\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7548 - val_loss: 0.7125\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7328 - val_loss: 0.7035\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7289 - val_loss: 0.6985\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7143 - val_loss: 0.6881\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7079 - val_loss: 0.6805\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7047 - val_loss: 0.6793\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6958 - val_loss: 0.6740\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7141 - val_loss: 0.6726\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7040 - val_loss: 0.6688\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6998 - val_loss: 0.6679\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6865 - val_loss: 0.6630\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6955 - val_loss: 0.6566\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6982 - val_loss: 0.6525\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6790 - val_loss: 0.6540\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6840 - val_loss: 0.6533\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6834 - val_loss: 0.6616\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6743 - val_loss: 0.6435\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6760 - val_loss: 0.6569\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6704 - val_loss: 0.6533\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6854 - val_loss: 0.6453\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6780 - val_loss: 0.6567\n",
            "Epoch 00028: early stopping\n",
            "Training:  [1, 4, 7, 2, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0513 - val_loss: 0.9456\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9590 - val_loss: 0.8840\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9059 - val_loss: 0.8515\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8731 - val_loss: 0.7966\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8347 - val_loss: 0.7688\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8138 - val_loss: 0.7548\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7969 - val_loss: 0.7335\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7878 - val_loss: 0.7372\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7850 - val_loss: 0.7130\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7775 - val_loss: 0.7070\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7443 - val_loss: 0.7036\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7630 - val_loss: 0.6948\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7550 - val_loss: 0.6920\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7403 - val_loss: 0.6906\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7362 - val_loss: 0.6856\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7262 - val_loss: 0.6853\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7302 - val_loss: 0.6780\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7432 - val_loss: 0.6782\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7457 - val_loss: 0.6799\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7323 - val_loss: 0.6738\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 110ms/step - loss: 0.7422 - val_loss: 0.6725\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7296 - val_loss: 0.6733\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7268 - val_loss: 0.6832\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7443 - val_loss: 0.7090\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7507 - val_loss: 0.6706\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7218 - val_loss: 0.6688\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7405 - val_loss: 0.6700\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7224 - val_loss: 0.6698\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7158 - val_loss: 0.6708\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7442 - val_loss: 0.6659\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7296 - val_loss: 0.6679\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7191 - val_loss: 0.6676\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7381 - val_loss: 0.6638\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7208 - val_loss: 0.6654\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7331 - val_loss: 0.6642\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7258 - val_loss: 0.6621\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7137 - val_loss: 0.6637\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7138 - val_loss: 0.6651\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7210 - val_loss: 0.6647\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7053 - val_loss: 0.6640\n",
            "Epoch 00040: early stopping\n",
            "13 \t16    \t0.730374\t0.826672\n",
            "Training:  [7, 1, 7, 1, 5, 9]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0334 - val_loss: 0.8005\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8315 - val_loss: 0.7485\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7890 - val_loss: 0.7264\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7811 - val_loss: 0.7198\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7357 - val_loss: 0.7116\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7371 - val_loss: 0.7047\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7418 - val_loss: 0.6954\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7215 - val_loss: 0.6906\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7186 - val_loss: 0.6852\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7203 - val_loss: 0.6809\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7056 - val_loss: 0.6814\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7126 - val_loss: 0.6797\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7139 - val_loss: 0.6782\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7006 - val_loss: 0.6646\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7009 - val_loss: 0.6686\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7007 - val_loss: 0.6680\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6940 - val_loss: 0.6567\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6970 - val_loss: 0.6682\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6841 - val_loss: 0.6637\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7057 - val_loss: 0.6589\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6909 - val_loss: 0.6622\n",
            "Epoch 00021: early stopping\n",
            "Training:  [7, 1, 7, 7, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.1059 - val_loss: 1.0454\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0681 - val_loss: 1.0319\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0481 - val_loss: 1.0272\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0374 - val_loss: 1.0256\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0389 - val_loss: 1.0250\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0520 - val_loss: 1.0247\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0299 - val_loss: 1.0247\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0374 - val_loss: 1.0246\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0375 - val_loss: 1.0246\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0273 - val_loss: 1.0245\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0317 - val_loss: 1.0245\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0393 - val_loss: 1.0245\n",
            "Epoch 00012: early stopping\n",
            "Training:  [7, 1, 7, 9, 5, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.2638 - val_loss: 1.0287\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.1791 - val_loss: 1.0269\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.1487 - val_loss: 1.0250\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0965 - val_loss: 1.0250\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0663 - val_loss: 1.0251\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0593 - val_loss: 1.0246\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0568 - val_loss: 1.0244\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0486 - val_loss: 1.0245\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0447 - val_loss: 1.0245\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0367 - val_loss: 1.0244\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0429 - val_loss: 1.0244\n",
            "Epoch 00011: early stopping\n",
            "Training:  [1, 1, 7, 1, 8, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 114ms/step - loss: 1.0639 - val_loss: 1.0453\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0487 - val_loss: 1.0188\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 1.0244 - val_loss: 0.9801\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.9963 - val_loss: 0.9281\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.9736 - val_loss: 0.9075\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.9347 - val_loss: 1.0117\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.9235 - val_loss: 0.8370\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8878 - val_loss: 0.8331\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8820 - val_loss: 0.8288\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8697 - val_loss: 0.8003\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8448 - val_loss: 0.9161\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8537 - val_loss: 0.8284\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8438 - val_loss: 0.7935\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8312 - val_loss: 0.8056\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8306 - val_loss: 0.7675\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8054 - val_loss: 0.7813\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.8011 - val_loss: 0.7643\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7816 - val_loss: 0.7574\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7791 - val_loss: 0.7579\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7814 - val_loss: 0.7447\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7796 - val_loss: 0.7402\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7816 - val_loss: 0.7511\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7543 - val_loss: 0.7462\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7701 - val_loss: 0.7454\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7680 - val_loss: 0.7393\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7567 - val_loss: 0.7421\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7635 - val_loss: 0.7476\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7568 - val_loss: 0.7186\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7524 - val_loss: 0.7181\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7390 - val_loss: 0.7166\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7466 - val_loss: 0.7031\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7197 - val_loss: 0.7117\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7169 - val_loss: 0.7002\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7065 - val_loss: 0.7149\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7113 - val_loss: 0.6888\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7128 - val_loss: 0.6840\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7158 - val_loss: 0.6761\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7102 - val_loss: 0.6736\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6967 - val_loss: 0.6757\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7188 - val_loss: 0.6616\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.7126 - val_loss: 0.6584\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6876 - val_loss: 0.6590\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6928 - val_loss: 0.6607\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6971 - val_loss: 0.6434\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6729 - val_loss: 0.6547\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6786 - val_loss: 0.6477\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6761 - val_loss: 0.6461\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6685 - val_loss: 0.6432\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6818 - val_loss: 0.6356\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6639 - val_loss: 0.6413\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6609 - val_loss: 0.6234\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6600 - val_loss: 0.6257\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6682 - val_loss: 0.6205\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6528 - val_loss: 0.6285\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6347 - val_loss: 0.6425\n",
            "Epoch 56/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6487 - val_loss: 0.6187\n",
            "Epoch 57/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6320 - val_loss: 0.6255\n",
            "Epoch 58/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6492 - val_loss: 0.6183\n",
            "Epoch 59/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6366 - val_loss: 0.6130\n",
            "Epoch 60/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6368 - val_loss: 0.6205\n",
            "Epoch 61/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6382 - val_loss: 0.6204\n",
            "Epoch 62/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6297 - val_loss: 0.6100\n",
            "Epoch 63/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6295 - val_loss: 0.6029\n",
            "Epoch 64/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6142 - val_loss: 0.6204\n",
            "Epoch 65/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6043 - val_loss: 0.6366\n",
            "Epoch 66/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6353 - val_loss: 0.6281\n",
            "Epoch 67/1000\n",
            "120/120 [==============================] - 13s 112ms/step - loss: 0.6331 - val_loss: 0.6243\n",
            "Epoch 00067: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.7842669687976536_[1, 1, 7, 1, 8, 7]/assets\n",
            "Training:  [9, 1, 7, 1, 3, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 113ms/step - loss: 1.0583 - val_loss: 1.0409\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0501 - val_loss: 1.0092\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0275 - val_loss: 0.9744\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9950 - val_loss: 1.0388\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9747 - val_loss: 0.8877\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9164 - val_loss: 0.9943\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.9000 - val_loss: 0.8522\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8547 - val_loss: 0.8061\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8626 - val_loss: 0.8005\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8396 - val_loss: 0.8031\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8298 - val_loss: 0.8209\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8003 - val_loss: 0.7903\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8268 - val_loss: 0.8466\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8098 - val_loss: 0.7689\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.8061 - val_loss: 0.7810\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7829 - val_loss: 0.7735\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7797 - val_loss: 0.7608\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7850 - val_loss: 0.7569\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7877 - val_loss: 0.7393\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7744 - val_loss: 0.7550\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7854 - val_loss: 0.7377\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7515 - val_loss: 0.7337\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7401 - val_loss: 0.7372\n",
            "Epoch 24/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7437 - val_loss: 0.7281\n",
            "Epoch 25/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7498 - val_loss: 0.7266\n",
            "Epoch 26/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7373 - val_loss: 0.7189\n",
            "Epoch 27/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7268 - val_loss: 0.7259\n",
            "Epoch 28/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7236 - val_loss: 0.7137\n",
            "Epoch 29/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7285 - val_loss: 0.7217\n",
            "Epoch 30/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7321 - val_loss: 0.7485\n",
            "Epoch 31/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7288 - val_loss: 0.7169\n",
            "Epoch 32/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7181 - val_loss: 0.6987\n",
            "Epoch 33/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7070 - val_loss: 0.7009\n",
            "Epoch 34/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7108 - val_loss: 0.6912\n",
            "Epoch 35/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7054 - val_loss: 0.7060\n",
            "Epoch 36/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.7069 - val_loss: 0.6827\n",
            "Epoch 37/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6895 - val_loss: 0.6856\n",
            "Epoch 38/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6761 - val_loss: 0.6814\n",
            "Epoch 39/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6902 - val_loss: 0.6743\n",
            "Epoch 40/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6723 - val_loss: 0.6697\n",
            "Epoch 41/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6656 - val_loss: 0.6773\n",
            "Epoch 42/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6730 - val_loss: 0.6738\n",
            "Epoch 43/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6678 - val_loss: 0.6675\n",
            "Epoch 44/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6705 - val_loss: 0.6589\n",
            "Epoch 45/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6503 - val_loss: 0.6617\n",
            "Epoch 46/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6556 - val_loss: 0.6544\n",
            "Epoch 47/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6572 - val_loss: 0.6491\n",
            "Epoch 48/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6329 - val_loss: 0.6468\n",
            "Epoch 49/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6403 - val_loss: 0.6592\n",
            "Epoch 50/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6503 - val_loss: 0.6453\n",
            "Epoch 51/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6416 - val_loss: 0.6244\n",
            "Epoch 52/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6002 - val_loss: 0.6544\n",
            "Epoch 53/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6337 - val_loss: 0.6332\n",
            "Epoch 54/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6137 - val_loss: 0.6396\n",
            "Epoch 55/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 0.6196 - val_loss: 0.6428\n",
            "Epoch 00055: early stopping\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/Run_2_f1_0.7694340468481127_[9, 1, 7, 1, 3, 7]/assets\n",
            "14 \t15    \t0.666441\t0.826672\n",
            "Training:  [7, 1, 7, 1, 5, 0]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 8s 46ms/step - loss: 0.8951 - val_loss: 0.8383\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.8001 - val_loss: 0.8030\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.7599 - val_loss: 0.7762\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.7541 - val_loss: 0.7792\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.7302 - val_loss: 0.7989\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7156 - val_loss: 0.9551\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.7357 - val_loss: 0.6995\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.7129 - val_loss: 0.6981\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6981 - val_loss: 0.6878\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.7087 - val_loss: 0.6893\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.7139 - val_loss: 0.6804\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6949 - val_loss: 0.6785\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6890 - val_loss: 0.6756\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6859 - val_loss: 0.6701\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6971 - val_loss: 0.6664\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6731 - val_loss: 0.6678\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6805 - val_loss: 0.6676\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6770 - val_loss: 0.6632\n",
            "Epoch 19/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6606 - val_loss: 0.6609\n",
            "Epoch 20/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6692 - val_loss: 0.6638\n",
            "Epoch 21/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6807 - val_loss: 0.6827\n",
            "Epoch 22/1000\n",
            "120/120 [==============================] - 5s 40ms/step - loss: 0.6754 - val_loss: 0.6704\n",
            "Epoch 23/1000\n",
            "120/120 [==============================] - 5s 39ms/step - loss: 0.6851 - val_loss: 0.6622\n",
            "Epoch 00023: early stopping\n",
            "Training:  [4, 1, 7, 1, 9, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 15s 117ms/step - loss: 1.0864 - val_loss: 1.0513\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0588 - val_loss: 1.0360\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0365 - val_loss: 1.0297\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0309 - val_loss: 1.0270\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0358 - val_loss: 1.0257\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0472 - val_loss: 1.0251\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0272 - val_loss: 1.0249\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 14s 116ms/step - loss: 1.0340 - val_loss: 1.0249\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0421 - val_loss: 1.0248\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0401 - val_loss: 1.0247\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 14s 116ms/step - loss: 1.0340 - val_loss: 1.0247\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 14s 116ms/step - loss: 1.0356 - val_loss: 1.0248\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 14s 115ms/step - loss: 1.0321 - val_loss: 1.0248\n",
            "Epoch 00013: early stopping\n",
            "Training:  [7, 1, 7, 1, 1, 7]\n",
            "Epoch 1/1000\n",
            "120/120 [==============================] - 14s 112ms/step - loss: 1.3303 - val_loss: 1.0741\n",
            "Epoch 2/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0864 - val_loss: 1.0381\n",
            "Epoch 3/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0592 - val_loss: 1.0328\n",
            "Epoch 4/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0503 - val_loss: 1.0300\n",
            "Epoch 5/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0507 - val_loss: 1.0281\n",
            "Epoch 6/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0481 - val_loss: 1.0270\n",
            "Epoch 7/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0373 - val_loss: 1.0263\n",
            "Epoch 8/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0455 - val_loss: 1.0261\n",
            "Epoch 9/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0340 - val_loss: 1.0256\n",
            "Epoch 10/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0481 - val_loss: 1.0255\n",
            "Epoch 11/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0378 - val_loss: 1.0256\n",
            "Epoch 12/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0410 - val_loss: 1.0253\n",
            "Epoch 13/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0320 - val_loss: 1.0251\n",
            "Epoch 14/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0333 - val_loss: 1.0250\n",
            "Epoch 15/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0373 - val_loss: 1.0250\n",
            "Epoch 16/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0371 - val_loss: 1.0250\n",
            "Epoch 17/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0382 - val_loss: 1.0250\n",
            "Epoch 18/1000\n",
            "120/120 [==============================] - 13s 111ms/step - loss: 1.0398 - val_loss: 1.0250\n",
            "Epoch 00018: early stopping\n",
            "15 \t12    \t0.715189\t0.826672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpN1d3yFcwas"
      },
      "source": [
        "## Results\n",
        "Below you can find the generational improvement plot as well as information on the best model created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rye7Wxlacwas",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "e406b984-3efa-4537-a04b-7f0ead8212b0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "#Show generational plot\n",
        "gen, avg, max_ = log.select(\"gen\", \"avg\", \"max\")\n",
        "plt.plot(gen, avg, label=\"Average\")\n",
        "plt.plot(gen, max_, label=\"Maximum\")\n",
        "plt.xlabel(\"Generation\")\n",
        "plt.ylabel(\"Fitness\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvqlqwKCcwas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf7e9bf-d74c-4ca0-dfba-609e9721b834"
      },
      "source": [
        "#Print best score and individual\n",
        "print(f'Best Individual:\\n{hof[0]}')\n",
        "print(f'Fitness: {seen_models[str(hof[0])]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Individual:\n",
            "[7, 1, 7, 1, 5, 7]\n",
            "Fitness: 0.8266718456410842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "hdnvOOJBW9nC",
        "outputId": "6976a753-fe74-4941-bcd1-73c0b4e20c95"
      },
      "source": [
        "chromosome = [7, 1, 7, 1, 5, 7]\n",
        "\n",
        "def get_activation_type(x):\n",
        "  if x % 3 == 0:\n",
        "    return 'relu'\n",
        "  elif x % 3 == 1:\n",
        "    return 'tanh'\n",
        "  else:\n",
        "    return 'sigmoid'\n",
        "\n",
        "def get_optimizer_type(x):\n",
        "  if x % 2 == 0:\n",
        "    return 'adam'\n",
        "  else:\n",
        "    return 'sgd'\n",
        "\n",
        "def get_base(x):\n",
        "  if x % 2 == 0:\n",
        "    return 'MobileNet'\n",
        "  else:\n",
        "    return 'VGG16'\n",
        "\n",
        "model = tf.keras.models.load_model(f'/content/drive/MyDrive/CSC180_Final_Project/Transfer_Learning_Models/717157/')\n",
        "\n",
        "# Print model info\n",
        "print('Model Info:')\n",
        "print(f'Transfer Learning Base: {get_base(chromosome[5])}')\n",
        "print(f'Activation Type: {get_activation_type(chromosome[1])}')\n",
        "print(f'Optimizer Type: {get_optimizer_type(chromosome[0])}')\n",
        "print(f'Number of Trainable Layers: {len(model.layers) - chromosome[2]}')\n",
        "print(f'Number of Fully-Connected Layers: {chromosome[4] + 2} (not including output layer)')\n",
        "print(f'Dropout Chance: {float(chromosome[3])/float(13)}\\n')\n",
        "\n",
        "# Predict\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Report\n",
        "print('Model Report:')\n",
        "print(metrics.classification_report(y_true, pred, zero_division=1))\n",
        "print('\\n')\n",
        "\n",
        "# Plot\n",
        "print('Plot:')\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm, label_decoder)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Info:\n",
            "Transfer Learning Base: VGG16\n",
            "Activation Type: tanh\n",
            "Optimizer Type: sgd\n",
            "Number of Trainable Layers: 27\n",
            "Number of Fully-Connected Layers: 7 (not including output layer)\n",
            "Dropout Chance: 0.07692307692307693\n",
            "\n",
            "Model Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.75      0.81       642\n",
            "           1       0.74      0.87      0.80       387\n",
            "           2       0.86      0.96      0.91       245\n",
            "\n",
            "    accuracy                           0.83      1274\n",
            "   macro avg       0.83      0.86      0.84      1274\n",
            "weighted avg       0.84      0.83      0.83      1274\n",
            "\n",
            "\n",
            "\n",
            "Plot:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-LbITlEcwat"
      },
      "source": [
        "## INFO: Panda Runner\n",
        "Unfortunately it is impossible to include all the game files in this notebook. As a result I have included a link to the GitHub repository that hosts it. In that repository you will find a detailed <u>ReadMe</u> that outlines the results of the models in the actual game, and how to run/play the game along with various other features. However, if you are just interested in seeing what it can do, I have linked a video below where our best model plays Panda Runner!<br>\n",
        "\n",
        "### GitHub Repository: <a href=\"https://github.com/Warthog710/CSC180-Final-Platformer\">Panda Runner Repository</a><br>\n",
        "### YouTube Video: <a href=\"https://youtu.be/30g_3F2pwN8\">Panda Runner - AI Run (3.38x Speed)</a>"
      ]
    }
  ]
}