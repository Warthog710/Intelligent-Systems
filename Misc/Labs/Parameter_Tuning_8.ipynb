{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSC 180  Intelligent Systems (Spring 2021)\n",
    "\n",
    "#### Dr. Haiquan Chen, Dept of Computer Scicence\n",
    "\n",
    "#### California State University, Sacramento\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8: Hyper-parameter Tuning for Backpropagation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions for Tensorflow (Little Gems)\n",
    "\n",
    "The following functions will be used with TensorFlow to help preprocess the data.  They allow you to build the feature vector for a neural network. \n",
    "\n",
    "* Predictors/Inputs \n",
    "    * Fill any missing inputs with the median for that column.  Use **missing_median**.\n",
    "    * Encode textual/categorical values with **encode_text_dummy**.\n",
    "    * Encode numeric values with **encode_numeric_zscore**.\n",
    "* Output\n",
    "    * Discard rows with missing outputs.\n",
    "    * Encode textual/categorical values with **encode_text_index**.\n",
    "    * Do not encode output numeric values.\n",
    "* Produce final feature vectors (x) and expected output (y) with **to_xy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning rate\n",
    "Backpropagation is the primary means by which a neural network's weights are determined during training. Backpropagation works by calculating a weight change amount ($v_t$) for every weight($\\theta$, theata) in the neural network.  This value is subtracted from every weight by the following equation: \n",
    "\n",
    "$ \\theta_t = \\theta_{t-1} - v_t $\n",
    "\n",
    "\n",
    "### The learning rate is an important concept for backpropagation training.  Setting the learning rate can be complex:\n",
    "\n",
    "* Too low of a learning rate will usually converge to a good solution; however, the process will be very slow.\n",
    "* Too high of a learning rate will either fail outright, or converge to a higher error than a better learning rate.\n",
    "\n",
    "#### Common values for learning rate are: 0.1, 0.01, 0.001, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch size\n",
    "\n",
    "#### Number of samples per gradient update.  In keras, you may set the batch_size parameter in function fit()\n",
    "\n",
    "https://keras.io/models/model/\n",
    "\n",
    "batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules (optimizers)\n",
    "\n",
    "The following image shows how each of these algorithms train (image credits: [author](http://sebastianruder.com/optimizing-gradient-descent/index.html#visualizationofalgorithms) ):\n",
    "\n",
    "![Training Techniques](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/contours_evaluation_optimizers.gif \"Training Techniques\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An optimizer is one of the two arguments required when you compile a Keras model using compile(). \n",
    "\n",
    "Specifying the Update Rule (Optimizer) in Tensorflow\n",
    "\n",
    "TensorFlow allows the update rule to be set to one of:\n",
    "\n",
    "* Adagrad\n",
    "* **Adam**\n",
    "* Ftrl\n",
    "* Momentum\n",
    "* RMSProp\n",
    "* **SGD**\n",
    "\n",
    "https://keras.io/optimizers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either instantiate an optimizer or you can call it by its name. In the latter case, the default parameters for the optimizer will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using all Default Parameters for a Particular Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass optimizer by name: default parameters will be used\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Complete Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n",
      "3/3 - 0s - loss: 15.1640 - val_loss: 16.1962\n",
      "Epoch 435/1000\n",
      "3/3 - 0s - loss: 15.1125 - val_loss: 16.1144\n",
      "Epoch 436/1000\n",
      "3/3 - 0s - loss: 15.0582 - val_loss: 16.0342\n",
      "Epoch 437/1000\n",
      "3/3 - 0s - loss: 15.0113 - val_loss: 15.9526\n",
      "Epoch 438/1000\n",
      "3/3 - 0s - loss: 14.9581 - val_loss: 15.8740\n",
      "Epoch 439/1000\n",
      "3/3 - 0s - loss: 14.9059 - val_loss: 15.7969\n",
      "Epoch 440/1000\n",
      "3/3 - 0s - loss: 14.8581 - val_loss: 15.7189\n",
      "Epoch 441/1000\n",
      "3/3 - 0s - loss: 14.8093 - val_loss: 15.6414\n",
      "Epoch 442/1000\n",
      "3/3 - 0s - loss: 14.7583 - val_loss: 15.5647\n",
      "Epoch 443/1000\n",
      "3/3 - 0s - loss: 14.7092 - val_loss: 15.4877\n",
      "Epoch 444/1000\n",
      "3/3 - 0s - loss: 14.6634 - val_loss: 15.4098\n",
      "Epoch 445/1000\n",
      "3/3 - 0s - loss: 14.6136 - val_loss: 15.3336\n",
      "Epoch 446/1000\n",
      "3/3 - 0s - loss: 14.5649 - val_loss: 15.2588\n",
      "Epoch 447/1000\n",
      "3/3 - 0s - loss: 14.5211 - val_loss: 15.1832\n",
      "Epoch 448/1000\n",
      "3/3 - 0s - loss: 14.4714 - val_loss: 15.1103\n",
      "Epoch 449/1000\n",
      "3/3 - 0s - loss: 14.4270 - val_loss: 15.0374\n",
      "Epoch 450/1000\n",
      "3/3 - 0s - loss: 14.3821 - val_loss: 14.9652\n",
      "Epoch 451/1000\n",
      "3/3 - 0s - loss: 14.3366 - val_loss: 14.8940\n",
      "Epoch 452/1000\n",
      "3/3 - 0s - loss: 14.2901 - val_loss: 14.8241\n",
      "Epoch 453/1000\n",
      "3/3 - 0s - loss: 14.2454 - val_loss: 14.7538\n",
      "Epoch 454/1000\n",
      "3/3 - 0s - loss: 14.2039 - val_loss: 14.6819\n",
      "Epoch 455/1000\n",
      "3/3 - 0s - loss: 14.1589 - val_loss: 14.6103\n",
      "Epoch 456/1000\n",
      "3/3 - 0s - loss: 14.1137 - val_loss: 14.5391\n",
      "Epoch 457/1000\n",
      "3/3 - 0s - loss: 14.0715 - val_loss: 14.4680\n",
      "Epoch 458/1000\n",
      "3/3 - 0s - loss: 14.0269 - val_loss: 14.3994\n",
      "Epoch 459/1000\n",
      "3/3 - 0s - loss: 13.9807 - val_loss: 14.3317\n",
      "Epoch 460/1000\n",
      "3/3 - 0s - loss: 13.9401 - val_loss: 14.2617\n",
      "Epoch 461/1000\n",
      "3/3 - 0s - loss: 13.8971 - val_loss: 14.1919\n",
      "Epoch 462/1000\n",
      "3/3 - 0s - loss: 13.8531 - val_loss: 14.1221\n",
      "Epoch 463/1000\n",
      "3/3 - 0s - loss: 13.8099 - val_loss: 14.0526\n",
      "Epoch 464/1000\n",
      "3/3 - 0s - loss: 13.7664 - val_loss: 13.9827\n",
      "Epoch 465/1000\n",
      "3/3 - 0s - loss: 13.7229 - val_loss: 13.9133\n",
      "Epoch 466/1000\n",
      "3/3 - 0s - loss: 13.6805 - val_loss: 13.8451\n",
      "Epoch 467/1000\n",
      "3/3 - 0s - loss: 13.6377 - val_loss: 13.7778\n",
      "Epoch 468/1000\n",
      "3/3 - 0s - loss: 13.5943 - val_loss: 13.7105\n",
      "Epoch 469/1000\n",
      "3/3 - 0s - loss: 13.5547 - val_loss: 13.6420\n",
      "Epoch 470/1000\n",
      "3/3 - 0s - loss: 13.5128 - val_loss: 13.5749\n",
      "Epoch 471/1000\n",
      "3/3 - 0s - loss: 13.4725 - val_loss: 13.5091\n",
      "Epoch 472/1000\n",
      "3/3 - 0s - loss: 13.4282 - val_loss: 13.4447\n",
      "Epoch 473/1000\n",
      "3/3 - 0s - loss: 13.3923 - val_loss: 13.3772\n",
      "Epoch 474/1000\n",
      "3/3 - 0s - loss: 13.3502 - val_loss: 13.3113\n",
      "Epoch 475/1000\n",
      "3/3 - 0s - loss: 13.3090 - val_loss: 13.2474\n",
      "Epoch 476/1000\n",
      "3/3 - 0s - loss: 13.2677 - val_loss: 13.1827\n",
      "Epoch 477/1000\n",
      "3/3 - 0s - loss: 13.2303 - val_loss: 13.1170\n",
      "Epoch 478/1000\n",
      "3/3 - 0s - loss: 13.1917 - val_loss: 13.0515\n",
      "Epoch 479/1000\n",
      "3/3 - 0s - loss: 13.1498 - val_loss: 12.9881\n",
      "Epoch 480/1000\n",
      "3/3 - 0s - loss: 13.1119 - val_loss: 12.9232\n",
      "Epoch 481/1000\n",
      "3/3 - 0s - loss: 13.0738 - val_loss: 12.8592\n",
      "Epoch 482/1000\n",
      "3/3 - 0s - loss: 13.0380 - val_loss: 12.7952\n",
      "Epoch 483/1000\n",
      "3/3 - 0s - loss: 12.9955 - val_loss: 12.7336\n",
      "Epoch 484/1000\n",
      "3/3 - 0s - loss: 12.9579 - val_loss: 12.6719\n",
      "Epoch 485/1000\n",
      "3/3 - 0s - loss: 12.9217 - val_loss: 12.6100\n",
      "Epoch 486/1000\n",
      "3/3 - 0s - loss: 12.8833 - val_loss: 12.5485\n",
      "Epoch 487/1000\n",
      "3/3 - 0s - loss: 12.8469 - val_loss: 12.4873\n",
      "Epoch 488/1000\n",
      "3/3 - 0s - loss: 12.8090 - val_loss: 12.4255\n",
      "Epoch 489/1000\n",
      "3/3 - 0s - loss: 12.7739 - val_loss: 12.3633\n",
      "Epoch 490/1000\n",
      "3/3 - 0s - loss: 12.7379 - val_loss: 12.3017\n",
      "Epoch 491/1000\n",
      "3/3 - 0s - loss: 12.6994 - val_loss: 12.2433\n",
      "Epoch 492/1000\n",
      "3/3 - 0s - loss: 12.6654 - val_loss: 12.1839\n",
      "Epoch 493/1000\n",
      "3/3 - 0s - loss: 12.6306 - val_loss: 12.1258\n",
      "Epoch 494/1000\n",
      "3/3 - 0s - loss: 12.5952 - val_loss: 12.0696\n",
      "Epoch 495/1000\n",
      "3/3 - 0s - loss: 12.5622 - val_loss: 12.0137\n",
      "Epoch 496/1000\n",
      "3/3 - 0s - loss: 12.5275 - val_loss: 11.9600\n",
      "Epoch 497/1000\n",
      "3/3 - 0s - loss: 12.4942 - val_loss: 11.9058\n",
      "Epoch 498/1000\n",
      "3/3 - 0s - loss: 12.4596 - val_loss: 11.8523\n",
      "Epoch 499/1000\n",
      "3/3 - 0s - loss: 12.4284 - val_loss: 11.7977\n",
      "Epoch 500/1000\n",
      "3/3 - 0s - loss: 12.3959 - val_loss: 11.7450\n",
      "Epoch 501/1000\n",
      "3/3 - 0s - loss: 12.3624 - val_loss: 11.6909\n",
      "Epoch 502/1000\n",
      "3/3 - 0s - loss: 12.3307 - val_loss: 11.6358\n",
      "Epoch 503/1000\n",
      "3/3 - 0s - loss: 12.2974 - val_loss: 11.5819\n",
      "Epoch 504/1000\n",
      "3/3 - 0s - loss: 12.2647 - val_loss: 11.5298\n",
      "Epoch 505/1000\n",
      "3/3 - 0s - loss: 12.2329 - val_loss: 11.4760\n",
      "Epoch 506/1000\n",
      "3/3 - 0s - loss: 12.2026 - val_loss: 11.4216\n",
      "Epoch 507/1000\n",
      "3/3 - 0s - loss: 12.1698 - val_loss: 11.3701\n",
      "Epoch 508/1000\n",
      "3/3 - 0s - loss: 12.1381 - val_loss: 11.3185\n",
      "Epoch 509/1000\n",
      "3/3 - 0s - loss: 12.1081 - val_loss: 11.2661\n",
      "Epoch 510/1000\n",
      "3/3 - 0s - loss: 12.0792 - val_loss: 11.2141\n",
      "Epoch 511/1000\n",
      "3/3 - 0s - loss: 12.0477 - val_loss: 11.1627\n",
      "Epoch 512/1000\n",
      "3/3 - 0s - loss: 12.0169 - val_loss: 11.1124\n",
      "Epoch 513/1000\n",
      "3/3 - 0s - loss: 11.9886 - val_loss: 11.0609\n",
      "Epoch 514/1000\n",
      "3/3 - 0s - loss: 11.9574 - val_loss: 11.0101\n",
      "Epoch 515/1000\n",
      "3/3 - 0s - loss: 11.9286 - val_loss: 10.9603\n",
      "Epoch 516/1000\n",
      "3/3 - 0s - loss: 11.8990 - val_loss: 10.9100\n",
      "Epoch 517/1000\n",
      "3/3 - 0s - loss: 11.8677 - val_loss: 10.8599\n",
      "Epoch 518/1000\n",
      "3/3 - 0s - loss: 11.8423 - val_loss: 10.8077\n",
      "Epoch 519/1000\n",
      "3/3 - 0s - loss: 11.8125 - val_loss: 10.7586\n",
      "Epoch 520/1000\n",
      "3/3 - 0s - loss: 11.7830 - val_loss: 10.7122\n",
      "Epoch 521/1000\n",
      "3/3 - 0s - loss: 11.7559 - val_loss: 10.6661\n",
      "Epoch 522/1000\n",
      "3/3 - 0s - loss: 11.7286 - val_loss: 10.6200\n",
      "Epoch 523/1000\n",
      "3/3 - 0s - loss: 11.7018 - val_loss: 10.5744\n",
      "Epoch 524/1000\n",
      "3/3 - 0s - loss: 11.6774 - val_loss: 10.5289\n",
      "Epoch 525/1000\n",
      "3/3 - 0s - loss: 11.6518 - val_loss: 10.4851\n",
      "Epoch 526/1000\n",
      "3/3 - 0s - loss: 11.6237 - val_loss: 10.4423\n",
      "Epoch 527/1000\n",
      "3/3 - 0s - loss: 11.5988 - val_loss: 10.3986\n",
      "Epoch 528/1000\n",
      "3/3 - 0s - loss: 11.5745 - val_loss: 10.3531\n",
      "Epoch 529/1000\n",
      "3/3 - 0s - loss: 11.5483 - val_loss: 10.3094\n",
      "Epoch 530/1000\n",
      "3/3 - 0s - loss: 11.5229 - val_loss: 10.2657\n",
      "Epoch 531/1000\n",
      "3/3 - 0s - loss: 11.4989 - val_loss: 10.2237\n",
      "Epoch 532/1000\n",
      "3/3 - 0s - loss: 11.4722 - val_loss: 10.1839\n",
      "Epoch 533/1000\n",
      "3/3 - 0s - loss: 11.4474 - val_loss: 10.1442\n",
      "Epoch 534/1000\n",
      "3/3 - 0s - loss: 11.4239 - val_loss: 10.1050\n",
      "Epoch 535/1000\n",
      "3/3 - 0s - loss: 11.4005 - val_loss: 10.0660\n",
      "Epoch 536/1000\n",
      "3/3 - 0s - loss: 11.3765 - val_loss: 10.0277\n",
      "Epoch 537/1000\n",
      "3/3 - 0s - loss: 11.3521 - val_loss: 9.9905\n",
      "Epoch 538/1000\n",
      "3/3 - 0s - loss: 11.3288 - val_loss: 9.9524\n",
      "Epoch 539/1000\n",
      "3/3 - 0s - loss: 11.3080 - val_loss: 9.9130\n",
      "Epoch 540/1000\n",
      "3/3 - 0s - loss: 11.2835 - val_loss: 9.8742\n",
      "Epoch 541/1000\n",
      "3/3 - 0s - loss: 11.2613 - val_loss: 9.8359\n",
      "Epoch 542/1000\n",
      "3/3 - 0s - loss: 11.2378 - val_loss: 9.7992\n",
      "Epoch 543/1000\n",
      "3/3 - 0s - loss: 11.2147 - val_loss: 9.7632\n",
      "Epoch 544/1000\n",
      "3/3 - 0s - loss: 11.1918 - val_loss: 9.7265\n",
      "Epoch 545/1000\n",
      "3/3 - 0s - loss: 11.1705 - val_loss: 9.6894\n",
      "Epoch 546/1000\n",
      "3/3 - 0s - loss: 11.1472 - val_loss: 9.6546\n",
      "Epoch 547/1000\n",
      "3/3 - 0s - loss: 11.1253 - val_loss: 9.6182\n",
      "Epoch 548/1000\n",
      "3/3 - 0s - loss: 11.1042 - val_loss: 9.5810\n",
      "Epoch 549/1000\n",
      "3/3 - 0s - loss: 11.0811 - val_loss: 9.5449\n",
      "Epoch 550/1000\n",
      "3/3 - 0s - loss: 11.0601 - val_loss: 9.5061\n",
      "Epoch 551/1000\n",
      "3/3 - 0s - loss: 11.0378 - val_loss: 9.4685\n",
      "Epoch 552/1000\n",
      "3/3 - 0s - loss: 11.0154 - val_loss: 9.4328\n",
      "Epoch 553/1000\n",
      "3/3 - 0s - loss: 10.9952 - val_loss: 9.3986\n",
      "Epoch 554/1000\n",
      "3/3 - 0s - loss: 10.9728 - val_loss: 9.3654\n",
      "Epoch 555/1000\n",
      "3/3 - 0s - loss: 10.9517 - val_loss: 9.3319\n",
      "Epoch 556/1000\n",
      "3/3 - 0s - loss: 10.9326 - val_loss: 9.2977\n",
      "Epoch 557/1000\n",
      "3/3 - 0s - loss: 10.9124 - val_loss: 9.2628\n",
      "Epoch 558/1000\n",
      "3/3 - 0s - loss: 10.8924 - val_loss: 9.2282\n",
      "Epoch 559/1000\n",
      "3/3 - 0s - loss: 10.8710 - val_loss: 9.1937\n",
      "Epoch 560/1000\n",
      "3/3 - 0s - loss: 10.8526 - val_loss: 9.1611\n",
      "Epoch 561/1000\n",
      "3/3 - 0s - loss: 10.8337 - val_loss: 9.1272\n",
      "Epoch 562/1000\n",
      "3/3 - 0s - loss: 10.8128 - val_loss: 9.0972\n",
      "Epoch 563/1000\n",
      "3/3 - 0s - loss: 10.7952 - val_loss: 9.0659\n",
      "Epoch 564/1000\n",
      "3/3 - 0s - loss: 10.7770 - val_loss: 9.0352\n",
      "Epoch 565/1000\n",
      "3/3 - 0s - loss: 10.7580 - val_loss: 9.0060\n",
      "Epoch 566/1000\n",
      "3/3 - 0s - loss: 10.7395 - val_loss: 8.9749\n",
      "Epoch 567/1000\n",
      "3/3 - 0s - loss: 10.7213 - val_loss: 8.9449\n",
      "Epoch 568/1000\n",
      "3/3 - 0s - loss: 10.7048 - val_loss: 8.9122\n",
      "Epoch 569/1000\n",
      "3/3 - 0s - loss: 10.6865 - val_loss: 8.8797\n",
      "Epoch 570/1000\n",
      "3/3 - 0s - loss: 10.6685 - val_loss: 8.8505\n",
      "Epoch 571/1000\n",
      "3/3 - 0s - loss: 10.6518 - val_loss: 8.8209\n",
      "Epoch 572/1000\n",
      "3/3 - 0s - loss: 10.6340 - val_loss: 8.7925\n",
      "Epoch 573/1000\n",
      "3/3 - 0s - loss: 10.6162 - val_loss: 8.7641\n",
      "Epoch 574/1000\n",
      "3/3 - 0s - loss: 10.6005 - val_loss: 8.7353\n",
      "Epoch 575/1000\n",
      "3/3 - 0s - loss: 10.5821 - val_loss: 8.7087\n",
      "Epoch 576/1000\n",
      "3/3 - 0s - loss: 10.5658 - val_loss: 8.6804\n",
      "Epoch 577/1000\n",
      "3/3 - 0s - loss: 10.5504 - val_loss: 8.6515\n",
      "Epoch 578/1000\n",
      "3/3 - 0s - loss: 10.5341 - val_loss: 8.6244\n",
      "Epoch 579/1000\n",
      "3/3 - 0s - loss: 10.5178 - val_loss: 8.5944\n",
      "Epoch 580/1000\n",
      "3/3 - 0s - loss: 10.4999 - val_loss: 8.5657\n",
      "Epoch 581/1000\n",
      "3/3 - 0s - loss: 10.4855 - val_loss: 8.5381\n",
      "Epoch 582/1000\n",
      "3/3 - 0s - loss: 10.4701 - val_loss: 8.5101\n",
      "Epoch 583/1000\n",
      "3/3 - 0s - loss: 10.4545 - val_loss: 8.4832\n",
      "Epoch 584/1000\n",
      "3/3 - 0s - loss: 10.4383 - val_loss: 8.4584\n",
      "Epoch 585/1000\n",
      "3/3 - 0s - loss: 10.4232 - val_loss: 8.4332\n",
      "Epoch 586/1000\n",
      "3/3 - 0s - loss: 10.4082 - val_loss: 8.4110\n",
      "Epoch 587/1000\n",
      "3/3 - 0s - loss: 10.3928 - val_loss: 8.3889\n",
      "Epoch 588/1000\n",
      "3/3 - 0s - loss: 10.3787 - val_loss: 8.3649\n",
      "Epoch 589/1000\n",
      "3/3 - 0s - loss: 10.3643 - val_loss: 8.3413\n",
      "Epoch 590/1000\n",
      "3/3 - 0s - loss: 10.3494 - val_loss: 8.3188\n",
      "Epoch 591/1000\n",
      "3/3 - 0s - loss: 10.3366 - val_loss: 8.2949\n",
      "Epoch 592/1000\n",
      "3/3 - 0s - loss: 10.3215 - val_loss: 8.2712\n",
      "Epoch 593/1000\n",
      "3/3 - 0s - loss: 10.3084 - val_loss: 8.2439\n",
      "Epoch 594/1000\n",
      "3/3 - 0s - loss: 10.2942 - val_loss: 8.2158\n",
      "Epoch 595/1000\n",
      "3/3 - 0s - loss: 10.2815 - val_loss: 8.1906\n",
      "Epoch 596/1000\n",
      "3/3 - 0s - loss: 10.2666 - val_loss: 8.1661\n",
      "Epoch 597/1000\n",
      "3/3 - 0s - loss: 10.2529 - val_loss: 8.1418\n",
      "Epoch 598/1000\n",
      "3/3 - 0s - loss: 10.2400 - val_loss: 8.1177\n",
      "Epoch 599/1000\n",
      "3/3 - 0s - loss: 10.2257 - val_loss: 8.0940\n",
      "Epoch 600/1000\n",
      "3/3 - 0s - loss: 10.2144 - val_loss: 8.0704\n",
      "Epoch 601/1000\n",
      "3/3 - 0s - loss: 10.2006 - val_loss: 8.0482\n",
      "Epoch 602/1000\n",
      "3/3 - 0s - loss: 10.1888 - val_loss: 8.0271\n",
      "Epoch 603/1000\n",
      "3/3 - 0s - loss: 10.1752 - val_loss: 8.0033\n",
      "Epoch 604/1000\n",
      "3/3 - 0s - loss: 10.1637 - val_loss: 7.9815\n",
      "Epoch 605/1000\n",
      "3/3 - 0s - loss: 10.1512 - val_loss: 7.9596\n",
      "Epoch 606/1000\n",
      "3/3 - 0s - loss: 10.1389 - val_loss: 7.9391\n",
      "Epoch 607/1000\n",
      "3/3 - 0s - loss: 10.1268 - val_loss: 7.9195\n",
      "Epoch 608/1000\n",
      "3/3 - 0s - loss: 10.1133 - val_loss: 7.9000\n",
      "Epoch 609/1000\n",
      "3/3 - 0s - loss: 10.1021 - val_loss: 7.8777\n",
      "Epoch 610/1000\n",
      "3/3 - 0s - loss: 10.0901 - val_loss: 7.8587\n",
      "Epoch 611/1000\n",
      "3/3 - 0s - loss: 10.0778 - val_loss: 7.8389\n",
      "Epoch 612/1000\n",
      "3/3 - 0s - loss: 10.0662 - val_loss: 7.8202\n",
      "Epoch 613/1000\n",
      "3/3 - 0s - loss: 10.0546 - val_loss: 7.8022\n",
      "Epoch 614/1000\n",
      "3/3 - 0s - loss: 10.0429 - val_loss: 7.7869\n",
      "Epoch 615/1000\n",
      "3/3 - 0s - loss: 10.0317 - val_loss: 7.7709\n",
      "Epoch 616/1000\n",
      "3/3 - 0s - loss: 10.0210 - val_loss: 7.7554\n",
      "Epoch 617/1000\n",
      "3/3 - 0s - loss: 10.0098 - val_loss: 7.7350\n",
      "Epoch 618/1000\n",
      "3/3 - 0s - loss: 9.9991 - val_loss: 7.7153\n",
      "Epoch 619/1000\n",
      "3/3 - 0s - loss: 9.9885 - val_loss: 7.6953\n",
      "Epoch 620/1000\n",
      "3/3 - 0s - loss: 9.9786 - val_loss: 7.6787\n",
      "Epoch 621/1000\n",
      "3/3 - 0s - loss: 9.9679 - val_loss: 7.6636\n",
      "Epoch 622/1000\n",
      "3/3 - 0s - loss: 9.9578 - val_loss: 7.6474\n",
      "Epoch 623/1000\n",
      "3/3 - 0s - loss: 9.9472 - val_loss: 7.6344\n",
      "Epoch 624/1000\n",
      "3/3 - 0s - loss: 9.9375 - val_loss: 7.6211\n",
      "Epoch 625/1000\n",
      "3/3 - 0s - loss: 9.9283 - val_loss: 7.6071\n",
      "Epoch 626/1000\n",
      "3/3 - 0s - loss: 9.9197 - val_loss: 7.5942\n",
      "Epoch 627/1000\n",
      "3/3 - 0s - loss: 9.9105 - val_loss: 7.5810\n",
      "Epoch 628/1000\n",
      "3/3 - 0s - loss: 9.9011 - val_loss: 7.5664\n",
      "Epoch 629/1000\n",
      "3/3 - 0s - loss: 9.8917 - val_loss: 7.5493\n",
      "Epoch 630/1000\n",
      "3/3 - 0s - loss: 9.8834 - val_loss: 7.5320\n",
      "Epoch 631/1000\n",
      "3/3 - 0s - loss: 9.8733 - val_loss: 7.5160\n",
      "Epoch 632/1000\n",
      "3/3 - 0s - loss: 9.8651 - val_loss: 7.5001\n",
      "Epoch 633/1000\n",
      "3/3 - 0s - loss: 9.8556 - val_loss: 7.4853\n",
      "Epoch 634/1000\n",
      "3/3 - 0s - loss: 9.8484 - val_loss: 7.4702\n",
      "Epoch 635/1000\n",
      "3/3 - 0s - loss: 9.8389 - val_loss: 7.4573\n",
      "Epoch 636/1000\n",
      "3/3 - 0s - loss: 9.8296 - val_loss: 7.4446\n",
      "Epoch 637/1000\n",
      "3/3 - 0s - loss: 9.8204 - val_loss: 7.4318\n",
      "Epoch 638/1000\n",
      "3/3 - 0s - loss: 9.8113 - val_loss: 7.4173\n",
      "Epoch 639/1000\n",
      "3/3 - 0s - loss: 9.8033 - val_loss: 7.4030\n",
      "Epoch 640/1000\n",
      "3/3 - 0s - loss: 9.7942 - val_loss: 7.3864\n",
      "Epoch 641/1000\n",
      "3/3 - 0s - loss: 9.7862 - val_loss: 7.3689\n",
      "Epoch 642/1000\n",
      "3/3 - 0s - loss: 9.7760 - val_loss: 7.3523\n",
      "Epoch 643/1000\n",
      "3/3 - 0s - loss: 9.7686 - val_loss: 7.3368\n",
      "Epoch 644/1000\n",
      "3/3 - 0s - loss: 9.7595 - val_loss: 7.3183\n",
      "Epoch 645/1000\n",
      "3/3 - 0s - loss: 9.7512 - val_loss: 7.3012\n",
      "Epoch 646/1000\n",
      "3/3 - 0s - loss: 9.7432 - val_loss: 7.2847\n",
      "Epoch 647/1000\n",
      "3/3 - 0s - loss: 9.7345 - val_loss: 7.2694\n",
      "Epoch 648/1000\n",
      "3/3 - 0s - loss: 9.7280 - val_loss: 7.2507\n",
      "Epoch 649/1000\n",
      "3/3 - 0s - loss: 9.7192 - val_loss: 7.2352\n",
      "Epoch 650/1000\n",
      "3/3 - 0s - loss: 9.7104 - val_loss: 7.2220\n",
      "Epoch 651/1000\n",
      "3/3 - 0s - loss: 9.7029 - val_loss: 7.2102\n",
      "Epoch 652/1000\n",
      "3/3 - 0s - loss: 9.6957 - val_loss: 7.1995\n",
      "Epoch 653/1000\n",
      "3/3 - 0s - loss: 9.6873 - val_loss: 7.1845\n",
      "Epoch 654/1000\n",
      "3/3 - 0s - loss: 9.6784 - val_loss: 7.1717\n",
      "Epoch 655/1000\n",
      "3/3 - 0s - loss: 9.6705 - val_loss: 7.1620\n",
      "Epoch 656/1000\n",
      "3/3 - 0s - loss: 9.6626 - val_loss: 7.1537\n",
      "Epoch 657/1000\n",
      "3/3 - 0s - loss: 9.6558 - val_loss: 7.1450\n",
      "Epoch 658/1000\n",
      "3/3 - 0s - loss: 9.6466 - val_loss: 7.1359\n",
      "Epoch 659/1000\n",
      "3/3 - 0s - loss: 9.6396 - val_loss: 7.1279\n",
      "Epoch 660/1000\n",
      "3/3 - 0s - loss: 9.6318 - val_loss: 7.1191\n",
      "Epoch 661/1000\n",
      "3/3 - 0s - loss: 9.6248 - val_loss: 7.1087\n",
      "Epoch 662/1000\n",
      "3/3 - 0s - loss: 9.6164 - val_loss: 7.0936\n",
      "Epoch 663/1000\n",
      "3/3 - 0s - loss: 9.6094 - val_loss: 7.0786\n",
      "Epoch 664/1000\n",
      "3/3 - 0s - loss: 9.6024 - val_loss: 7.0625\n",
      "Epoch 665/1000\n",
      "3/3 - 0s - loss: 9.5943 - val_loss: 7.0508\n",
      "Epoch 666/1000\n",
      "3/3 - 0s - loss: 9.5873 - val_loss: 7.0414\n",
      "Epoch 667/1000\n",
      "3/3 - 0s - loss: 9.5804 - val_loss: 7.0332\n",
      "Epoch 668/1000\n",
      "3/3 - 0s - loss: 9.5730 - val_loss: 7.0221\n",
      "Epoch 669/1000\n",
      "3/3 - 0s - loss: 9.5663 - val_loss: 7.0092\n",
      "Epoch 670/1000\n",
      "3/3 - 0s - loss: 9.5585 - val_loss: 6.9971\n",
      "Epoch 671/1000\n",
      "3/3 - 0s - loss: 9.5540 - val_loss: 6.9851\n",
      "Epoch 672/1000\n",
      "3/3 - 0s - loss: 9.5444 - val_loss: 6.9784\n",
      "Epoch 673/1000\n",
      "3/3 - 0s - loss: 9.5388 - val_loss: 6.9700\n",
      "Epoch 674/1000\n",
      "3/3 - 0s - loss: 9.5319 - val_loss: 6.9619\n",
      "Epoch 675/1000\n",
      "3/3 - 0s - loss: 9.5251 - val_loss: 6.9501\n",
      "Epoch 676/1000\n",
      "3/3 - 0s - loss: 9.5177 - val_loss: 6.9366\n",
      "Epoch 677/1000\n",
      "3/3 - 0s - loss: 9.5118 - val_loss: 6.9229\n",
      "Epoch 678/1000\n",
      "3/3 - 0s - loss: 9.5050 - val_loss: 6.9114\n",
      "Epoch 679/1000\n",
      "3/3 - 0s - loss: 9.4973 - val_loss: 6.9023\n",
      "Epoch 680/1000\n",
      "3/3 - 0s - loss: 9.4914 - val_loss: 6.8925\n",
      "Epoch 681/1000\n",
      "3/3 - 0s - loss: 9.4853 - val_loss: 6.8850\n",
      "Epoch 682/1000\n",
      "3/3 - 0s - loss: 9.4787 - val_loss: 6.8817\n",
      "Epoch 683/1000\n",
      "3/3 - 0s - loss: 9.4723 - val_loss: 6.8765\n",
      "Epoch 684/1000\n",
      "3/3 - 0s - loss: 9.4663 - val_loss: 6.8674\n",
      "Epoch 685/1000\n",
      "3/3 - 0s - loss: 9.4601 - val_loss: 6.8589\n",
      "Epoch 686/1000\n",
      "3/3 - 0s - loss: 9.4542 - val_loss: 6.8527\n",
      "Epoch 687/1000\n",
      "3/3 - 0s - loss: 9.4486 - val_loss: 6.8482\n",
      "Epoch 688/1000\n",
      "3/3 - 0s - loss: 9.4427 - val_loss: 6.8374\n",
      "Epoch 689/1000\n",
      "3/3 - 0s - loss: 9.4367 - val_loss: 6.8291\n",
      "Epoch 690/1000\n",
      "3/3 - 0s - loss: 9.4306 - val_loss: 6.8244\n",
      "Epoch 691/1000\n",
      "3/3 - 0s - loss: 9.4244 - val_loss: 6.8177\n",
      "Epoch 692/1000\n",
      "3/3 - 0s - loss: 9.4185 - val_loss: 6.8113\n",
      "Epoch 693/1000\n",
      "3/3 - 0s - loss: 9.4127 - val_loss: 6.8073\n",
      "Epoch 694/1000\n",
      "3/3 - 0s - loss: 9.4076 - val_loss: 6.8014\n",
      "Epoch 695/1000\n",
      "3/3 - 0s - loss: 9.4017 - val_loss: 6.7932\n",
      "Epoch 696/1000\n",
      "3/3 - 0s - loss: 9.3961 - val_loss: 6.7836\n",
      "Epoch 697/1000\n",
      "3/3 - 0s - loss: 9.3903 - val_loss: 6.7738\n",
      "Epoch 698/1000\n",
      "3/3 - 0s - loss: 9.3844 - val_loss: 6.7688\n",
      "Epoch 699/1000\n",
      "3/3 - 0s - loss: 9.3791 - val_loss: 6.7653\n",
      "Epoch 700/1000\n",
      "3/3 - 0s - loss: 9.3734 - val_loss: 6.7569\n",
      "Epoch 701/1000\n",
      "3/3 - 0s - loss: 9.3701 - val_loss: 6.7449\n",
      "Epoch 702/1000\n",
      "3/3 - 0s - loss: 9.3634 - val_loss: 6.7410\n",
      "Epoch 703/1000\n",
      "3/3 - 0s - loss: 9.3578 - val_loss: 6.7370\n",
      "Epoch 704/1000\n",
      "3/3 - 0s - loss: 9.3523 - val_loss: 6.7331\n",
      "Epoch 705/1000\n",
      "3/3 - 0s - loss: 9.3476 - val_loss: 6.7257\n",
      "Epoch 706/1000\n",
      "3/3 - 0s - loss: 9.3425 - val_loss: 6.7202\n",
      "Epoch 707/1000\n",
      "3/3 - 0s - loss: 9.3368 - val_loss: 6.7080\n",
      "Epoch 708/1000\n",
      "3/3 - 0s - loss: 9.3313 - val_loss: 6.6981\n",
      "Epoch 709/1000\n",
      "3/3 - 0s - loss: 9.3258 - val_loss: 6.6899\n",
      "Epoch 710/1000\n",
      "3/3 - 0s - loss: 9.3206 - val_loss: 6.6797\n",
      "Epoch 711/1000\n",
      "3/3 - 0s - loss: 9.3164 - val_loss: 6.6713\n",
      "Epoch 712/1000\n",
      "3/3 - 0s - loss: 9.3090 - val_loss: 6.6671\n",
      "Epoch 713/1000\n",
      "3/3 - 0s - loss: 9.3044 - val_loss: 6.6617\n",
      "Epoch 714/1000\n",
      "3/3 - 0s - loss: 9.2989 - val_loss: 6.6569\n",
      "Epoch 715/1000\n",
      "3/3 - 0s - loss: 9.2942 - val_loss: 6.6501\n",
      "Epoch 716/1000\n",
      "3/3 - 0s - loss: 9.2893 - val_loss: 6.6402\n",
      "Epoch 717/1000\n",
      "3/3 - 0s - loss: 9.2844 - val_loss: 6.6302\n",
      "Epoch 718/1000\n",
      "3/3 - 0s - loss: 9.2784 - val_loss: 6.6205\n",
      "Epoch 719/1000\n",
      "3/3 - 0s - loss: 9.2736 - val_loss: 6.6111\n",
      "Epoch 720/1000\n",
      "3/3 - 0s - loss: 9.2682 - val_loss: 6.5994\n",
      "Epoch 721/1000\n",
      "3/3 - 0s - loss: 9.2638 - val_loss: 6.5895\n",
      "Epoch 722/1000\n",
      "3/3 - 0s - loss: 9.2584 - val_loss: 6.5819\n",
      "Epoch 723/1000\n",
      "3/3 - 0s - loss: 9.2541 - val_loss: 6.5785\n",
      "Epoch 724/1000\n",
      "3/3 - 0s - loss: 9.2485 - val_loss: 6.5736\n",
      "Epoch 725/1000\n",
      "3/3 - 0s - loss: 9.2443 - val_loss: 6.5684\n",
      "Epoch 726/1000\n",
      "3/3 - 0s - loss: 9.2375 - val_loss: 6.5621\n",
      "Epoch 727/1000\n",
      "3/3 - 0s - loss: 9.2339 - val_loss: 6.5606\n",
      "Epoch 728/1000\n",
      "3/3 - 0s - loss: 9.2278 - val_loss: 6.5553\n",
      "Epoch 729/1000\n",
      "3/3 - 0s - loss: 9.2224 - val_loss: 6.5500\n",
      "Epoch 730/1000\n",
      "3/3 - 0s - loss: 9.2177 - val_loss: 6.5456\n",
      "Epoch 731/1000\n",
      "3/3 - 0s - loss: 9.2133 - val_loss: 6.5421\n",
      "Epoch 732/1000\n",
      "3/3 - 0s - loss: 9.2078 - val_loss: 6.5396\n",
      "Epoch 733/1000\n",
      "3/3 - 0s - loss: 9.2031 - val_loss: 6.5379\n",
      "Epoch 734/1000\n",
      "3/3 - 0s - loss: 9.1987 - val_loss: 6.5341\n",
      "Epoch 735/1000\n",
      "3/3 - 0s - loss: 9.1934 - val_loss: 6.5281\n",
      "Epoch 736/1000\n",
      "3/3 - 0s - loss: 9.1891 - val_loss: 6.5193\n",
      "Epoch 737/1000\n",
      "3/3 - 0s - loss: 9.1839 - val_loss: 6.5133\n",
      "Epoch 738/1000\n",
      "3/3 - 0s - loss: 9.1790 - val_loss: 6.5079\n",
      "Epoch 739/1000\n",
      "3/3 - 0s - loss: 9.1748 - val_loss: 6.5001\n",
      "Epoch 740/1000\n",
      "3/3 - 0s - loss: 9.1701 - val_loss: 6.4899\n",
      "Epoch 741/1000\n",
      "3/3 - 0s - loss: 9.1646 - val_loss: 6.4816\n",
      "Epoch 742/1000\n",
      "3/3 - 0s - loss: 9.1599 - val_loss: 6.4720\n",
      "Epoch 743/1000\n",
      "3/3 - 0s - loss: 9.1566 - val_loss: 6.4646\n",
      "Epoch 744/1000\n",
      "3/3 - 0s - loss: 9.1527 - val_loss: 6.4590\n",
      "Epoch 745/1000\n",
      "3/3 - 0s - loss: 9.1479 - val_loss: 6.4526\n",
      "Epoch 746/1000\n",
      "3/3 - 0s - loss: 9.1425 - val_loss: 6.4513\n",
      "Epoch 747/1000\n",
      "3/3 - 0s - loss: 9.1380 - val_loss: 6.4509\n",
      "Epoch 748/1000\n",
      "3/3 - 0s - loss: 9.1342 - val_loss: 6.4513\n",
      "Epoch 749/1000\n",
      "3/3 - 0s - loss: 9.1290 - val_loss: 6.4434\n",
      "Epoch 750/1000\n",
      "3/3 - 0s - loss: 9.1245 - val_loss: 6.4363\n",
      "Epoch 751/1000\n",
      "3/3 - 0s - loss: 9.1203 - val_loss: 6.4316\n",
      "Epoch 752/1000\n",
      "3/3 - 0s - loss: 9.1163 - val_loss: 6.4278\n",
      "Epoch 753/1000\n",
      "3/3 - 0s - loss: 9.1115 - val_loss: 6.4243\n",
      "Epoch 754/1000\n",
      "3/3 - 0s - loss: 9.1070 - val_loss: 6.4215\n",
      "Epoch 755/1000\n",
      "3/3 - 0s - loss: 9.1027 - val_loss: 6.4199\n",
      "Epoch 756/1000\n",
      "3/3 - 0s - loss: 9.0989 - val_loss: 6.4172\n",
      "Epoch 757/1000\n",
      "3/3 - 0s - loss: 9.0947 - val_loss: 6.4106\n",
      "Epoch 758/1000\n",
      "3/3 - 0s - loss: 9.0910 - val_loss: 6.4083\n",
      "Epoch 759/1000\n",
      "3/3 - 0s - loss: 9.0868 - val_loss: 6.4044\n",
      "Epoch 760/1000\n",
      "3/3 - 0s - loss: 9.0825 - val_loss: 6.3988\n",
      "Epoch 761/1000\n",
      "3/3 - 0s - loss: 9.0782 - val_loss: 6.3943\n",
      "Epoch 762/1000\n",
      "3/3 - 0s - loss: 9.0741 - val_loss: 6.3906\n",
      "Epoch 763/1000\n",
      "3/3 - 0s - loss: 9.0695 - val_loss: 6.3881\n",
      "Epoch 764/1000\n",
      "3/3 - 0s - loss: 9.0665 - val_loss: 6.3827\n",
      "Epoch 765/1000\n",
      "3/3 - 0s - loss: 9.0616 - val_loss: 6.3794\n",
      "Epoch 766/1000\n",
      "3/3 - 0s - loss: 9.0582 - val_loss: 6.3706\n",
      "Epoch 767/1000\n",
      "3/3 - 0s - loss: 9.0534 - val_loss: 6.3662\n",
      "Epoch 768/1000\n",
      "3/3 - 0s - loss: 9.0488 - val_loss: 6.3689\n",
      "Epoch 769/1000\n",
      "3/3 - 0s - loss: 9.0447 - val_loss: 6.3682\n",
      "Epoch 770/1000\n",
      "3/3 - 0s - loss: 9.0415 - val_loss: 6.3748\n",
      "Epoch 771/1000\n",
      "3/3 - 0s - loss: 9.0367 - val_loss: 6.3736\n",
      "Epoch 772/1000\n",
      "3/3 - 0s - loss: 9.0322 - val_loss: 6.3666\n",
      "Epoch 00772: early stopping\n",
      "Score (RMSE): 2.5231244564056396\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 382.603125 248.518125\" width=\"382.603125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-02-19T21:20:25.598002</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 382.603125 248.518125 \nL 382.603125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 224.64 \nL 375.403125 224.64 \nL 375.403125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m045ad7cdf2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.821307\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(52.640057 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"94.348349\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(87.985849 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"132.875392\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(126.512892 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.402435\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g transform=\"translate(165.039935 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.929477\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g transform=\"translate(203.566977 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.45652\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g transform=\"translate(242.09402 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"286.983562\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <g transform=\"translate(280.621062 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.510605\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 70 -->\n      <g transform=\"translate(319.148105 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"364.037647\" xlink:href=\"#m045ad7cdf2\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 80 -->\n      <g transform=\"translate(357.675147 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mcf767e1aa2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mcf767e1aa2\" y=\"214.733589\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 218.532808)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mcf767e1aa2\" y=\"185.66742\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 189.466638)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mcf767e1aa2\" y=\"156.60125\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 160.400469)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mcf767e1aa2\" y=\"127.535081\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 131.334299)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mcf767e1aa2\" y=\"98.468911\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 102.26813)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mcf767e1aa2\" y=\"69.402742\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 73.20196)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mcf767e1aa2\" y=\"40.336572\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 40 -->\n      <g transform=\"translate(20.878125 44.135791)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mcf767e1aa2\" y=\"11.270402\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 45 -->\n      <g transform=\"translate(20.878125 15.069621)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- output -->\n     <g transform=\"translate(14.798438 132.411406)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"61.181641\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"124.560547\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"163.769531\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"227.246094\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"290.625\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pa6a5cef6a2)\" d=\"M 55.821307 214.733589 \nL 59.674011 208.920355 \nL 63.526715 203.107121 \nL 67.37942 203.107121 \nL 71.232124 197.293887 \nL 75.084828 197.293887 \nL 78.937532 197.293887 \nL 82.790237 197.293887 \nL 86.642941 191.480654 \nL 90.495645 191.480654 \nL 94.348349 191.480654 \nL 98.201054 185.66742 \nL 102.053758 185.66742 \nL 105.906462 185.66742 \nL 109.759166 179.854186 \nL 113.611871 179.854186 \nL 117.464575 179.854186 \nL 121.317279 174.040952 \nL 125.169983 174.040952 \nL 129.022688 174.040952 \nL 132.875392 171.134335 \nL 136.728096 168.227718 \nL 140.5808 168.227718 \nL 144.433505 168.227718 \nL 148.286209 167.646392 \nL 152.138913 165.321101 \nL 155.991618 162.414484 \nL 159.844322 162.414484 \nL 163.697026 161.251833 \nL 167.54973 157.182576 \nL 171.402435 150.788016 \nL 175.255139 150.788016 \nL 179.107843 150.788016 \nL 182.960547 144.974782 \nL 186.813252 144.974782 \nL 190.665956 144.974782 \nL 194.51866 144.974782 \nL 198.371364 139.161548 \nL 202.224069 139.161548 \nL 206.076773 139.161548 \nL 209.929477 139.161548 \nL 213.782181 136.254931 \nL 217.634886 134.510966 \nL 221.48759 133.348314 \nL 225.340294 133.348314 \nL 229.192998 130.441698 \nL 233.045703 130.441698 \nL 236.898407 127.535081 \nL 240.751111 127.535081 \nL 244.603815 127.535081 \nL 248.45652 127.535081 \nL 252.309224 127.535081 \nL 256.161928 127.535081 \nL 260.014632 121.721847 \nL 263.867337 121.721847 \nL 267.720041 121.721847 \nL 271.572745 117.071264 \nL 275.42545 115.908613 \nL 279.278154 115.908613 \nL 283.130858 115.908613 \nL 286.983562 110.095379 \nL 290.836267 110.095379 \nL 294.688971 110.095379 \nL 298.541675 104.282145 \nL 302.394379 104.282145 \nL 306.247084 99.631562 \nL 310.099788 92.655677 \nL 313.952492 92.655677 \nL 317.805196 84.517141 \nL 321.657901 81.029209 \nL 325.510605 81.029209 \nL 329.363309 78.122592 \nL 333.216013 74.634661 \nL 337.068718 72.890673 \nL 340.921422 71.146707 \nL 344.774126 63.589508 \nL 348.62683 63.589508 \nL 352.479535 56.613622 \nL 356.332239 51.96304 \nL 360.184943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pa6a5cef6a2)\" d=\"M 55.821307 214.756364 \nL 59.674011 187.222221 \nL 63.526715 201.062602 \nL 67.37942 193.945779 \nL 71.232124 188.359747 \nL 75.084828 180.991917 \nL 78.937532 202.09623 \nL 82.790237 202.752847 \nL 86.642941 194.920802 \nL 90.495645 195.948458 \nL 94.348349 198.548063 \nL 98.201054 188.571802 \nL 102.053758 196.462148 \nL 105.906462 187.115278 \nL 109.759166 192.659205 \nL 113.611871 201.451953 \nL 117.464575 178.478947 \nL 121.317279 175.045756 \nL 125.169983 186.576685 \nL 129.022688 157.024906 \nL 132.875392 168.356736 \nL 136.728096 155.43209 \nL 140.5808 171.9671 \nL 144.433505 181.053793 \nL 148.286209 169.268403 \nL 152.138913 170.258793 \nL 155.991618 165.133139 \nL 159.844322 163.43296 \nL 163.697026 171.353475 \nL 167.54973 168.121751 \nL 171.402435 146.754595 \nL 175.255139 160.608637 \nL 179.107843 159.933253 \nL 182.960547 139.176439 \nL 186.813252 140.498379 \nL 190.665956 173.821257 \nL 194.51866 134.131572 \nL 198.371364 164.181235 \nL 202.224069 140.426729 \nL 206.076773 133.71336 \nL 209.929477 179.373815 \nL 213.782181 136.803947 \nL 217.634886 125.614518 \nL 221.48759 136.241848 \nL 225.340294 103.589431 \nL 229.192998 96.921433 \nL 233.045703 130.360656 \nL 236.898407 138.48653 \nL 240.751111 141.676287 \nL 244.603815 147.074657 \nL 248.45652 103.596161 \nL 252.309224 132.195077 \nL 256.161928 138.447313 \nL 260.014632 130.420753 \nL 263.867337 128.267112 \nL 267.720041 116.79175 \nL 271.572745 146.183858 \nL 275.42545 97.83117 \nL 279.278154 112.829058 \nL 283.130858 130.102331 \nL 286.983562 90.361787 \nL 290.836267 145.1092 \nL 294.688971 86.258335 \nL 298.541675 118.564367 \nL 302.394379 95.660843 \nL 306.247084 105.91185 \nL 310.099788 54.3615 \nL 313.952492 87.663001 \nL 317.805196 79.865738 \nL 321.657901 84.189406 \nL 325.510605 77.938246 \nL 329.363309 103.078369 \nL 333.216013 70.62509 \nL 337.068718 65.611623 \nL 340.921422 69.669094 \nL 344.774126 76.118683 \nL 348.62683 57.449226 \nL 352.479535 65.481873 \nL 356.332239 53.617593 \nL 360.184943 46.047931 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 224.64 \nL 40.603125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 375.403125 224.64 \nL 375.403125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 224.64 \nL 375.403125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 375.403125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 129.770313 44.55625 \nQ 131.770313 44.55625 131.770313 42.55625 \nL 131.770313 14.2 \nQ 131.770313 12.2 129.770313 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 49.603125 20.298437 \nL 69.603125 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_19\">\n     <!-- expected -->\n     <g transform=\"translate(77.603125 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"59.773438\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"118.953125\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"182.429688\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"243.953125\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"298.933594\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"338.142578\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"399.666016\" xlink:href=\"#DejaVuSans-100\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 49.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_20\">\n     <!-- prediction -->\n     <g transform=\"translate(77.603125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"102.339844\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"163.863281\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"227.339844\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"255.123047\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"310.103516\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"349.3125\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"377.095703\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"438.277344\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa6a5cef6a2\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.603125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "path = \"./data/\"\n",
    "preprocess = True\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "encode_text_dummy(df, 'origin')\n",
    "df.drop('name',1,inplace=True)\n",
    "if preprocess:\n",
    "    encode_numeric_zscore(df, 'horsepower')\n",
    "    encode_numeric_zscore(df, 'weight')\n",
    "    encode_numeric_zscore(df, 'cylinders')\n",
    "    encode_numeric_zscore(df, 'displacement')\n",
    "    encode_numeric_zscore(df, 'acceleration')\n",
    "    encode_numeric_zscore(df, 'year')\n",
    "\n",
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
    "#model.add(Dense(10, activation='tanh'))   \n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "# batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test), batch_size= 128, callbacks=[monitor,checkpointer],verbose=2,epochs=1000)\n",
    "model.load_weights('dnn/best_weights.hdf5') # load weights from best model\n",
    "\n",
    "# Predict and measure RMSE\n",
    "pred = model.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "# Plot the chart\n",
    "chart_regression(pred.flatten(),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try varying optimizer paramters and batch size after class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "* [Google Colab](https://colab.research.google.com/) - Free web based platform that includes Python, Juypter Notebooks, and TensorFlow with free GPU support.  No setup needed.\n",
    "* [IBM Cognitive Class Labs](https://www.datascientistworkbench.com) - Free web based platform that includes Python, Juypter Notebooks, and TensorFlow.  No setup needed.\n",
    "* [Python Anaconda](https://www.continuum.io/downloads) - Python distribution that includes many data science packages, such as Numpy, Scipy, Scikit-Learn, Pandas, and much more.\n",
    "* [TensorFlow](https://www.tensorflow.org/) - Google's mathematics package for deep learning.\n",
    "* [Kaggle](https://www.kaggle.com/) - Competitive data science.  Good source of sample data.\n",
    "* T81-558: Applications of Deep Neural Networks. Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}